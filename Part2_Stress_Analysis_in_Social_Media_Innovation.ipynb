{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8HmbrwMi4SU6",
        "DSq5tGCK6QZC",
        "giU3QbQd7s5z",
        "oNSAxD8qrr96"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8ed4a89349eb4402a00cc2e76137b884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1a93959e8a1495fbef04da51c736584",
              "IPY_MODEL_d0bfd7a5ffd34941952cb0ccaf4c8ea5",
              "IPY_MODEL_b802e34e49a246e5bb866d1134f9203c"
            ],
            "layout": "IPY_MODEL_9b02c83c77004c319064e9c0d6dc6a40"
          }
        },
        "d1a93959e8a1495fbef04da51c736584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a7f8de16a044b2b945eaa95b39698b9",
            "placeholder": "​",
            "style": "IPY_MODEL_c5f019bcabf6453f925fa46a2319ef42",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "d0bfd7a5ffd34941952cb0ccaf4c8ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a1cc6b422ed49329a490ff91963509a",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7712506241b74f9c8254218588b1ef51",
            "value": 440473133
          }
        },
        "b802e34e49a246e5bb866d1134f9203c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02ef16e49d2f4a47900ee1a6ac228245",
            "placeholder": "​",
            "style": "IPY_MODEL_9e384b8367c44c3c97da9f9a4aa6de52",
            "value": " 420M/420M [00:06&lt;00:00, 59.5MB/s]"
          }
        },
        "9b02c83c77004c319064e9c0d6dc6a40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a7f8de16a044b2b945eaa95b39698b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5f019bcabf6453f925fa46a2319ef42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a1cc6b422ed49329a490ff91963509a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7712506241b74f9c8254218588b1ef51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02ef16e49d2f4a47900ee1a6ac228245": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e384b8367c44c3c97da9f9a4aa6de52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stress Analysis in Social Media - Innovation\n",
        "\n",
        "Shir Cohen 205790124\n",
        "\n",
        "Guy Assa 204118616\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lyAYHtuEE_Vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialisation "
      ],
      "metadata": {
        "id": "6t6YlmWaBCTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "X4PHH-41BFA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "CUDA_LAUNCH_BLOCKING=1"
      ],
      "metadata": {
        "id": "CU3OxnNitfTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD7RnxMV8Sb_",
        "outputId": "bbd53dc8-c83c-470a-eaee-706c3c596267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.0-py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3165914 sha256=a38fc89fc65a7277b903e2c0fcd5149788f0c19c112a3f67aa5fd7072b79cf81\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import string \n",
        "import tqdm\n",
        "import warnings\n",
        "import logging # This allows for seeing if the model converges. A log file is created.\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "import re\n",
        "\n",
        "# from spellchecker import SpellChecker\n",
        "\n",
        "from gensim.models import LdaModel\n",
        "from gensim import models, corpora, similarities\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from keras.layers import LSTM, Activation, Dropout, Dense, Input\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM,Dense,Dropout,Embedding,CuDNNLSTM,Bidirectional\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import fasttext.util"
      ],
      "metadata": {
        "id": "oZMUM2sn2Fbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------ https://towardsdatascience.com/unsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUAtunOe3SWR",
        "outputId": "da7e16b0-e6db-47b5-bf48-2ad73db68529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "m-qgz8pu7Gm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pm4U29cjnU7",
        "outputId": "8b73b983-68f1-477d-b2b1-f74ba349ea2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data"
      ],
      "metadata": {
        "id": "NtBLniSkBIc3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_LVgZgQa92V"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"dreaddit-train.csv\", index_col = 0)\n",
        "df_test = pd.read_csv(\"dreaddit-test.csv\", index_col = 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = df_train.drop(['post_id', 'sentence_range', 'id'], axis = 1)\n",
        "test = df_test.drop(['post_id', 'sentence_range', 'id'], axis = 1)"
      ],
      "metadata": {
        "id": "Y2_fsdL0yoqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "mHexFFAf3Wnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\"\n",
        "}"
      ],
      "metadata": {
        "id": "ye-45L-ruAya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text, remove_stopwords = True):\n",
        "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
        "    \n",
        "    # Convert words to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Replace contractions with their longer forms \n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "    \n",
        "    # Format words and remove unwanted characters\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    \n",
        "    # remove stop words\n",
        "    if remove_stopwords:\n",
        "        text = text.split()\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "        text = \" \".join(text)\n",
        "\n",
        "    # Tokenize each word\n",
        "    text =  nltk.WordPunctTokenizer().tokenize(text)\n",
        "        \n",
        "    return text"
      ],
      "metadata": {
        "id": "ck1uSH0gQfht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(s):\n",
        "    \"\"\"\n",
        "    Given a text, cleans and normalizes it.\n",
        "    \"\"\"\n",
        "    s = s.lower()\n",
        "    # Replace ips\n",
        "    s = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', ' _ip_ ', s)\n",
        "    # Isolate punctuation\n",
        "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\-\\\\\\/\\,])', r' \\1 ', s)\n",
        "    # Remove some special characters\n",
        "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
        "    # Replace numbers and symbols with language\n",
        "    s = s.replace('&', ' and ')\n",
        "    s = s.replace('@', ' at ')\n",
        "    s = s.replace('0', ' zero ')\n",
        "    s = s.replace('1', ' one ')\n",
        "    s = s.replace('2', ' two ')\n",
        "    s = s.replace('3', ' three ')\n",
        "    s = s.replace('4', ' four ')\n",
        "    s = s.replace('5', ' five ')\n",
        "    s = s.replace('6', ' six ')\n",
        "    s = s.replace('7', ' seven ')\n",
        "    s = s.replace('8', ' eight ')\n",
        "    s = s.replace('9', ' nine ')\n",
        "    return s\n",
        "\n",
        "def text_preprocessing(s):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    - Lowercase the sentence\n",
        "    - Change \"'t\" to \"not\"\n",
        "    - Remove \"@name\"\n",
        "    - Isolate and remove punctuations except \"?\"\n",
        "    - Remove other special characters\n",
        "    - Remove stop words except \"not\" and \"can\"\n",
        "    - Remove trailing whitespace\n",
        "    \"\"\"\n",
        "    s = s.lower()\n",
        "\n",
        "    s = s.split()\n",
        "    new_text = []\n",
        "    for word in s:\n",
        "      try:\n",
        "        new_text.append(contractions.fix(word))\n",
        "      except:\n",
        "        new_text.append(word)\n",
        "    s = \" \".join(new_text)\n",
        "\n",
        "    # Change 't to 'not'\n",
        "    s = re.sub(r\"\\'t\", \" not\", s)\n",
        "    # Remove @name\n",
        "    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n",
        "    # Isolate and remove punctuations except '?'\n",
        "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
        "    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
        "    # Remove some special characters\n",
        "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
        "    # Remove stopwords except 'not' and 'can'\n",
        "    s = \" \".join([word for word in s.split()\n",
        "                  if word not in stopwords.words('english')\n",
        "                  or word in ['not', 'can']])\n",
        "    # Remove trailing whitespace\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    # Replace '&amp;' with '&'\n",
        "    setattr = re.sub(r'&amp;', '&', s)\n",
        "    # Remove trailing whitespace\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', s, flags=re.MULTILINE)\n",
        "    slice = re.sub(r'\\<a href', ' ', s)\n",
        "    s = re.sub(r'&amp;', '', s) \n",
        "    s = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', s)\n",
        "    s = re.sub(r'<br />', ' ', s)\n",
        "    s = re.sub(r'\\'', ' ', s)\n",
        "\n",
        "    return s"
      ],
      "metadata": {
        "id": "-S62EOyduWeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean data using the built in cleaner in gensim\n",
        "train['text_clean1'] = train['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
        "test['text_clean1'] = test['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
        "\n",
        "train['text_clean2'] = train['text'].apply(lambda x: text_preprocessing(x))\n",
        "test['text_clean2'] = test['text'].apply(lambda x: text_preprocessing(x))\n",
        "\n",
        "train['text_clean3'] = list(map(clean_text, train['text']))\n",
        "test['text_clean3'] = list(map(clean_text, test['text']))"
      ],
      "metadata": {
        "id": "OyHs_qqR58RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature engineering"
      ],
      "metadata": {
        "id": "wkgrqe6e2dtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker\n",
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "o8uJyy-vbobu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74b84bd1-fa72-4449-d541-fe43a1a2c256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.1.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.2)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.8.3)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from numexpr->pyLDAvis) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->numexpr->pyLDAvis) (3.0.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
            "Building wheels for collected packages: pyLDAvis, sklearn\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136898 sha256=9a4f9d4192b84cd54ab7eaa93b4a747e2522512e10ba860cf972de68931707a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=2411be47e314844fef7d80587af4bb9d738c8a934d015a27652ba0c260119a29\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "Successfully built pyLDAvis sklearn\n",
            "Installing collected packages: sklearn, funcy, pyLDAvis\n",
            "Successfully installed funcy-1.17 pyLDAvis-3.3.1 sklearn-0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def punctuations(text):\n",
        "  count = 0\n",
        "  for i in text:\n",
        "    if i in string.punctuation:\n",
        "      count +=1\n",
        "  return count/(len(text))"
      ],
      "metadata": {
        "id": "5CotvE0SCAnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spelling mistakes\n",
        "\n",
        "# def spell3():\n",
        "#     s = df_train['text'][1]\n",
        "#     wordlist=s.split()\n",
        "#     spell = SpellChecker()\n",
        "#     print(list(spell.known(wordlist)))\n",
        "#     print(list(spell.unknown(wordlist)))\n",
        "#     amount_miss = len(list(spell.unknown(wordlist))) + len(list(spell.known(wordlist)))\n",
        "#     print(\"Possible amount of misspelled words in the text:\",amount_miss)\n",
        "# spell3()"
      ],
      "metadata": {
        "id": "g9gjWKwoalnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['punctuations'] = train['text'].apply(punctuations)\n",
        "test['punctuations'] = train['text'].apply(punctuations)\n",
        "train['punctuations'].sort_values(ascending = False) #.value_counts().reset_index().sort_values(by = 'punctuations', ascending = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq7KTx6kDqov",
        "outputId": "61a08704-1b6f-4260-f482-23adfd547e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "485     0.461883\n",
              "2570    0.356955\n",
              "296     0.333333\n",
              "2564    0.333333\n",
              "1593    0.333333\n",
              "          ...   \n",
              "951     0.007485\n",
              "343     0.006579\n",
              "24      0.006309\n",
              "1138    0.005800\n",
              "2667    0.000000\n",
              "Name: punctuations, Length: 2838, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA"
      ],
      "metadata": {
        "id": "kDw1fjS34IG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def bigrams(words, bi_min=15, tri_min=10):\n",
        "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
        "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "    return bigram_mod\n",
        "    \n",
        "def get_corpus(df):\n",
        "    df['text_lda'] = df['text_clean2'].str.strip() #strip_newline(df.text)\n",
        "    words = list(sent_to_words(df['text_lda']))\n",
        "    words = remove_stopwords(words)\n",
        "    bigram_mod = bigrams(words)\n",
        "    bigram = [bigram_mod[r] for r in words]\n",
        "    id2word = gensim.corpora.Dictionary(bigram)\n",
        "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
        "    id2word.compactify()\n",
        "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
        "    return corpus, id2word, bigram"
      ],
      "metadata": {
        "id": "GSK6WV3yIJYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "# stop_words.extend(['come','order','try','go','get','make','drink','plate','dish','restaurant','place',\n",
        "#                   'would','really','like','great','service','came','got'])\n",
        "\n",
        "\n",
        "train_corpus, train_id2word, bigram_train = get_corpus(train)\n",
        "test_corpus, test_id2word, bigram_test = get_corpus(test)"
      ],
      "metadata": {
        "id": "DRG0C0bk5mEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from gensim.test.utils import common_corpus, common_dictionary\n",
        "# from gensim.models import HdpModel\n",
        "\n",
        "# hdp = HdpModel(train_corpus, train_id2word)\n",
        "# hdp.get_topics()"
      ],
      "metadata": {
        "id": "9Q7RNWWS5l5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter('ignore')\n",
        "    lda_train = gensim.models.ldamulticore.LdaMulticore(\n",
        "                           corpus=train_corpus,\n",
        "                           num_topics= 10, #20, ################# number of subredit\n",
        "                           id2word=train_id2word,\n",
        "                           chunksize=100,\n",
        "                           workers=7, # Num. Processing Cores - 1\n",
        "                           passes=50,\n",
        "                           eval_every = 1,\n",
        "                           per_word_topics=True)\n",
        "    lda_train.save('lda_train.model')"
      ],
      "metadata": {
        "id": "nX0W1FD0w2_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis.gensim_models\n",
        "lda_display = pyLDAvis.gensim_models.prepare(lda_train, train_corpus, train_id2word, sort_topics=False)\n",
        "pyLDAvis.display(lda_display)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "Ez-Qrekdu_YW",
        "outputId": "72ec8100-c0b5-443f-f542-c0f469c951df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n",
            "/usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el781401096980457767022966737\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el781401096980457767022966737_data = {\"mdsDat\": {\"x\": [0.3602487772056298, 0.0016696510958081323, -0.029543836373285524, -0.04867666290880806, 0.12426290692801362, -0.10262431751129893, -0.08385743187654977, -0.08005119302451608, -0.10359160386845574, -0.03783628966653692], \"y\": [-0.0661801152224556, -0.16784926924349114, -0.03215945834660669, 0.10079021057489436, 0.18346653578756167, -0.0590230517721667, 0.042476797446129935, 0.1314754834258455, -0.12587474758535838, -0.007122385064352826], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [3.201299809435955, 6.565139514981916, 11.86654405671696, 9.590246502023767, 7.650994220793686, 18.02631663888351, 9.412819421814818, 18.982220890363617, 7.547155453934942, 7.157263491050829]}, \"tinfo\": {\"Term\": [\"anxiety\", \"job\", \"help\", \"feel\", \"people\", \"money\", \"get\", \"feel_like\", \"ptsd\", \"would\", \"want\", \"url\", \"one\", \"treatment\", \"like\", \"survey\", \"thank\", \"told\", \"car\", \"know\", \"need\", \"please\", \"back\", \"pay\", \"relationship\", \"family\", \"time\", \"support\", \"feeling\", \"symptoms\", \"treatment\", \"write\", \"ocd\", \"department\", \"english\", \"nbsp\", \"prior\", \"language\", \"psychology\", \"match\", \"psychological\", \"book\", \"dogs\", \"patient\", \"speak\", \"research\", \"project\", \"dm\", \"sessions\", \"designed\", \"stories\", \"military\", \"seeking\", \"choose\", \"benefit\", \"hello\", \"accepted\", \"adults\", \"series\", \"meaning\", \"online\", \"participate\", \"provide\", \"survey\", \"interested\", \"study\", \"minutes\", \"complete\", \"ptsd\", \"longer\", \"name\", \"writing\", \"one\", \"four\", \"anyone\", \"people\", \"least\", \"post\", \"time\", \"mental_health\", \"may\", \"well\", \"card\", \"paying\", \"surgery\", \"credit\", \"cost\", \"return\", \"vet\", \"loan\", \"brothers\", \"scary\", \"receive\", \"gym\", \"awesome\", \"original\", \"july\", \"april\", \"gift\", \"charge\", \"rock\", \"determined\", \"decisions\", \"finances\", \"vacation\", \"transfer\", \"believed\", \"bottom\", \"closer\", \"payments\", \"saving\", \"couple_months\", \"paid\", \"agreed\", \"debt\", \"major\", \"money\", \"social\", \"rest\", \"full\", \"time\", \"months\", \"first\", \"would\", \"year\", \"one\", \"left\", \"work\", \"also\", \"since\", \"best\", \"put\", \"family\", \"much\", \"parents\", \"move\", \"made\", \"still\", \"enough\", \"back\", \"able\", \"get\", \"life\", \"even\", \"buy\", \"pm\", \"members\", \"couch\", \"housing\", \"donate\", \"art\", \"funds\", \"shelter\", \"save\", \"goal\", \"limited\", \"loans\", \"unemployed\", \"fill\", \"hot\", \"storage\", \"minimum\", \"groceries\", \"dollar\", \"pay\", \"afford\", \"new_job\", \"job\", \"area\", \"gas\", \"income\", \"site\", \"rent\", \"car\", \"assistance\", \"dog\", \"bills\", \"money\", \"food\", \"store\", \"due\", \"homeless\", \"get\", \"help\", \"work\", \"need\", \"family\", \"able\", \"working\", \"school\", \"jobs\", \"month\", \"home\", \"house\", \"could\", \"go\", \"going\", \"living\", \"would\", \"back\", \"know\", \"live\", \"even\", \"time\", \"really\", \"much\", \"got\", \"forever\", \"laptop\", \"sobbing\", \"worthless\", \"subject\", \"assumed\", \"management\", \"finding\", \"younger\", \"pregnant\", \"skills\", \"horrible\", \"many_times\", \"evidence\", \"government\", \"abuser\", \"men\", \"domestic_violence\", \"carry\", \"convinced\", \"situations\", \"shame\", \"fire\", \"obvious\", \"feel_guilty\", \"ruin\", \"human\", \"remember\", \"truly\", \"ability\", \"memories\", \"wish\", \"fault\", \"email\", \"app\", \"true\", \"response\", \"done\", \"could\", \"feel\", \"nothing\", \"afraid\", \"hate\", \"trust\", \"phone\", \"someone\", \"life\", \"abuse\", \"really\", \"even\", \"would\", \"keep\", \"scared\", \"everything\", \"know\", \"think\", \"like\", \"also\", \"person\", \"people\", \"things\", \"family\", \"want\", \"work\", \"feel_like\", \"go\", \"always\", \"going\", \"still\", \"way\", \"session\", \"sharing\", \"feel_free\", \"click\", \"answers\", \"welcome\", \"relevant\", \"greatly\", \"anonymous\", \"proud\", \"participation\", \"purpose\", \"particular\", \"terms\", \"connect\", \"triggering\", \"url\", \"link\", \"cancer\", \"responses\", \"healing\", \"improve\", \"awareness\", \"consider\", \"topic\", \"form\", \"member\", \"beyond\", \"diagnosis\", \"emdr\", \"survivors\", \"share\", \"mental_health\", \"thank\", \"please\", \"community\", \"support\", \"others\", \"read\", \"questions\", \"let_know\", \"post\", \"survey\", \"people\", \"advice\", \"help\", \"similar\", \"forward\", \"would\", \"may\", \"might\", \"need\", \"reading\", \"take\", \"best\", \"story\", \"want\", \"therapy\", \"anyone\", \"like\", \"ask\", \"experience\", \"life\", \"also\", \"person\", \"make\", \"time\", \"sex\", \"son\", \"drunk\", \"assault\", \"sexual\", \"turning\", \"mutual\", \"seemed\", \"dates\", \"aunt\", \"ignoring\", \"odd\", \"threatened\", \"girl\", \"sister\", \"later\", \"cheated\", \"mum\", \"dated\", \"opened\", \"occasions\", \"birth\", \"quick\", \"ex\", \"shy\", \"tests\", \"married\", \"drinks\", \"five\", \"bar\", \"met\", \"months_ago\", \"knew\", \"went\", \"guy\", \"friend\", \"came\", \"girlfriend\", \"said\", \"told\", \"asked\", \"dating\", \"night\", \"wanted\", \"got\", \"seen\", \"relationship\", \"started\", \"mother\", \"years\", \"friends\", \"home\", \"mom\", \"never\", \"time\", \"since\", \"would\", \"one\", \"really\", \"thought\", \"like\", \"going\", \"know\", \"family\", \"symptoms\", \"meds\", \"suffering\", \"mg\", \"intense\", \"experiencing\", \"attacks\", \"illness\", \"shaking\", \"tend\", \"lexapro\", \"knife\", \"skin\", \"acts\", \"medications\", \"finished\", \"zoloft\", \"level\", \"affect\", \"throat\", \"traumatic\", \"chronic\", \"depression\", \"event\", \"medication\", \"breathing\", \"random\", \"panic\", \"weed\", \"hurts\", \"anxiety\", \"diagnosed\", \"experienced\", \"panic_attacks\", \"therapy\", \"better\", \"fear\", \"ptsd\", \"issues\", \"past\", \"control\", \"doctor\", \"mental\", \"feeling\", \"made\", \"thing\", \"trauma\", \"something\", \"life\", \"worse\", \"like\", \"therapist\", \"things\", \"really\", \"years\", \"day\", \"going\", \"tried\", \"much\", \"since\", \"still\", \"time\", \"also\", \"first\", \"get\", \"feel\", \"dont\", \"cry\", \"freaking\", \"dreams\", \"afterwards\", \"flashbacks\", \"perspective\", \"breath\", \"keeps\", \"doubt\", \"wake\", \"painful\", \"asks\", \"feels_like\", \"nightmares\", \"worries\", \"admit\", \"nervous\", \"idk\", \"tired\", \"cycle\", \"struggle\", \"im\", \"sleep\", \"pain\", \"shake\", \"breathe\", \"feel_like\", \"makes\", \"asshole\", \"sorry\", \"awful\", \"want\", \"feel\", \"asleep\", \"know\", \"stop\", \"anxious\", \"sometimes\", \"thinking\", \"feeling\", \"even\", \"whenever\", \"bad\", \"think\", \"deal\", \"get\", \"like\", \"something\", \"tell\", \"say\", \"really\", \"every\", \"time\", \"go\", \"people\", \"actually\", \"things\", \"help\", \"anything\", \"see\", \"talk\", \"day\", \"never\", \"way\", \"one\", \"anxiety\", \"stayed\", \"occasionally\", \"texted\", \"mistake\", \"empty\", \"fired\", \"bf\", \"interview\", \"responded\", \"hates\", \"figured\", \"tells\", \"romantic\", \"manager\", \"number\", \"eye\", \"boss\", \"baby\", \"hang\", \"lease\", \"denied\", \"drugs\", \"hitting\", \"silent\", \"best_friend\", \"movie\", \"city\", \"friendship\", \"texts\", \"weekend\", \"despite\", \"computer\", \"liked\", \"ago\", \"gf\", \"lived\", \"drug\", \"moved\", \"water\", \"moving\", \"back\", \"homeless\", \"together\", \"three\", \"place\", \"apartment\", \"let\", \"move\", \"relationship\", \"living\", \"friends\", \"two\", \"boyfriend\", \"loves\", \"live\", \"told\", \"call\", \"started\", \"one\", \"months\", \"year\", \"love\", \"everything\", \"week\", \"years\", \"person\", \"time\", \"people\", \"away\", \"go\", \"see\", \"good\", \"home\", \"get\", \"hair\", \"grandma\", \"arm\", \"annoying\", \"bottle\", \"headaches\", \"cute\", \"restraining\", \"tries\", \"training\", \"wear\", \"red\", \"yrs\", \"yell\", \"per\", \"certain\", \"manipulative\", \"music\", \"legs\", \"safety\", \"shortly\", \"laundry\", \"directly\", \"dropped\", \"clear\", \"absolute\", \"grabbed\", \"shared\", \"clean\", \"mad\", \"negative\", \"following\", \"result\", \"extra\", \"abused\", \"fucking\", \"nice\", \"heard\", \"years_old\", \"dad\", \"child\", \"mom\", \"used\", \"well\", \"like\", \"would\", \"body\", \"older\", \"never\", \"issues\", \"physically\", \"point\", \"anything\", \"think\", \"us\", \"put\", \"one\", \"could\", \"call\", \"say\", \"years\", \"got\", \"going\", \"know\", \"told\", \"get\", \"little\", \"around\", \"really\"], \"Freq\": [383.0, 248.0, 494.0, 419.0, 474.0, 193.0, 836.0, 279.0, 177.0, 733.0, 558.0, 137.0, 572.0, 75.0, 774.0, 95.0, 124.0, 307.0, 141.0, 813.0, 292.0, 142.0, 402.0, 122.0, 226.0, 299.0, 700.0, 111.0, 202.0, 81.0, 75.11073348212534, 37.869135187872395, 18.180179555216526, 17.665101678002276, 16.48118437874478, 14.650788691513954, 13.326607634965965, 12.954529146815045, 12.479396002767377, 11.148419707040063, 10.29070645543512, 20.24755184992677, 18.998722167260937, 16.291647935076934, 25.32138346298628, 34.88522479039064, 14.871167379294707, 7.817015808871384, 16.478944768117564, 8.365704129016295, 16.192412391485767, 7.9990906415178, 14.351049125590578, 20.17913475145415, 12.475173560136316, 26.6880494822042, 8.664403428370377, 11.385790876794408, 8.708886880277287, 10.787155982958136, 33.33937990036641, 16.076442397123024, 21.10471271434681, 46.67812001214177, 25.158402939289108, 26.83591639190363, 31.963835603673274, 24.507891248060954, 44.34253179840697, 22.820462382848543, 24.946541786332208, 20.010725551885418, 50.02886309838588, 18.57835677213066, 26.930395894358597, 29.27271296060424, 20.753255428520173, 21.474937386094894, 25.42223358045803, 19.08276894213533, 18.669206301656285, 18.435718125718694, 42.953003541799944, 37.71445239044344, 29.501637906573187, 26.567804202625215, 25.944886350922747, 22.57948132503056, 18.88913269604233, 17.082396583114466, 16.52822502625232, 16.364302520546946, 14.985040565984772, 14.94376838591193, 14.602054668444472, 13.99604821261613, 15.457372885837481, 13.661473064167305, 12.870400683564721, 11.233653190317582, 11.059295119071294, 10.50669756658704, 10.445551574403753, 10.172508373516068, 9.971880291218563, 8.911358397406259, 8.80626743599436, 11.571158003381647, 13.09508194169823, 9.602299439530638, 10.092594418075349, 14.847262844544126, 34.08434490983252, 19.2610138674698, 13.983231411565937, 23.702099008037536, 69.38633087263868, 31.37559575848942, 20.80957729985862, 27.255084769452164, 84.09861593825038, 42.12895220533346, 46.240346684314176, 78.33613389349769, 41.97143874234184, 64.21871074190528, 30.64313850460571, 48.86580910543373, 47.13319954384476, 39.959520787408735, 31.02328079080969, 30.110050001079454, 36.35177431554457, 35.12363733129703, 26.891385081408778, 25.826849508542274, 29.25875702213632, 31.654752595312463, 27.1981585988793, 31.690448009160235, 27.66489103371559, 32.15258751078123, 28.381624940725224, 27.325371032671995, 35.09741947195744, 30.031079439315207, 19.77269356805343, 18.496890374011873, 18.38540621550846, 18.288868019531492, 18.089551374817216, 15.998217022829266, 32.89746804188238, 31.204811088252463, 15.185396242523389, 14.482346337038953, 13.093568566982025, 13.011134781587447, 12.337108551701899, 12.022655993427655, 11.750889271278567, 11.160232400397126, 9.324348529314635, 9.078794699271283, 109.42860810084733, 43.2683804352969, 21.92697832697942, 218.19671516250173, 35.89479748389955, 15.09636701141001, 30.085509858295488, 11.81535563247721, 51.85735475876555, 120.09436082957502, 31.58744444389222, 48.63770006150295, 34.755503301274395, 118.10756464045843, 66.53753544833594, 27.58896534334102, 60.72968948826915, 59.605566455863055, 289.3712366454287, 177.2646872178952, 140.09820608181542, 112.50675206624258, 111.50148634813114, 79.8731552481321, 61.06474091661129, 58.63966911807523, 29.212433958795156, 54.98305279318717, 81.0743203296843, 59.54820627535616, 87.90542845109103, 91.66186560491852, 92.03446998290772, 47.426886732007304, 91.19861969836725, 72.64819617386166, 83.80431019315877, 52.75348569924913, 69.04712924184915, 67.0926254321583, 65.66479152656802, 51.9785335956645, 51.98277269593994, 20.133417180146253, 17.637664319365612, 14.834400898568305, 13.820752650639351, 13.316035026825933, 10.639735731365796, 10.525343063843962, 31.18205034163604, 27.012551542526367, 10.762575975757052, 12.690510018041142, 34.86340584098881, 16.047411022517117, 16.522306495450035, 16.571539780209722, 22.99949192319442, 28.865044642638107, 13.580829272267462, 9.253196149415254, 9.319972813967839, 15.920885927271549, 8.216487564495283, 10.2473732251619, 8.225648096214316, 14.87732734720438, 9.959170608190922, 15.539395044425754, 76.08920575034348, 26.167671902781002, 8.099202739340088, 26.228556370740936, 40.6867685191082, 25.439144618295423, 21.8468953417654, 18.42578337209696, 22.69117647467648, 18.651519887060047, 51.953290063624415, 107.32250441228786, 105.55584732367551, 53.02877870187087, 34.5623850463006, 36.68217992910783, 25.716958732628758, 42.38801387646557, 54.864344752895235, 73.54187060619338, 38.74133612575855, 94.99944006655514, 86.36001079949699, 97.1796259775185, 43.33428132441674, 37.68158878011275, 47.20384001829445, 85.93805410034477, 62.4474638671615, 80.54214162611358, 57.54383921179982, 46.78860010430437, 57.39028749916302, 51.96993193077195, 47.2332813011895, 55.155650599659, 48.612027160915595, 44.49869044198566, 48.61558443037577, 42.241944437355755, 45.038637416354796, 42.580665534012695, 40.38640814575435, 19.736475206513443, 19.45710020136619, 20.28333456203777, 15.809426310043635, 15.616971033308308, 14.69690767151343, 12.715513562909216, 12.457950814388152, 12.357654099359772, 12.212658910253536, 11.256950526719347, 10.817378735053225, 10.785335037001865, 10.04534160162813, 9.867041652455944, 9.327628175853352, 122.62949614352398, 44.313398875340305, 16.879263425314566, 22.39183474085524, 14.100406628495017, 12.822215125843135, 11.239330085133929, 16.334421769013577, 9.862247371586244, 24.990432264850266, 8.535998554309641, 19.36452702342418, 14.79601292616957, 9.910525465614112, 14.238729789197668, 59.81907613412923, 54.19991551636811, 87.66584604342287, 95.64784620923659, 28.310942513708408, 65.26261908536523, 54.738126306527114, 47.14119236237338, 39.9334737958493, 24.068887061637653, 57.552212012041394, 48.163168993470535, 139.8775195476654, 57.47286314614552, 133.9708697203878, 26.80208368426733, 23.082227650996494, 124.44613193861888, 44.04041674362249, 43.23518112360686, 66.15713639920409, 32.74761463545106, 58.38208511663874, 42.479944928558886, 28.43766240706735, 60.992449060214376, 35.44033004480537, 43.181562413237096, 63.76340270280601, 37.5105403394743, 32.40906326621278, 44.78866882548344, 41.73108214841668, 37.50479558456394, 38.039599636271674, 39.9108869403002, 86.71449892116077, 51.35668964161982, 39.56246336448313, 20.655897912329852, 35.43314441383312, 13.765256403716721, 13.685687591458166, 52.06360277424484, 12.949194256948529, 12.075958061358673, 11.209764477096478, 10.273300831411083, 18.59231757524534, 86.01459666050339, 69.52646464358777, 71.01291761774209, 23.38489569541599, 24.940522841424112, 10.129115090407465, 12.119582048400815, 12.340140624439305, 18.17247813746252, 16.687927761611295, 76.53460702627234, 9.208888772316154, 9.175997097203355, 32.11835265923505, 16.65561885641432, 19.217545938277503, 13.359175978500183, 79.3367622708941, 34.24619814715779, 59.907781059284495, 143.70450882707738, 90.64093282940702, 110.40867175704687, 86.10287440049036, 54.15608003532477, 175.10700843394997, 187.21776076872453, 99.17591475856656, 47.802128181685596, 98.7955150021973, 97.370135609902, 178.09186248800629, 39.748836886488235, 117.90694039026634, 114.25020640384992, 68.00534523510652, 126.19593522612246, 124.46522319048772, 101.99849604465543, 81.9079748788593, 124.63932415995389, 164.88439536048205, 94.06777510464293, 151.98423576980923, 119.69935374176926, 114.09059268893259, 78.17843080056961, 107.30004125212515, 89.52194405911828, 93.51513642126169, 80.33622766455937, 80.43794374159347, 28.966233705126186, 24.44931933874767, 23.635612775463482, 21.47884059983969, 20.726346867944763, 17.713467917167772, 18.24118067786658, 17.06811735691995, 15.89023486748423, 14.131351620348205, 13.693004326828163, 13.462028597207405, 13.338221820035756, 12.011132938995848, 12.008402015469171, 11.750454042493566, 25.200536920943346, 10.181264095024794, 10.261586025157364, 27.21294765425404, 19.74228444798296, 76.34947684076506, 22.290512321251335, 43.06153731907084, 20.536445480375612, 17.31609736694889, 12.13995224067162, 15.01290312633745, 10.192969499878012, 295.4920098557372, 43.40109626227927, 27.97731528695815, 32.43830372784429, 65.85482684102453, 55.387055168860115, 50.983480574039206, 76.4685694141415, 59.332232372144496, 66.41404190028764, 36.06323180862202, 32.499739379022316, 27.769341557183463, 65.74984004931656, 65.72057515606913, 53.075913075287694, 29.125682441069806, 70.33650253403387, 74.72443186500944, 40.26201476123001, 83.86670201694311, 36.93567624630299, 62.47584024543149, 71.28354188083057, 54.17655812340216, 49.94807200391264, 55.75513872800127, 40.672400933489726, 48.446970075282124, 44.257291428753625, 46.402464084961004, 50.68916325537791, 45.458529483320056, 41.328010411449704, 45.59721914919204, 42.28782230415405, 50.13865578045872, 41.61768931727042, 22.691677567560607, 19.267884350083605, 14.866552444329512, 31.228270879714774, 9.803165575018268, 12.41062954505293, 28.991685548685115, 14.341838655206006, 29.753544569435668, 15.319000412596681, 13.736151570389492, 29.176650919376, 29.547909274471014, 9.37338173679831, 10.31101774158248, 19.001156542681354, 17.01439469003954, 69.28843078692664, 13.76258779450445, 20.56486639419039, 70.4458574197424, 107.16862569230041, 43.99554760180754, 15.906524256374212, 13.759706965586847, 224.2311412530954, 95.6109814591931, 11.41107846436967, 64.38035986764481, 24.978884902316068, 345.59944045354297, 258.34057156123856, 29.81326304810013, 444.33016930825335, 87.79554420246383, 62.283038429929995, 72.45192444689681, 78.02943204766162, 110.96830441644359, 238.43908124551018, 32.04483315585783, 113.68104072672826, 178.4301541230357, 53.65766025440229, 300.89503625538254, 273.0646580565211, 143.1262948239266, 97.24062265018405, 95.94763458803136, 195.70572478131305, 85.59511452769169, 199.19017044905976, 146.55416000905834, 148.3281787271631, 69.16385420056521, 120.47695977820744, 131.6659618907959, 96.69780503769825, 99.11180412407371, 89.55555520329214, 97.80967316692245, 100.34784034313219, 87.78807242962178, 98.46700349678264, 86.81221878951918, 19.064450693519937, 17.832634608021518, 16.84374623124945, 11.096431444512595, 9.58049934807771, 10.959780363431136, 32.3440947031181, 14.972572410726908, 10.824678863765275, 14.454242964065385, 18.415638961541003, 30.707243578944492, 17.063519546684326, 13.92876285251988, 31.95716149212774, 13.14875421168361, 17.38084192830854, 24.65156504349351, 21.617902479814592, 13.836749165775107, 9.745186852894356, 23.289173653497894, 11.706087269534935, 7.453669694395123, 18.99205371824504, 8.080515611490926, 36.34729765417414, 13.04059652951421, 11.698999355684935, 26.93439745888979, 19.68989541693932, 16.12947114412677, 16.427229968098136, 45.435106151579994, 19.60666539845695, 24.819539964243294, 17.19760300310478, 46.383869302482324, 19.855463650974073, 24.701805736040956, 111.80261658178806, 42.855297454518464, 59.63138921216705, 32.088374642429166, 57.72179699389905, 28.149124592216577, 49.60825662059519, 42.42368846032356, 61.83400661170188, 39.962690356259, 70.12125365902408, 43.348352261187415, 43.3862243558135, 23.12660122765013, 45.132421740564865, 56.84096146815726, 36.934907195340365, 48.427589272018295, 65.89194618232823, 40.95090840433866, 42.28353365680359, 41.0798300345286, 40.95754081950671, 36.18653688758978, 44.7293386281702, 39.99345755734355, 50.060412518057795, 46.06330453801995, 38.09044755341641, 42.214758523560576, 39.686169435176595, 38.78746536794689, 37.33212862291376, 37.423133863630085, 22.190984440571707, 17.727110354165493, 17.622098202333472, 16.45919752275136, 12.13715712179487, 11.753854436310693, 10.992735819705233, 10.64730063070952, 10.461769640990324, 8.989103626697936, 15.763220549593044, 11.93393087668176, 9.799889377157198, 14.292119278952699, 12.43572701322111, 23.708670965174893, 8.660223215508136, 23.71920060478417, 8.934889284781486, 15.75222463737873, 8.214671036924287, 8.786851233226884, 10.428695090829327, 8.142632619554874, 22.858859037804738, 9.988890807271554, 7.889674830558439, 10.076482846553176, 15.345724539739386, 24.281203316312826, 21.51491016653656, 19.608658937080623, 16.395220160598782, 15.523753861345991, 29.973380866411475, 50.80771225750024, 35.46993395509897, 26.30386027173651, 20.330766779173555, 55.73366249549949, 35.754766307402, 52.35924905305479, 38.526052634684476, 61.4838818835082, 117.0517529630108, 105.99824137968578, 24.66963780088849, 23.290315221448886, 58.824279746985695, 34.79544052722325, 22.644149604601782, 38.495802324296335, 46.00115301031542, 54.330644026284915, 38.977295299355035, 34.220006924320636, 50.56045615633479, 43.33564755456516, 29.626978126754853, 34.23193958239912, 38.104788638103614, 37.42131141132721, 35.75863090917348, 38.93858207117455, 32.77463466003939, 37.723789311283966, 29.932449692866484, 28.728564631454123, 31.041002309709974], \"Total\": [383.0, 248.0, 494.0, 419.0, 474.0, 193.0, 836.0, 279.0, 177.0, 733.0, 558.0, 137.0, 572.0, 75.0, 774.0, 95.0, 124.0, 307.0, 141.0, 813.0, 292.0, 142.0, 402.0, 122.0, 226.0, 299.0, 700.0, 111.0, 202.0, 81.0, 75.99320145152774, 38.751609539474515, 19.062647923243894, 18.54759401896802, 17.363647237033923, 15.533287304914625, 14.209254493281492, 13.836979701759176, 13.361869146776954, 12.030891424048706, 11.173211016024824, 22.621817215878988, 22.37015499477131, 19.280716245151964, 30.33021480272079, 44.15649805570732, 18.9575862491502, 10.12278013276603, 21.878257018940968, 11.224975363646033, 21.73009657722625, 10.887179248125571, 19.749999840441575, 28.03160096081938, 17.480678980402914, 37.500349585398254, 12.23029551473124, 16.324718627943152, 12.663014314685231, 15.80190636449264, 51.671037673524715, 24.46488021927891, 35.39437357817501, 95.62620788536623, 49.80319760032721, 54.738079588884446, 70.96643045032191, 49.54683674368552, 177.27174830987914, 56.437341409005434, 69.11206626621887, 44.96543692793879, 572.0126721084658, 39.835129561516105, 222.06733854174402, 474.698827961508, 95.22455865203383, 123.2208088484191, 700.6333942000886, 74.06772325986849, 120.15222738799345, 247.64800622542685, 43.834603550659835, 38.59607873873093, 30.38924049229593, 27.449374644298192, 26.826483036842543, 23.461070674524745, 19.770742634067876, 17.96395388581177, 17.409916821586652, 17.246102059106658, 15.866610277691729, 15.825378657467514, 15.48365208983364, 14.877705205265894, 16.44126718834443, 14.54302271050313, 13.752134349418775, 12.115245331319935, 11.940940833171057, 11.388350474665538, 11.327166078955104, 11.054077438647091, 11.651485689248666, 10.609160006777143, 10.52384019875142, 13.837719403225778, 15.757144888672547, 11.565657618904346, 12.21589826296305, 18.966759569346888, 48.639218272801216, 26.39009662712349, 18.23147029114, 35.46483862010683, 193.57515413877516, 74.54422082488614, 39.5775322540485, 70.49228959864075, 700.6333942000886, 179.48293254438093, 217.4689310326801, 733.005876238535, 197.3995791324984, 572.0126721084658, 116.46616660065109, 393.275408158336, 376.2880892975681, 257.39113826407083, 133.47273806017114, 150.26899659187546, 299.7827743763351, 359.15139383222385, 127.51867631531266, 114.72913448319889, 210.10502377393658, 328.33043757618617, 156.67191361607115, 402.610372472596, 184.39104896139764, 836.2895666021873, 378.8345828074998, 530.4082220743965, 35.980810105253454, 30.914457961148752, 20.65611128385798, 19.380243074129382, 19.268759336042393, 19.172375816205257, 18.973058867167605, 16.88153759591935, 34.71723299488715, 32.97421708778406, 16.068796290077618, 15.365755208328064, 13.976890230390222, 13.894509959661711, 13.239130815152986, 12.90620890929974, 12.634223246143772, 12.043637101519892, 10.207644083262833, 9.962146099680371, 122.53292908697428, 48.56390484767262, 24.658116889406415, 248.40902581429165, 41.078378519639045, 17.378617161116605, 35.008100495671016, 13.84170984869221, 60.87350606790663, 141.08322980838227, 37.50514404495799, 59.45069499226853, 44.85468632697916, 193.57515413877516, 99.1150222105935, 36.112119556612996, 98.92952122385996, 103.24485841961443, 836.2895666021873, 494.73907788222306, 393.275408158336, 292.9853816536875, 299.7827743763351, 184.39104896139764, 127.78959460203235, 135.03689666112595, 42.02394796875408, 124.96993029715291, 253.59975149602045, 165.5086551184461, 394.81604386551453, 444.7252195217542, 449.4192882936278, 110.37615136388472, 733.005876238535, 402.610372472596, 813.0754872914624, 177.87941736559105, 530.4082220743965, 700.6333942000886, 636.2243188441812, 359.15139383222385, 365.1794660334484, 21.01532425140436, 18.519554404344863, 15.7163886802437, 14.702725540322348, 14.198066822001934, 11.52164510575191, 11.407448083523361, 34.36747147579119, 29.948426410804338, 12.602008664067057, 14.884980523474349, 40.92055087469204, 19.17101012124007, 19.901561133973665, 20.164162434197188, 28.624653000868197, 36.288036780317704, 17.07335572870992, 12.440511558607431, 12.598328212426287, 21.75543055046931, 11.248660124537231, 14.130267068977707, 11.351387996923629, 20.54646852247063, 13.878126060318406, 21.858534648022168, 108.62006385323153, 37.64443737784048, 11.809525842351386, 38.290991466912295, 60.67091374513934, 37.99881197686536, 32.38077105415251, 27.022758835924417, 37.35772296154877, 30.111616816198413, 133.991629827892, 394.81604386551453, 419.361978357135, 154.07911193859266, 82.95151420520473, 92.40892030720981, 51.77540951552581, 128.23217604971853, 212.4310381894663, 378.8345828074998, 112.42487779747657, 636.2243188441812, 530.4082220743965, 733.005876238535, 149.77243652285637, 112.58858391815083, 200.02745052242622, 813.0754872914624, 400.69105877796954, 774.0627065157179, 376.2880892975681, 216.64938022468627, 474.698827961508, 396.79957698646143, 299.7827743763351, 558.1446004031794, 393.275408158336, 279.4496646287476, 444.7252195217542, 238.23117718216417, 449.4192882936278, 328.33043757618617, 279.89200181403464, 20.61928545113651, 20.339905927264347, 21.2641847789227, 16.69215583141873, 16.499725715810662, 15.579628401691584, 13.598274362457785, 13.340783864145278, 13.240362445586399, 13.095439491678828, 12.139891022614842, 11.700107415112376, 11.668249756175001, 10.92807525750049, 10.74982515868036, 10.21042042699746, 137.29090003138617, 49.658111293089895, 18.951832802117274, 25.543918395904008, 16.088565001035548, 14.822696359101982, 12.995628326663502, 19.665879682981732, 11.877148355634942, 30.59211091804012, 10.588978135162224, 24.74267674466224, 19.09664146345441, 12.864060549000836, 18.583616363506827, 81.11933113517753, 74.06772325986849, 124.26329854148017, 142.49975370263417, 37.684420564807304, 111.31779249600952, 90.30044765174412, 75.85366480457361, 61.58579015534398, 35.80698758004338, 123.2208088484191, 95.62620788536623, 474.698827961508, 130.13723357156374, 494.73907788222306, 44.59647059735958, 36.927013581941395, 733.005876238535, 120.15222738799345, 118.37337585956485, 292.9853816536875, 73.94690532300959, 249.92546648887094, 133.47273806017114, 58.040772072644614, 558.1446004031794, 114.74704489867091, 222.06733854174402, 774.0627065157179, 145.11303363836257, 90.63943601202959, 378.8345828074998, 376.2880892975681, 216.64938022468627, 289.37144612848766, 700.6333942000886, 87.77049488032263, 52.250427115140425, 40.44481080839103, 21.538318302253973, 37.430546963534255, 14.647654051861803, 14.568075168429207, 55.50356085961586, 13.831561476275551, 12.958332596400643, 12.092160135713142, 11.155704593043286, 20.619883266397775, 96.62487046899882, 80.20051741720661, 82.02693863350875, 27.0373510893285, 28.85906202783835, 11.747052365193412, 14.078399605111056, 14.544035991678829, 21.57547622277125, 19.849081941857555, 91.61511338698317, 11.161199416159151, 11.190445728476714, 39.377786761435104, 20.49167915829948, 23.771538397498126, 16.59763279075489, 99.99463132013219, 43.679045774495485, 78.5447733319109, 204.58957171986071, 128.2686576801596, 159.32129262933407, 123.99073872608797, 74.30024224241035, 281.0986430441352, 307.4719427525903, 155.825089191909, 66.63277151747037, 164.84352497663096, 167.43416982660344, 365.1794660334484, 55.85321300168268, 226.965390255147, 231.21487759818822, 116.15628664533304, 316.9430609799214, 330.083681939959, 253.59975149602045, 181.56472648896514, 385.9340137537905, 700.6333942000886, 257.39113826407083, 733.005876238535, 572.0126721084658, 636.2243188441812, 200.93481430921017, 774.0627065157179, 449.4192882936278, 813.0754872914624, 299.7827743763351, 81.32075127578581, 29.849013792182582, 25.3333803382068, 24.518361889194516, 22.361728085228787, 21.609234790258554, 18.596250082747584, 19.155748677687473, 17.950926151973324, 16.77310411216918, 15.014093851328727, 14.575870497878796, 14.344955072877633, 14.221044774561097, 12.894120555508056, 12.891437313174956, 12.633282815578987, 27.353802608561175, 11.064082069640587, 11.184230118125598, 29.869727318084028, 21.673082381304255, 84.03057095850919, 24.55578452152282, 47.67864017713965, 22.96688626428297, 20.400039910524487, 14.395259008167267, 18.134351738501405, 12.620495798733586, 383.0895465349599, 59.2456073717481, 36.96935160496943, 46.23671498216791, 114.74704489867091, 94.77590710142503, 86.38308192160937, 177.27174830987914, 132.2944985803634, 161.42069750886412, 65.1840223780286, 55.64587438762408, 45.4041036853767, 202.82867895383728, 210.10502377393658, 158.90986238898768, 50.3047512735839, 326.11977721434806, 378.8345828074998, 102.17574030453893, 774.0627065157179, 94.35943052145431, 396.79957698646143, 636.2243188441812, 316.9430609799214, 303.1168670153486, 449.4192882936278, 153.78237429114927, 359.15139383222385, 257.39113826407083, 328.33043757618617, 700.6333942000886, 376.2880892975681, 217.4689310326801, 836.2895666021873, 419.361978357135, 51.02145604794319, 43.213927163365874, 23.604967764246645, 20.15063744296296, 15.749465361547719, 33.51750313382101, 10.686151632181218, 13.572172121626311, 32.444931246183586, 16.357220212289985, 34.6554792867203, 17.89028336931019, 16.14942019769533, 34.438701828380005, 34.91139522828727, 11.172271417205682, 12.404025056073364, 22.94209403628889, 20.60450908454756, 84.3785432315042, 16.826164553794406, 25.167846861318324, 86.2607474755315, 131.28390510007983, 53.982203531326974, 19.548856096095975, 17.022891906607985, 279.4496646287476, 119.19227895916795, 14.292996296885228, 80.89565436130694, 31.89150887236431, 558.1446004031794, 419.361978357135, 39.55830961136065, 813.0754872914624, 136.45242099008277, 92.24199736784061, 113.26465958721992, 124.92230764035858, 202.82867895383728, 530.4082220743965, 43.98584633576445, 217.51023588199092, 400.69105877796954, 85.52275493957312, 836.2895666021873, 774.0627065157179, 326.11977721434806, 192.58739984070075, 192.5696950846995, 636.2243188441812, 179.95162581299644, 700.6333942000886, 444.7252195217542, 474.698827961508, 129.7224761789653, 396.79957698646143, 494.73907788222306, 262.0383963415476, 284.93962354496773, 220.47962211815044, 303.1168670153486, 385.9340137537905, 279.89200181403464, 572.0126721084658, 383.0895465349599, 19.945344849595784, 18.71350996058576, 17.724690772003605, 11.977314200430984, 10.461575119099598, 12.634266341525345, 39.641900697922274, 18.635116384980016, 13.515205162171723, 18.129081864301597, 23.587131696966388, 39.35956743781594, 22.098818517146423, 18.288739104467666, 42.244803002674274, 17.43719221000938, 23.353445101639203, 33.53261811235463, 29.543103870718408, 19.4574728407905, 13.731273436671723, 33.02550785263149, 16.661077759191457, 10.696800884505146, 27.616295139177616, 11.758026286581103, 53.0341277737078, 19.65324817174196, 17.851428415401568, 41.3100561803881, 30.463477274334167, 25.005567793734645, 25.556496837094127, 79.68969019941257, 31.340580636438755, 41.52570223505505, 26.98326593278412, 88.03702447608597, 32.76413856488978, 44.93289397697458, 402.610372472596, 103.24485841961443, 172.99968372436766, 67.91837180502449, 169.98816117205243, 55.42788264656075, 138.67159359947962, 114.72913448319889, 226.965390255147, 110.37615136388472, 330.083681939959, 139.67746876811077, 141.69275628136324, 41.1712349651216, 177.87941736559105, 307.4719427525903, 116.54307361052376, 231.21487759818822, 572.0126721084658, 179.48293254438093, 197.3995791324984, 189.61929498210176, 200.02745052242622, 135.24346067516566, 316.9430609799214, 216.64938022468627, 700.6333942000886, 474.698827961508, 193.26944454157785, 444.7252195217542, 284.93962354496773, 270.580095934775, 253.59975149602045, 836.2895666021873, 23.072771802463798, 18.615608038397614, 18.5080797108333, 17.340840669886905, 13.018650734640593, 12.635307480339547, 11.87429045492067, 11.528783027256377, 11.343264032673165, 9.870766188264016, 17.601695918872956, 14.194762351231095, 11.704570836327362, 17.440794813250314, 15.322281986496412, 29.69618540818136, 10.854999952091514, 29.92556978063678, 11.65015748978044, 21.043698745743384, 11.037800162463313, 11.868257839646748, 14.13868231982483, 11.549970936299362, 32.7786545466865, 14.345633476480462, 11.369205767812307, 14.574727727790531, 22.340534090863223, 36.63004337296877, 33.383036647920875, 30.319800994985762, 25.42020474266635, 23.9328359903636, 51.692628763285875, 96.44787808677083, 67.89008074924737, 47.32442880148032, 35.417931480633925, 146.5844310088718, 92.1189045805558, 181.56472648896514, 109.71582662109329, 247.64800622542685, 774.0627065157179, 733.005876238535, 57.084216183114165, 51.632359180382146, 385.9340137537905, 132.2944985803634, 51.921706610078466, 169.7783924037716, 262.0383963415476, 400.69105877796954, 202.6309572782227, 150.26899659187546, 572.0126721084658, 394.81604386551453, 116.54307361052376, 192.5696950846995, 316.9430609799214, 365.1794660334484, 449.4192882936278, 813.0754872914624, 307.4719427525903, 836.2895666021873, 176.60218596659894, 212.85515937929713, 636.2243188441812], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.5783, -4.2631, -4.9969, -5.0257, -5.095, -5.2128, -5.3075, -5.3358, -5.3732, -5.486, -5.566, -4.8892, -4.9529, -5.1066, -4.6656, -4.3452, -5.1978, -5.841, -5.0952, -5.7731, -5.1127, -5.8179, -5.2334, -4.8926, -5.3735, -4.613, -5.738, -5.4649, -5.7329, -5.5189, -4.3905, -5.1199, -4.8478, -4.054, -4.6721, -4.6075, -4.4327, -4.6983, -4.1053, -4.7696, -4.6805, -4.901, -3.9847, -4.9753, -4.604, -4.5206, -4.8646, -4.8304, -4.6616, -4.9485, -4.9704, -4.983, -4.8554, -4.9854, -5.231, -5.3358, -5.3595, -5.4984, -5.6769, -5.7774, -5.8104, -5.8204, -5.9084, -5.9112, -5.9343, -5.9767, -5.8774, -6.0009, -6.0605, -6.1966, -6.2122, -6.2635, -6.2693, -6.2958, -6.3157, -6.4281, -6.44, -6.167, -6.0432, -6.3535, -6.3037, -5.9177, -5.0866, -5.6574, -5.9776, -5.4499, -4.3758, -5.1694, -5.5801, -5.3102, -4.1835, -4.8747, -4.7816, -4.2545, -4.8785, -4.4532, -5.1931, -4.7264, -4.7625, -4.9276, -5.1807, -5.2106, -5.0222, -5.0566, -5.3237, -5.3641, -5.2393, -5.1606, -5.3123, -5.1595, -5.2953, -5.145, -5.2697, -5.3077, -5.6493, -5.8052, -6.2231, -6.2898, -6.2959, -6.3011, -6.3121, -6.4349, -5.714, -5.7669, -6.4871, -6.5345, -6.6353, -6.6416, -6.6948, -6.7206, -6.7435, -6.7951, -6.9748, -7.0015, -4.5122, -5.44, -6.1197, -3.822, -5.6268, -6.493, -5.8034, -6.738, -5.2589, -4.4191, -5.7547, -5.323, -5.6591, -4.4358, -5.0097, -5.89, -5.101, -5.1197, -3.5397, -4.0298, -4.2651, -4.4844, -4.4934, -4.827, -5.0955, -5.136, -5.8328, -5.2004, -4.8121, -5.1206, -4.7312, -4.6893, -4.6853, -5.3482, -4.6944, -4.9218, -4.7789, -5.2418, -4.9726, -5.0013, -5.0229, -5.2566, -5.2565, -5.9921, -6.1244, -6.2975, -6.3683, -6.4055, -6.6299, -6.6407, -5.5546, -5.6981, -6.6184, -6.4536, -5.443, -6.2189, -6.1897, -6.1868, -5.859, -5.6318, -6.3858, -6.7695, -6.7623, -6.2268, -6.8883, -6.6674, -6.8872, -6.2946, -6.696, -6.2511, -4.6625, -5.7299, -6.9027, -5.7276, -5.2885, -5.7582, -5.9104, -6.0807, -5.8725, -6.0685, -5.0441, -4.3186, -4.3352, -5.0236, -5.4517, -5.3922, -5.7473, -5.2476, -4.9896, -4.6966, -5.3375, -4.4406, -4.5359, -4.4179, -5.2255, -5.3653, -5.14, -4.5408, -4.8601, -4.6057, -4.9419, -5.1488, -4.9446, -5.0438, -5.1393, -4.9843, -5.1106, -5.199, -5.1105, -5.251, -5.1869, -5.243, -5.296, -5.7861, -5.8003, -5.7587, -6.0079, -6.0202, -6.0809, -6.2257, -6.2462, -6.2543, -6.2661, -6.3475, -6.3874, -6.3903, -6.4614, -6.4793, -6.5356, -3.9594, -4.9772, -5.9424, -5.6598, -6.1223, -6.2174, -6.3491, -5.9753, -6.4798, -5.55, -6.6242, -5.8051, -6.0742, -6.4749, -6.1126, -4.6772, -4.7759, -4.295, -4.2079, -5.4253, -4.5901, -4.766, -4.9154, -5.0813, -5.5876, -4.7158, -4.8939, -3.8278, -4.7172, -3.8709, -5.4801, -5.6295, -3.9447, -4.9834, -5.0019, -4.5765, -5.2797, -4.7015, -5.0195, -5.4208, -4.6578, -5.2007, -5.0031, -4.6134, -5.1439, -5.2901, -4.9666, -5.0373, -5.1441, -5.1299, -5.0819, -5.1629, -5.6867, -5.9477, -6.5975, -6.0579, -7.0034, -7.0092, -5.6731, -7.0645, -7.1343, -7.2087, -7.296, -6.7028, -5.171, -5.3838, -5.3627, -6.4734, -6.409, -7.3101, -7.1307, -7.1127, -6.7256, -6.8108, -5.2878, -7.4054, -7.4089, -6.1561, -6.8128, -6.6697, -7.0333, -5.2518, -6.092, -5.5327, -4.6578, -5.1186, -4.9213, -5.17, -5.6337, -4.4601, -4.3933, -5.0286, -5.7585, -5.0325, -5.047, -4.4432, -5.943, -4.8556, -4.8871, -5.4059, -4.7877, -4.8015, -5.0006, -5.2199, -4.8001, -4.5203, -5.0815, -4.6018, -4.8405, -4.8885, -5.2665, -4.9499, -5.131, -5.0874, -5.2393, -4.5883, -5.6096, -5.7792, -5.813, -5.9087, -5.9444, -6.1014, -6.0721, -6.1386, -6.2101, -6.3274, -6.3589, -6.3759, -6.3851, -6.4899, -6.4902, -6.5119, -5.7489, -6.6552, -6.6474, -5.6721, -5.993, -4.6405, -5.8716, -5.2131, -5.9536, -6.1241, -6.4793, -6.2669, -6.6541, -3.2871, -5.2053, -5.6444, -5.4964, -4.7883, -4.9614, -5.0443, -4.6389, -4.8926, -4.7799, -5.3905, -5.4945, -5.6518, -4.7899, -4.7904, -5.004, -5.6042, -4.7225, -4.662, -5.2804, -4.5465, -5.3666, -4.841, -4.7091, -4.9835, -5.0648, -4.9548, -5.2702, -5.0953, -5.1858, -5.1384, -5.0501, -5.159, -5.2542, -5.1559, -5.2313, -5.7624, -5.9487, -6.5552, -6.7188, -6.9781, -6.2359, -7.3945, -7.1586, -6.3102, -7.014, -6.2843, -6.9481, -7.0572, -6.3038, -6.2912, -7.4393, -7.344, -6.7327, -6.8431, -5.4389, -7.0552, -6.6536, -5.4224, -5.0028, -5.8931, -6.9105, -7.0555, -4.2645, -5.1169, -7.2426, -5.5124, -6.4592, -3.8319, -4.1229, -6.2822, -3.5806, -5.2022, -5.5455, -5.3943, -5.3201, -4.968, -4.2031, -6.2101, -4.9438, -4.493, -5.6946, -3.9704, -4.0675, -4.7135, -5.1, -5.1134, -4.4006, -5.2276, -4.3829, -4.6898, -4.6778, -5.4407, -4.8857, -4.7969, -5.1056, -5.081, -5.1823, -5.0942, -5.0686, -5.2023, -5.0875, -5.2135, -5.807, -5.8738, -5.9309, -6.3482, -6.4951, -6.3606, -5.2784, -6.0487, -6.373, -6.0839, -5.8417, -5.3304, -5.9179, -6.1209, -5.2905, -6.1785, -5.8995, -5.55, -5.6813, -6.1275, -6.4781, -5.6069, -6.2948, -6.7462, -5.8108, -6.6654, -5.1618, -6.1868, -6.2954, -5.4615, -5.7748, -5.9742, -5.9559, -4.9386, -5.779, -5.5432, -5.9101, -4.9179, -5.7664, -5.548, -4.0381, -4.997, -4.6667, -5.2864, -4.6992, -5.4174, -4.8507, -5.0072, -4.6304, -5.0669, -4.5046, -4.9856, -4.9847, -5.6139, -4.9453, -4.7146, -5.1457, -4.8748, -4.5669, -5.0425, -5.0105, -5.0394, -5.0423, -5.1662, -4.9542, -5.0662, -4.8416, -4.9249, -5.1149, -5.0121, -5.0739, -5.0968, -5.135, -5.1326, -5.6021, -5.8267, -5.8327, -5.9009, -6.2056, -6.2376, -6.3046, -6.3365, -6.3541, -6.5058, -5.9441, -6.2224, -6.4195, -6.0421, -6.1813, -5.536, -6.5431, -5.5355, -6.5119, -5.9448, -6.5959, -6.5286, -6.3573, -6.6047, -5.5725, -6.4004, -6.6363, -6.3916, -5.971, -5.5121, -5.6331, -5.7259, -5.9048, -5.9595, -5.3015, -4.7738, -5.1331, -5.4321, -5.6897, -4.6812, -5.1251, -4.7437, -5.0505, -4.5831, -3.9392, -4.0384, -5.4963, -5.5538, -4.6273, -5.1523, -5.5819, -5.0513, -4.8732, -4.7067, -5.0388, -5.169, -4.7787, -4.9329, -5.3131, -5.1687, -5.0615, -5.0796, -5.125, -5.0398, -5.2122, -5.0715, -5.3029, -5.3439, -5.2665], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 3.4299, 3.4186, 3.3942, 3.3929, 3.3895, 3.3831, 3.3775, 3.3757, 3.3733, 3.3654, 3.3593, 3.3307, 3.2783, 3.2732, 3.2611, 3.2059, 3.1988, 3.1831, 3.1582, 3.1476, 3.1475, 3.1334, 3.1223, 3.1129, 3.1043, 3.1015, 3.0969, 3.0813, 3.0673, 3.0598, 3.0035, 3.0217, 2.9246, 2.7244, 2.7587, 2.7288, 2.644, 2.7377, 2.0559, 2.5361, 2.4226, 2.632, 1.0051, 2.6789, 1.3319, 0.6556, 1.9181, 1.6945, 0.1253, 2.0854, 1.5797, 0.8439, 2.7031, 2.7003, 2.6938, 2.6908, 2.69, 2.6851, 2.6778, 2.6731, 2.6714, 2.6709, 2.6662, 2.6661, 2.6648, 2.6623, 2.6617, 2.6609, 2.6571, 2.6478, 2.6467, 2.6428, 2.6424, 2.6403, 2.5677, 2.549, 2.5452, 2.5445, 2.5383, 2.5374, 2.5325, 2.4785, 2.3678, 2.4085, 2.4581, 2.3204, 1.6974, 1.858, 2.0805, 1.7731, 0.6034, 1.2741, 1.1752, 0.4873, 1.1752, 0.5365, 1.3882, 0.638, 0.646, 0.8607, 1.2642, 1.1158, 0.6136, 0.3985, 1.1669, 1.2322, 0.752, 0.3843, 0.9724, 0.1814, 0.8265, -0.5351, 0.132, -0.2424, 2.1066, 2.1025, 2.0877, 2.0848, 2.0845, 2.0843, 2.0838, 2.0777, 2.0776, 2.0763, 2.0749, 2.0722, 2.0662, 2.0658, 2.0609, 2.0605, 2.059, 2.0553, 2.0409, 2.0386, 2.0183, 2.016, 2.0141, 2.0018, 1.9966, 1.9907, 1.9799, 1.9732, 1.9711, 1.9704, 1.9597, 1.9307, 1.8764, 1.6374, 1.7329, 1.8622, 1.6435, 1.5821, 1.0702, 1.1051, 1.0993, 1.1743, 1.1424, 1.2948, 1.393, 1.2973, 1.7678, 1.3104, 0.9911, 1.1092, 0.6293, 0.5521, 0.5457, 1.2867, 0.0473, 0.4191, -0.1409, 0.916, 0.0926, -0.2145, -0.1395, 0.1985, 0.182, 2.3016, 2.2956, 2.2867, 2.2826, 2.2803, 2.2648, 2.2639, 2.2472, 2.2412, 2.1866, 2.1849, 2.1842, 2.1666, 2.1583, 2.1482, 2.1256, 2.1156, 2.1156, 2.0484, 2.043, 2.0322, 2.0303, 2.0231, 2.0223, 2.0216, 2.0126, 2.0032, 1.9885, 1.9808, 1.9673, 1.9661, 1.9449, 1.9432, 1.9509, 1.9615, 1.8459, 1.8654, 1.397, 1.0418, 0.9649, 1.2778, 1.4689, 1.4205, 1.6447, 1.2374, 0.9907, 0.7052, 1.279, 0.4427, 0.5293, 0.3238, 1.1043, 1.2499, 0.9004, 0.0972, 0.4856, 0.0816, 0.4666, 0.8118, 0.2316, 0.3117, 0.4965, 0.03, 0.2538, 0.5071, 0.1309, 0.6146, 0.044, 0.3018, 0.4085, 2.5266, 2.526, 2.5231, 2.516, 2.5153, 2.512, 2.5032, 2.5019, 2.5013, 2.5005, 2.4948, 2.4919, 2.4917, 2.4861, 2.4846, 2.4799, 2.4574, 2.4565, 2.4545, 2.4386, 2.4384, 2.4254, 2.4251, 2.3847, 2.3844, 2.3681, 2.3548, 2.3252, 2.3152, 2.3095, 2.304, 2.2657, 2.258, 2.2215, 2.1717, 2.2843, 2.0364, 2.0698, 2.0947, 2.1371, 2.1731, 1.809, 1.8845, 1.3484, 1.7531, 1.2639, 2.0612, 2.1005, 0.7971, 1.5667, 1.5631, 1.0822, 1.7558, 1.1162, 1.4255, 1.8569, 0.3565, 1.3955, 0.9328, 0.0739, 1.2174, 1.5419, 0.4352, 0.3712, 0.8165, 0.5413, -0.295, 1.7012, 1.6961, 1.6913, 1.6715, 1.6585, 1.6512, 1.6509, 1.6494, 1.6474, 1.6428, 1.6376, 1.6309, 1.6098, 1.597, 1.5705, 1.5692, 1.5682, 1.5674, 1.5651, 1.5635, 1.549, 1.5417, 1.5399, 1.5335, 1.5211, 1.5149, 1.5096, 1.5061, 1.5007, 1.4963, 1.4819, 1.47, 1.4425, 1.3601, 1.3661, 1.3466, 1.3487, 1.3971, 1.24, 1.2172, 1.2615, 1.3812, 1.2014, 1.1713, 0.9952, 1.3732, 1.0584, 1.0084, 1.178, 0.7925, 0.738, 0.8025, 0.9173, 0.5831, 0.2666, 0.7068, 0.14, 0.1492, -0.0052, 0.7694, -0.2627, 0.0999, -0.4494, 0.3965, 2.3522, 2.3331, 2.3276, 2.3264, 2.3228, 2.3214, 2.3145, 2.3142, 2.3127, 2.309, 2.3025, 2.3006, 2.2996, 2.299, 2.2922, 2.2921, 2.2907, 2.2811, 2.2799, 2.277, 2.2699, 2.2698, 2.2672, 2.2663, 2.2612, 2.2512, 2.1992, 2.1927, 2.1742, 2.1495, 2.1035, 2.0519, 2.0844, 2.0087, 1.8078, 1.8259, 1.8358, 1.5223, 1.5612, 1.475, 1.7712, 1.8253, 1.8714, 1.2366, 1.2009, 1.2665, 1.8166, 0.8291, 0.7398, 1.4318, 0.1407, 1.4252, 0.5144, 0.1742, 0.5966, 0.56, 0.2761, 1.0331, 0.3598, 0.6025, 0.4064, -0.2632, 0.2495, 0.7026, -0.546, 0.0689, 1.6442, 1.624, 1.6222, 1.6169, 1.604, 1.5909, 1.5754, 1.5722, 1.5491, 1.5302, 1.5092, 1.5065, 1.4998, 1.4959, 1.4949, 1.4861, 1.4769, 1.4732, 1.4702, 1.4646, 1.4607, 1.4597, 1.4591, 1.4587, 1.4571, 1.4555, 1.4489, 1.4415, 1.4412, 1.4365, 1.4333, 1.4174, 1.1823, 1.1772, 1.3788, 1.0574, 1.2207, 1.2689, 1.2149, 1.1911, 1.0586, 0.8621, 1.3449, 1.0128, 0.8527, 1.1955, 0.6395, 0.6197, 0.8381, 0.9783, 0.965, 0.4827, 0.9186, 0.4039, 0.5516, 0.4984, 1.0327, 0.4697, 0.3379, 0.6648, 0.6056, 0.7607, 0.5306, 0.3146, 0.5022, -0.0978, 0.1771, 2.5388, 2.5358, 2.533, 2.5076, 2.496, 2.4418, 2.3805, 2.3652, 2.362, 2.3575, 2.3365, 2.3358, 2.3254, 2.3117, 2.3049, 2.3017, 2.2886, 2.2763, 2.2717, 2.2431, 2.2411, 2.2347, 2.231, 2.2228, 2.2096, 2.2089, 2.2062, 2.1738, 2.1614, 2.1563, 2.1476, 2.1455, 2.142, 2.0221, 2.115, 2.0693, 2.1336, 1.9432, 2.0831, 1.9857, 1.3028, 1.7047, 1.5189, 1.8342, 1.5039, 1.9064, 1.556, 1.5891, 1.2837, 1.5681, 1.0349, 1.4139, 1.4005, 2.0072, 1.2125, 0.8959, 1.4349, 1.0207, 0.4229, 1.1063, 1.0432, 1.0545, 0.9981, 1.2656, 0.6259, 0.8944, -0.0548, 0.2513, 0.9599, 0.2293, 0.6127, 0.6415, 0.6681, -0.5227, 2.5981, 2.5881, 2.588, 2.5849, 2.5669, 2.5647, 2.5599, 2.5575, 2.5561, 2.5435, 2.5267, 2.4636, 2.4594, 2.4379, 2.4283, 2.4119, 2.4112, 2.4046, 2.3717, 2.3474, 2.3416, 2.3364, 2.3327, 2.2875, 2.2766, 2.2751, 2.2717, 2.268, 2.2615, 2.2259, 2.1977, 2.2012, 2.1985, 2.2042, 2.092, 1.9961, 1.9878, 2.0497, 2.082, 1.67, 1.6906, 1.3936, 1.5905, 1.2438, 0.748, 0.7033, 1.7981, 1.8409, 0.7559, 1.3015, 1.8072, 1.1531, 0.8972, 0.6389, 0.9886, 1.1574, 0.2111, 0.4276, 1.2675, 0.9097, 0.5187, 0.3589, 0.1059, -0.4018, 0.3983, -0.4616, 0.8621, 0.6343, -0.3832]}, \"token.table\": {\"Topic\": [4, 5, 1, 2, 3, 4, 6, 7, 8, 10, 3, 9, 10, 3, 4, 5, 6, 7, 10, 4, 10, 4, 6, 1, 3, 7, 2, 4, 5, 6, 8, 10, 8, 10, 1, 4, 3, 5, 8, 7, 2, 3, 10, 4, 6, 7, 8, 8, 2, 3, 5, 6, 7, 9, 2, 4, 7, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 6, 7, 8, 9, 10, 10, 5, 5, 7, 8, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 8, 10, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 9, 10, 1, 4, 2, 3, 4, 10, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 3, 4, 5, 6, 8, 10, 1, 4, 5, 6, 9, 10, 6, 8, 6, 8, 6, 8, 10, 2, 3, 4, 7, 6, 4, 5, 2, 3, 4, 5, 6, 8, 9, 2, 5, 6, 7, 8, 3, 8, 9, 2, 3, 4, 6, 7, 8, 9, 10, 2, 3, 4, 6, 7, 8, 9, 10, 6, 8, 1, 2, 1, 8, 2, 3, 4, 5, 6, 9, 10, 6, 9, 10, 3, 5, 7, 8, 5, 6, 9, 10, 2, 3, 6, 9, 4, 7, 8, 10, 1, 10, 9, 10, 10, 2, 8, 6, 7, 8, 9, 10, 8, 3, 8, 7, 9, 2, 3, 3, 4, 6, 8, 9, 10, 4, 6, 7, 9, 10, 5, 6, 3, 10, 2, 3, 4, 4, 10, 2, 6, 8, 3, 6, 8, 10, 1, 5, 7, 10, 3, 9, 2, 4, 10, 3, 6, 7, 10, 5, 2, 5, 2, 3, 5, 1, 4, 5, 10, 2, 4, 9, 5, 5, 6, 2, 4, 6, 7, 9, 4, 8, 2, 3, 1, 2, 3, 4, 6, 7, 8, 9, 10, 2, 9, 2, 7, 8, 10, 3, 8, 3, 4, 6, 9, 10, 6, 9, 6, 2, 6, 9, 2, 3, 4, 6, 7, 8, 9, 10, 4, 6, 8, 2, 3, 2, 2, 9, 1, 6, 7, 1, 9, 5, 8, 9, 2, 2, 7, 2, 5, 2, 10, 1, 4, 2, 7, 8, 10, 2, 3, 10, 1, 2, 3, 4, 9, 10, 3, 2, 3, 4, 6, 7, 8, 10, 8, 8, 9, 8, 6, 10, 3, 10, 7, 9, 5, 6, 9, 6, 2, 3, 6, 10, 4, 5, 5, 10, 9, 1, 2, 3, 4, 5, 7, 8, 9, 10, 2, 3, 4, 6, 7, 8, 10, 7, 9, 3, 5, 6, 7, 8, 3, 4, 7, 8, 9, 10, 4, 6, 4, 6, 7, 10, 1, 3, 5, 7, 8, 4, 6, 7, 9, 7, 6, 8, 10, 6, 9, 2, 3, 4, 5, 6, 7, 4, 8, 2, 3, 4, 6, 7, 8, 4, 5, 7, 8, 9, 5, 4, 8, 4, 7, 8, 3, 4, 7, 8, 10, 7, 8, 1, 9, 3, 2, 1, 4, 10, 7, 4, 6, 9, 1, 9, 1, 2, 3, 4, 5, 6, 7, 9, 10, 2, 6, 7, 6, 8, 2, 9, 10, 2, 3, 9, 10, 4, 2, 5, 8, 2, 3, 5, 9, 1, 2, 6, 8, 2, 6, 8, 9, 2, 3, 4, 5, 6, 8, 9, 4, 9, 4, 7, 8, 10, 1, 2, 3, 4, 8, 3, 2, 3, 2, 3, 4, 6, 7, 8, 9, 10, 8, 9, 2, 6, 9, 2, 6, 8, 9, 2, 3, 4, 6, 7, 8, 9, 10, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 6, 7, 8, 9, 2, 3, 4, 6, 7, 8, 9, 10, 4, 10, 3, 4, 10, 10, 5, 3, 4, 6, 8, 9, 2, 10, 2, 5, 6, 8, 9, 4, 5, 8, 9, 2, 9, 10, 4, 5, 3, 6, 9, 10, 1, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 3, 9, 3, 4, 6, 9, 10, 3, 9, 4, 6, 8, 3, 2, 3, 6, 8, 9, 10, 3, 2, 4, 8, 6, 7, 7, 8, 6, 7, 3, 4, 8, 9, 5, 10, 2, 3, 7, 1, 3, 5, 10, 7, 9, 3, 5, 6, 7, 10, 3, 7, 9, 3, 9, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 10, 4, 6, 7, 7, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 4, 3, 4, 5, 6, 10, 2, 10, 2, 4, 9, 1, 3, 4, 8, 9, 2, 4, 6, 7, 9, 10, 9, 10, 2, 3, 4, 5, 6, 7, 8, 9, 3, 4, 5, 2, 7, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 6, 7, 8, 9, 10, 6, 9, 3, 5, 10, 2, 3, 6, 7, 8, 9, 10, 3, 4, 5, 8, 9, 10, 2, 3, 6, 9, 2, 3, 4, 9, 10, 2, 3, 1, 3, 4, 5, 6, 8, 4, 5, 6, 8, 9, 10, 8, 9, 8, 10, 2, 4, 6, 7, 8, 9, 10, 2, 6, 10, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 4, 9, 10, 4, 10, 4, 5, 2, 5, 6, 1, 1, 2, 3, 4, 5, 7, 10, 1, 4, 2, 4, 5, 7, 7, 7, 3, 5, 3, 4, 8, 2, 4, 5, 1, 5, 7, 8, 1, 5, 6, 9, 10, 7, 2, 3, 4, 5, 6, 8, 1, 9, 3, 1, 6, 7, 10, 9, 3, 6, 7, 8, 9, 10, 2, 3, 4, 1, 2, 3, 4, 6, 7, 8, 9, 2, 3, 6, 7, 9, 10, 2, 6, 9, 3, 4, 5, 6, 10, 2, 3, 4, 8, 9, 3, 5, 6, 9, 1, 9, 3, 8, 9, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 7, 7, 10, 6, 1, 2, 3, 4, 9, 10, 1, 1, 2, 3, 4, 5, 8, 7, 8, 10, 8, 9, 2, 4, 6, 7, 8, 9, 10, 2, 3, 2, 4, 5, 6, 9, 10, 3, 6, 7, 8, 6, 8, 2, 4, 6, 7, 8, 10, 2, 4, 9, 4, 10, 9, 4, 6, 1, 6, 1, 4, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 6, 10, 2, 2, 4, 5, 6, 8, 10, 2, 3, 10, 5, 8, 9, 10, 7, 8, 7, 8, 7, 8, 2, 3, 5, 6, 9, 1, 10, 5, 5, 2, 4, 5, 6, 7, 8, 1, 2, 2, 3, 2, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 10, 4, 5, 6, 8, 9, 10, 8, 3, 4, 6, 9, 10, 3, 4, 7, 10, 2, 3, 4, 5, 8, 9, 10, 1, 3, 5, 8, 10, 3, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 6, 7, 8, 4, 10, 1, 1, 2, 5, 1, 2, 4, 5, 1, 1, 1, 4, 5, 6, 7, 8, 5, 2, 3, 4, 6, 7, 8, 9, 10, 1, 4, 5, 6, 4, 6, 7, 9, 5, 6, 7, 8, 3, 5, 6, 8, 10, 1, 3, 4, 5, 6, 7, 8, 9, 10, 2, 7, 10, 4, 5, 6, 7, 9, 5, 4, 6, 8, 2, 3, 1, 5, 2, 9, 1, 4, 6, 9, 3, 5, 2, 7, 10, 10, 8, 9, 10, 2, 2, 2, 5, 9, 4, 8, 4, 9, 10, 2, 3, 4, 6, 7, 8, 9, 10, 3, 10, 2, 5, 4, 5, 6, 8, 9, 10, 4, 5, 6, 7, 8, 2, 2, 3, 4, 6, 7, 8, 10, 1, 3, 4, 5, 6, 7, 8, 9, 10, 1, 8, 6, 9, 3, 5, 6, 8, 1, 5, 5, 1, 5, 6, 6, 9, 7, 8, 7, 4, 8, 3, 4, 5, 10, 6, 10, 5, 1, 3, 7, 10, 6, 9, 8, 9, 10, 1, 4, 5, 10, 2, 3, 4, 6, 7, 8, 9, 10, 2, 6, 8, 3, 7, 1, 4, 7, 4, 10, 7, 3, 6, 7, 8, 4, 2, 5, 6, 7, 9, 2, 4, 5, 6, 8, 10, 3, 4, 5, 6, 7, 8, 9, 10, 4, 7, 8, 9, 10, 6, 2, 3, 8, 10, 1, 2, 4, 5, 6, 7, 8, 9, 10, 9, 2, 3, 4, 6, 7, 8, 9, 10, 4, 6, 7, 8, 10, 3, 2, 3, 6, 10, 1, 6, 7, 2, 4, 5, 6, 2, 8, 1, 2, 10, 4, 7, 3, 5, 6, 2, 1, 5, 2, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 10, 2, 4, 5, 6, 7, 8, 3, 4, 5, 6, 8, 9, 10, 8, 9, 10, 7, 5, 4, 6, 9, 6, 9, 3, 5, 1, 4, 5, 7, 8, 10, 1, 5, 7, 3, 4, 6, 7, 8, 10, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 7, 8, 9, 10, 3, 6, 7, 8, 9, 4, 5, 6, 7, 8, 10, 3, 6, 6, 9, 10, 7, 1, 2, 3, 5, 6, 7, 8, 9, 10, 4, 6, 7, 8, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 6, 7, 8, 9, 10, 4, 5, 10, 2, 4, 5, 6, 7, 8, 10, 4, 6, 7, 1, 2, 3, 6, 7, 9, 10, 10, 5, 4, 6, 7, 9, 2, 4, 9, 4, 5, 7, 9, 6, 2, 3, 6, 7, 9, 10, 3, 1, 2, 3, 5, 1, 2, 3, 4, 6, 9, 10, 2, 4, 5, 6, 7, 8, 10, 1, 2, 2, 2, 8, 3, 4, 5, 7, 8, 9, 10, 2, 3, 4, 6, 8, 9, 10, 3, 7, 9, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 10, 3, 5, 7, 2, 3, 4, 6, 7, 9, 6, 8, 9, 5, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 2, 6, 7, 9, 6, 7, 8, 9, 10, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 9, 10, 8, 9, 2, 4, 6, 7, 8, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 6, 8, 10, 1, 2, 3, 4, 6, 7, 9, 2, 4, 5, 6, 7, 8, 9, 10, 3, 7, 9, 10, 3, 10, 2, 4, 5, 10, 7], \"Freq\": [0.6774192382314247, 0.25403221433678425, 0.037962797215093956, 0.15185118886037582, 0.4338605396010738, 0.032539540470080536, 0.054232567450134224, 0.054232567450134224, 0.09219536466522818, 0.1464279321153624, 0.1394152445954361, 0.1394152445954361, 0.6970762229771805, 0.01778965687294606, 0.34689830902244817, 0.09784311280120334, 0.22237071091182575, 0.10673794123767637, 0.2045810540388797, 0.4062474767178995, 0.5803535381684278, 0.8035031900405012, 0.17467460653054373, 0.7358775582453924, 0.2452925194151308, 0.9141381808497413, 0.038543821759168345, 0.10021393657383769, 0.007708764351833668, 0.16188405138850703, 0.5319047402765231, 0.1464665226848397, 0.806189922609332, 0.0806189922609332, 0.6738247837957348, 0.24502719410753995, 0.2612626614757649, 0.43799916776819403, 0.291999445178796, 0.9038255444109198, 0.041182849819702015, 0.8854312711235933, 0.041182849819702015, 0.42193322611829975, 0.060276175159757105, 0.1808285254792713, 0.3375465808946398, 0.9524132823341713, 0.06274337354666812, 0.050194698837334496, 0.037646024128000874, 0.1631327712213371, 0.11293807238400261, 0.5646903619200131, 0.7199670493237975, 0.1894650129799467, 0.03789300259598934, 0.12490429895811149, 0.132876913785225, 0.154137219990861, 0.111616607579589, 0.111616607579589, 0.1195892224067025, 0.21791813860776899, 0.015945229654227, 0.010630153102818, 0.06296405104244691, 0.17629934291885135, 0.20148496333583013, 0.025185620416978766, 0.3525986858377027, 0.12592810208489383, 0.054568844236787324, 0.9226772971730643, 0.9063196003369328, 0.9697130895132514, 0.7700549458169018, 0.22710095012227272, 0.010841048855568705, 0.032523146566706115, 0.2710262213892176, 0.6721450290452597, 0.12158474171529085, 0.049534524402525905, 0.09906904880505181, 0.13509415746143427, 0.1936349590280558, 0.1531067117896255, 0.16661612753576893, 0.07655335589481276, 0.026713642342994726, 0.12593574247411798, 0.026713642342994726, 0.03434611158385036, 0.041978580824706, 0.08014092702898418, 0.37017475818149836, 0.11448703861283455, 0.17554679253967964, 0.3427877647997967, 0.5051609165470689, 0.14433169044201968, 0.29604675261227187, 0.6661051933776118, 0.9626609459867683, 0.8763734426077422, 0.0973748269564158, 0.97254822116765, 0.03758424284066521, 0.07047045532624727, 0.11745075887707877, 0.12214878923216192, 0.21610939633382495, 0.09396060710166301, 0.18322318384824288, 0.023490151775415753, 0.13624288029741138, 0.948713653713927, 0.15849713442914093, 0.1722794939447184, 0.26186483079597195, 0.08958533685125357, 0.30321190934270437, 0.01378235951557747, 0.06417451805648788, 0.03208725902824394, 0.07059196986213667, 0.63532772875923, 0.13476648791862456, 0.051339614445190314, 0.12384345540067242, 0.8669041878047069, 0.22751224934584446, 0.7583741644861483, 0.9750064840393022, 0.7696076995694144, 0.13992867264898445, 0.13331504590427445, 0.8532162937873565, 0.9547247722904179, 0.9679370797825123, 0.9260450687408013, 0.07694895351448852, 0.8464384886593737, 0.051741236302092805, 0.05691535993230208, 0.07243773082292992, 0.025870618151046403, 0.3363180359636032, 0.2638803051406733, 0.19661669794795267, 0.9687636943127133, 0.03135630879059954, 0.0940689263717986, 0.06271261758119907, 0.7839077197649883, 0.059643419231352156, 0.20875196730973256, 0.7455427403919019, 0.07948131043786784, 0.181316739436386, 0.06209477377958424, 0.17883294848520262, 0.029805491414200438, 0.12667333851035187, 0.2781845865325374, 0.06457856473076762, 0.009194969569547476, 0.05057233263251112, 0.10574215004979597, 0.07355975655637981, 0.15631448268230708, 0.5241132654642061, 0.05976730220205859, 0.018389939139094952, 0.7832442230702428, 0.12049911124157582, 0.09502234746196954, 0.8552011271577258, 0.6864721910088765, 0.22882406366962546, 0.23225716689819326, 0.08241383341548793, 0.0074921666741352665, 0.31467100031368117, 0.112382500112029, 0.15733550015684059, 0.0899060000896232, 0.036210505245555503, 0.6879995996655546, 0.2534735367188885, 0.11606325211140718, 0.07385843316180457, 0.5803162605570359, 0.21102409474801306, 0.7679039820984157, 0.20207999528905676, 0.8072266827931688, 0.17658083686100567, 0.20064793084032084, 0.7802975088234699, 0.8342805421371134, 0.13904675702285224, 0.1926977496671639, 0.21021572690963336, 0.14014381793975556, 0.43794943106173617, 0.8841022721181458, 0.08841022721181457, 0.7279439896774268, 0.21410117343453727, 0.9217545077901105, 0.8671949221056341, 0.07226624350880284, 0.2399558092615917, 0.028230095207246085, 0.338761142486953, 0.3034735234778954, 0.07763276181992673, 0.8841620849236679, 0.11748885036529166, 0.8224219525570416, 0.9143599074924763, 0.0870818959516644, 0.9764549810439994, 0.9727407442360435, 0.034322074028763236, 0.13728829611505294, 0.23167399969415187, 0.02574155552157243, 0.31747918476605996, 0.2574155552157243, 0.07258606644712551, 0.693600190494755, 0.08871630343537563, 0.1129116589177508, 0.024195355482375173, 0.8970108684211684, 0.0527653452012452, 0.8505617582116791, 0.14176029303527984, 0.9809601665566502, 0.1607650931859169, 0.7234429193366261, 0.1683717935914587, 0.8081846092390018, 0.9079469461145092, 0.8506750503779188, 0.11095761526668507, 0.054277675388851566, 0.521065683732975, 0.021711070155540628, 0.3907992627997313, 0.7134804761224522, 0.24971816664285829, 0.9228036717680962, 0.04614018358840481, 0.30169252652312234, 0.6788081846770253, 0.13428506175360116, 0.13428506175360116, 0.6714253087680059, 0.122030634122057, 0.03050765853051425, 0.122030634122057, 0.7016761462018277, 0.9585340660362202, 0.825022559089712, 0.12692654755226337, 0.18575315462159855, 0.0530723298918853, 0.7430126184863942, 0.5045730795959666, 0.02018292318383867, 0.2825609245737413, 0.2018292318383867, 0.15996437405441494, 0.15996437405441494, 0.6398574962176597, 0.930247687975196, 0.8135918788238049, 0.1525484772794634, 0.09204709652932377, 0.12272946203909837, 0.18409419305864755, 0.5522825791759427, 0.04602354826466189, 0.7143804994001429, 0.15875122208892065, 0.9691915248186845, 0.9287809204017744, 0.027861076496037505, 0.032926726768044326, 0.22288861196830004, 0.27101228955236484, 0.1874290600642523, 0.007598475408010229, 0.09878018030413298, 0.040525202176054556, 0.10891148084814661, 0.7908572861461395, 0.1581714572292279, 0.983628966046717, 0.023140687867122128, 0.9719088904191293, 0.9263711412282014, 0.11886250093453338, 0.8320375065417337, 0.1569061587352886, 0.027288027606137147, 0.38885439338745437, 0.04093204140920572, 0.3820323864859201, 0.8512773833910932, 0.08512773833910932, 0.9398794215893933, 0.07503815144007712, 0.7203662538247405, 0.19509919374420054, 0.04618680622378925, 0.1286632459091272, 0.03958869104896221, 0.2210368583567057, 0.16495287937067588, 0.32330764356652475, 0.052784921398616284, 0.026392460699308142, 0.08184956161603907, 0.28062706839784823, 0.6314109038951585, 0.7679029599057418, 0.16455063426551608, 0.8828333521637982, 0.2184793722035995, 0.7282645740119983, 0.9704762774941045, 0.08330301603515594, 0.9044327455245502, 0.7126964417141923, 0.17817411042854808, 0.26260954808137066, 0.06565238702034266, 0.6565238702034266, 0.9658993218087676, 0.2531833272613711, 0.7257922048159305, 0.15709568647142244, 0.7854784323571122, 0.21218384656634454, 0.7072794885544818, 0.7902967263020081, 0.19757418157550202, 0.07188313678272651, 0.5750650942618121, 0.26956176293522444, 0.07188313678272651, 0.05046198367218659, 0.8242123999790477, 0.11774462856843539, 0.8493459256067277, 0.13410725141158858, 0.90341979629156, 0.8199911149545207, 0.058570793925322905, 0.11714158785064581, 0.9388507805477964, 0.02238945823596149, 0.12687359667044845, 0.3880839427566658, 0.12687359667044845, 0.10448413843448695, 0.12687359667044845, 0.11194729117980745, 0.9799798726444937, 0.8558911488812208, 0.06113508206294434, 0.9428982112243408, 0.8296050249798446, 0.14640088676114904, 0.25974091333611676, 0.6926424355629781, 0.3335400548776856, 0.6300201036578505, 0.12111850082212369, 0.18167775123318555, 0.6964313797272113, 0.9890020301863114, 0.11119026822245455, 0.6166005783245208, 0.19205591783878515, 0.08086564961633058, 0.6794155692959856, 0.30882525877090256, 0.7773595251599394, 0.15547190503198788, 0.9558790035109623, 0.9214653915494506, 0.17233465384334454, 0.24892783332927546, 0.10212423931457454, 0.11488976922889636, 0.0765931794859309, 0.14680359401470092, 0.10212423931457454, 0.025531059828643635, 0.05090418827672869, 0.13008848115163998, 0.1621392663629136, 0.11123507808618492, 0.06975759134218376, 0.4487109929578307, 0.026394764291637098, 0.8959192478952277, 0.040723602177055805, 0.19449671455800893, 0.033342293924230104, 0.28340949835595586, 0.011114097974743367, 0.4779062129139648, 0.13998078727129884, 0.23496775006253734, 0.13498147344018102, 0.2499656915558908, 0.20497186707583046, 0.03499519681782471, 0.8542043453555785, 0.15074194329804327, 0.08732183702276194, 0.8404726813440837, 0.010915229627845243, 0.05457614813922621, 0.03309817593747872, 0.12135997843742198, 0.353047209999773, 0.34201448468728013, 0.1434254290624078, 0.10819773207659716, 0.08114829905744787, 0.7573841245361801, 0.02704943301914929, 0.9718067392866129, 0.1671343923307114, 0.12535079424803355, 0.6685375693228456, 0.22939473005888536, 0.7455328726913775, 0.1200869532110176, 0.37360385443427696, 0.15678018891438408, 0.05337197920489671, 0.26685989602448357, 0.026685989602448355, 0.6579153057527334, 0.315799346761312, 0.04630536340009154, 0.02315268170004577, 0.0810343859501602, 0.02315268170004577, 0.5903933833511672, 0.2315268170004577, 0.2527649273671844, 0.009538299145931487, 0.10015214103228062, 0.615220294912581, 0.021461173078345847, 0.9405486364953066, 0.730052465395465, 0.24335082179848833, 0.1574523270888679, 0.03578461979292452, 0.8015754833615092, 0.004930269255599671, 0.09860538511199342, 0.3253977708695783, 0.5472598873715635, 0.019721077022398684, 0.1161483966478582, 0.842075875696972, 0.16958399399255705, 0.7631279729665067, 0.9064039148450195, 0.9046435630202985, 0.029097281733525566, 0.9020157337392926, 0.05819456346705113, 0.9308504326151504, 0.7077007073669894, 0.14154014147339788, 0.07077007073669894, 0.07914982737962996, 0.8706481011759295, 0.036786864045410705, 0.21152446826111154, 0.009196716011352676, 0.059778654073792396, 0.06437701207946873, 0.27590148034058026, 0.18853267823272987, 0.11955730814758479, 0.03218850603973437, 0.08413422667716336, 0.7992751534330519, 0.08413422667716336, 0.05967031589479855, 0.9248898963693776, 0.1319269872734822, 0.19789048091022332, 0.6596349363674111, 0.13116074344793466, 0.6759822931547402, 0.020178575915066874, 0.1715178952780684, 0.951686481766442, 0.03268816600067639, 0.8172041500169097, 0.13075266400270555, 0.05416089214910328, 0.21664356859641312, 0.6228502597146878, 0.10832178429820656, 0.47696593959005235, 0.4016555280758336, 0.125517352523698, 0.974371167531821, 0.15691562368981826, 0.6904287442352003, 0.03765974968555638, 0.10670262410907641, 0.04847255673459902, 0.11209278744876024, 0.042413487142774146, 0.08482697428554829, 0.37566231469314243, 0.12118139183649755, 0.2120674357138707, 0.3052930460943847, 0.6614682665378335, 0.1866293002714484, 0.03110488337857473, 0.24883906702859784, 0.5287830174357704, 0.02837189728674332, 0.3830206133710348, 0.25534707558068986, 0.09930164050360163, 0.2127892296505749, 0.9477809653942637, 0.05754197763429805, 0.8631296645144708, 0.038264258311884465, 0.34557408287920655, 0.04783032288985558, 0.06337517782905865, 0.05500487132333392, 0.3599231797461632, 0.04424304867311641, 0.0454388067453628, 0.3509826485859879, 0.6381502701563416, 0.9453078096600647, 0.8900400029782424, 0.10349302360212122, 0.12113015689285099, 0.7267809413571059, 0.06729453160713944, 0.06729453160713944, 0.004497158947160108, 0.20686931156936497, 0.11018039420542265, 0.1461576657827035, 0.047220168945181135, 0.33054118261626797, 0.09444033789036227, 0.06296022526024152, 0.9334862256771783, 0.03337640459748083, 0.20470861486454908, 0.10012921379244248, 0.028926217317816718, 0.20025842758488496, 0.12460524383059508, 0.1735573039069003, 0.057852434635633436, 0.08010337103395399, 0.1182644269876887, 0.05543645015047908, 0.13674324370451504, 0.17000511379480251, 0.11087290030095816, 0.2623991973789343, 0.1441347703912456, 0.07393624919076625, 0.14239573918221646, 0.002738379599658009, 0.48743156873912563, 0.05476759199316018, 0.06845948999145023, 0.06845948999145023, 0.10132004518734633, 0.84307989758945, 0.14877880545696176, 0.17591378332356855, 0.08795689166178428, 0.7036551332942742, 0.9669305435993374, 0.899497370034697, 0.881692183484045, 0.038980683905397974, 0.7094484470782432, 0.07016523102971636, 0.1793111459648307, 0.9478446187397833, 0.9535048579490895, 0.03384884690437514, 0.06769769380875028, 0.03384884690437514, 0.06769769380875028, 0.744674631896253, 0.4003942463237852, 0.08657172893487247, 0.43285864467436236, 0.07575026281801342, 0.16547997424554473, 0.772239879812542, 0.9497196659972003, 0.06215594740336597, 0.8701832636471235, 0.04226147152012619, 0.3380917721610095, 0.06339220728018928, 0.5493991297616404, 0.7199932880229244, 0.13333209037461563, 0.13333209037461563, 0.004042534922774217, 0.024255209536645304, 0.3577643406655182, 0.04244661668912928, 0.2708498398258726, 0.01414887222970976, 0.018191407152483977, 0.26680730490309834, 0.24008050726450217, 0.7202415217935065, 0.31940094389749857, 0.10646698129916618, 0.4022085960190723, 0.14589919659515366, 0.023659329177592486, 0.5811427408437534, 0.41648563093802327, 0.855315953765576, 0.04887519735803292, 0.07331279603704938, 0.9297850425583333, 0.006041980096354182, 0.36251880578125095, 0.4289805868411469, 0.05437782086718764, 0.11479762183072946, 0.0362518805781251, 0.9341545911744734, 0.18299488343615627, 0.7319795337446251, 0.09149744171807814, 0.15847237952416193, 0.7923618976208097, 0.14559919810221847, 0.8250621225792379, 0.9096803115857238, 0.9396657005093367, 0.023185516686686662, 0.10433482509008998, 0.8114930840340332, 0.03477827503002999, 0.8770334145054018, 0.06746410880810784, 0.11425926980798705, 0.8569445235599029, 0.9391045235842803, 0.5019758008436741, 0.060237096101240895, 0.24094838440496358, 0.20079032033746966, 0.1609863838799533, 0.8049319193997664, 0.04535335984780387, 0.15117786615934622, 0.09070671969560774, 0.4459747051700714, 0.2645612657788559, 0.8775848594284767, 0.06440989793970472, 0.05635866069724163, 0.6900827123991842, 0.2855514671996624, 0.9123384364578555, 0.15356630721895234, 0.28710222653978046, 0.04006077579624844, 0.04006077579624844, 0.006676795966041407, 0.26707183864165623, 0.08012155159249688, 0.12018232738874532, 0.8938222053841213, 0.09246436607421944, 0.08912114330535682, 0.7638955140459156, 0.14004751090841785, 0.9604915193255454, 0.020908267763219412, 0.01229898103718789, 0.10331144071237829, 0.10577123691981585, 0.033207248800407305, 0.11561042174956616, 0.015988675348344257, 0.5460747580511424, 0.04796602604503277, 0.9395113876149745, 0.9719456314660049, 0.07314670155871142, 0.03657335077935571, 0.012191116926451903, 0.8655693017780851, 0.012191116926451903, 0.168516729837033, 0.7583252842666485, 0.05139413572268273, 0.2055765428907309, 0.7195179001175581, 0.22053134503607882, 0.3255462712437354, 0.042005970483062635, 0.37805373434756373, 0.03150447786229698, 0.26617172097966774, 0.12879276821596825, 0.3348611973615175, 0.16313750640689312, 0.09444803002504339, 0.008586184547731216, 0.17167149901230153, 0.7725217455553568, 0.17307077373985022, 0.10095795134824596, 0.028845128956641704, 0.007211282239160426, 0.07211282239160426, 0.05769025791328341, 0.20191590269649193, 0.3605641119580213, 0.05585501979269201, 0.251347589067114, 0.670260237512304, 0.0365579884563114, 0.9139497114077849, 0.9324572057847512, 0.03431576891333015, 0.07391088689024956, 0.03431576891333015, 0.1953359153528024, 0.11878535393075822, 0.0976679576764012, 0.19797558988459704, 0.20061526441639166, 0.04487446704050866, 0.0052793490635892545, 0.045215975017767245, 0.10464268504111848, 0.08268064003248868, 0.138231695054317, 0.1085183400426414, 0.3526846051385845, 0.016794505006599264, 0.15115054505939338, 0.31303194843153753, 0.6260638968630751, 0.9111169487076144, 0.8860586690521747, 0.10068848511956531, 0.12457376945583729, 0.11324888132348844, 0.24348509484550016, 0.09626154912496518, 0.15854843385288384, 0.09059910505879076, 0.16987332198523267, 0.2979546525670839, 0.0899485743598744, 0.16303179102727233, 0.1236792897448273, 0.25298036538714674, 0.07308321666739795, 0.02408147114140367, 0.12040735570701834, 0.24081471141403668, 0.6020367785350917, 0.02717978442743208, 0.42581662269643594, 0.07247942513981888, 0.3623971256990944, 0.10871913770972833, 0.9463395479670478, 0.9301067537708674, 0.40753159921757054, 0.08859382591686316, 0.03543753036674526, 0.10631259110023579, 0.08859382591686316, 0.26578147775058947, 0.15293802248730712, 0.17403292214072877, 0.1476642975739517, 0.19512782179415045, 0.21622272144757212, 0.11074822318046378, 0.4129096446682162, 0.558642460433469, 0.32759993969473233, 0.6551998793894647, 0.138026209364716, 0.09043096475619324, 0.2903309921119888, 0.3141286144162502, 0.05235476906937503, 0.08091191583448869, 0.02855714676511365, 0.6767266096170299, 0.028196942067376243, 0.28196942067376246, 0.08984991555958632, 0.12440757539019645, 0.09676144752570835, 0.13131910735631847, 0.09676144752570835, 0.03455765983061013, 0.269549746678759, 0.07948261761040329, 0.07948261761040329, 0.033559220739208216, 0.05872863629361438, 0.06711844147841643, 0.8054212977409971, 0.033559220739208216, 0.9642822758832565, 0.7654983714311945, 0.2187138204089127, 0.09212344582344494, 0.8291110124110044, 0.8345934772770881, 0.10432418465963601, 0.1269751403320814, 0.050790056132832564, 0.812640898125321, 0.9143129642091156, 0.15813273222680704, 0.09987330456429919, 0.12484163070537399, 0.14148718146609052, 0.3662021167357637, 0.04993665228214959, 0.05825942766250786, 0.696118540780455, 0.25313401482925635, 0.041947505058228036, 0.020973752529114018, 0.020973752529114018, 0.9018713587519028, 0.9306567243838814, 0.9715563871525652, 0.09443781895057053, 0.8499403705551347, 0.9682364567637323, 0.6790108849092328, 0.28727383592313693, 0.0826718738784781, 0.7991614474919551, 0.11022916517130414, 0.1321466456330999, 0.022024440938849987, 0.6166843462877996, 0.22024440938849985, 0.2565219931674964, 0.7290625068970951, 0.7900424148480731, 0.1700091272457879, 0.030001610690433157, 0.9788582168932353, 0.025343536738861985, 0.13516552927393058, 0.21119613949051655, 0.36325735992368846, 0.11826983811468926, 0.14361337485355125, 0.7348092483530431, 0.18370231208826077, 0.91334535466963, 0.4509174238712868, 0.4086439153833537, 0.05636467798391085, 0.07045584747988856, 0.9184028919942823, 0.1597226540682863, 0.4516295735723958, 0.016523033179477894, 0.03855374408544842, 0.04406142181194105, 0.28639924177761683, 0.3564507041566579, 0.6095823636302266, 0.025829761170772313, 0.09602309908844837, 0.09602309908844837, 0.44010587082205505, 0.07201732431633627, 0.1360327237086352, 0.03200769969614946, 0.03200769969614946, 0.09602309908844837, 0.2340055369310094, 0.04457248322495417, 0.2897211409622021, 0.1560036912873396, 0.22843397652789013, 0.0390009228218349, 0.0686828191139588, 0.7784052832915331, 0.11447136518993134, 0.16357272213783694, 0.09469999702716875, 0.008609090638833522, 0.5854181634406795, 0.13774545022133636, 0.22662072817961929, 0.16560745520818332, 0.06972945482449824, 0.16560745520818332, 0.36607963782861574, 0.1476651451745881, 0.034076571963366484, 0.283971433028054, 0.5225074367716194, 0.2551448624862969, 0.6803862999634585, 0.26706492589035735, 0.15578787343604178, 0.5563852622715778, 0.09745193976986237, 0.14478573908665268, 0.0946675986335806, 0.0779615518158899, 0.11972666886011664, 0.13364837454152553, 0.23945333772023328, 0.03898077590794495, 0.05290248158935386, 0.8662790209842657, 0.10395348251811187, 0.16708119633649313, 0.8019897424151671, 0.9610054752009864, 0.3617313350710756, 0.1736310408341163, 0.04340776020852907, 0.04340776020852907, 0.33279282826538953, 0.04340776020852907, 0.9656680975220296, 0.020478837429138624, 0.06484965185893897, 0.38568477158211073, 0.0921547684311238, 0.22526721172052486, 0.20820151386290933, 0.11982133447554788, 0.2096873353322088, 0.6590173396155133, 0.8281720042619718, 0.13076400067294291, 0.025911165234530416, 0.10364466093812166, 0.3238895654316302, 0.044048980898701705, 0.25911165234530414, 0.09068907832085646, 0.15287587488372945, 0.08110919454920895, 0.8922011400412985, 0.029459384610058398, 0.073648461525146, 0.147296923050292, 0.0883781538301752, 0.13256723074526278, 0.5155392306760219, 0.09706174387054778, 0.6005695401990144, 0.024265435967636945, 0.27905251362782485, 0.14321971285606785, 0.8593182771364071, 0.05192137921451906, 0.34397913729618873, 0.15576413764355718, 0.10384275842903812, 0.29205775808166967, 0.04543120681270418, 0.11835775396285973, 0.0946862031702878, 0.7574896253623024, 0.7047596295861002, 0.17618990739652504, 0.9618719330532568, 0.06875670553704187, 0.8250804664445025, 0.9442549677504056, 0.8964023667528822, 0.154941593353333, 0.309883186706666, 0.058103097507499875, 0.44545708089083236, 0.08741065091389956, 0.11188563316979144, 0.031467834329003845, 0.061187455639729695, 0.06293566865800769, 0.20978556219335895, 0.059439242621451706, 0.17132487579124314, 0.11538205920634742, 0.08915886393217755, 0.6386556470668401, 0.34835762567282186, 0.8523696113614712, 0.07103080094678926, 0.9410053369685512, 0.02214828444387419, 0.09966727999743386, 0.6090778222065403, 0.07751899555355966, 0.11074142221937096, 0.08859313777549677, 0.6990243924009077, 0.18503586857671087, 0.10279770476483938, 0.03704924714381767, 0.8150834371639887, 0.018524623571908833, 0.12967236500336185, 0.11179252774893948, 0.8384439581170462, 0.8336077866464023, 0.06946731555386686, 0.6920906905333007, 0.2811618430291534, 0.21173369093980937, 0.2901535764730721, 0.05489391987328391, 0.2666276108130933, 0.172523748173178, 0.6539987057607426, 0.3269993528803713, 0.9061036857339665, 0.9427292207366958, 0.018584977306489836, 0.06814491679046272, 0.13009484114542885, 0.2911646444683407, 0.4088695007427764, 0.07433990922595934, 0.8298446902367083, 0.10373058627958853, 0.0979328584521338, 0.889556797606882, 0.984555976715511, 0.8646287422216925, 0.08646287422216925, 0.061091366339652155, 0.0021065988392983503, 0.025279186071580202, 0.12007613384000596, 0.29492383750176904, 0.061091366339652155, 0.01053299419649175, 0.3117766282161558, 0.09690354660772411, 0.014746191875088452, 0.13052885998068747, 0.7831731598841248, 0.21694038520330164, 0.1753986093133077, 0.08769930465665385, 0.28156092547662553, 0.1846301150666397, 0.05538903451999191, 0.9357905768326475, 0.19495886890593614, 0.32753089976197275, 0.10917696658732425, 0.2729424164683106, 0.08578190231861191, 0.05777930264367838, 0.36593558340996307, 0.13481837283524956, 0.44297465360153426, 0.05294486355959053, 0.2411932673270235, 0.17060011591423616, 0.023531050470929123, 0.11177248973691334, 0.3412002318284723, 0.05882762617732281, 0.03508777994405455, 0.11228089582097456, 0.6736853749258473, 0.07719311587692, 0.10526333983216364, 0.9704197316900078, 0.09424049652884194, 0.15314080685936815, 0.023560124132210484, 0.14725077582631552, 0.12958068272715767, 0.19437102409073648, 0.029450155165263104, 0.2238211792559996, 0.170425760034032, 0.06492409906058362, 0.47069971818923123, 0.08115512382572952, 0.05680858667801067, 0.1541947352688861, 0.8728767209441007, 0.07935242917673643, 0.9148966968074744, 0.7912399713160949, 0.15824799426321898, 0.916349543489938, 0.5933146395038641, 0.08475923421483772, 0.1130123122864503, 0.19777154650128803, 0.8949978645939665, 0.8980779461453225, 0.2482064988894112, 0.039487397550588145, 0.09589796547999978, 0.07333373830823513, 0.42872031626352847, 0.11282113585882328, 0.9401623087487141, 0.19964197991871066, 0.026618930655828085, 0.21295144524662468, 0.11978518795122639, 0.03992839598374213, 0.12643992061518342, 0.04658312864769915, 0.22626091057453873, 0.032475023783168175, 0.16237511891584086, 0.6495004756633634, 0.14613760702425677, 0.10076032764933168, 0.8564627850193193, 0.8333317030046402, 0.09803902388289885, 0.6196140967096178, 0.026366557306792243, 0.039549835960188365, 0.3032154090281108, 0.13523216362224647, 0.4462661399534133, 0.06761608181112323, 0.31103397633116686, 0.04056964908667394, 0.012574181405912738, 0.10373699659878008, 0.14931840419521375, 0.05029672562365095, 0.1791820850342565, 0.11159585997747555, 0.30806744444486206, 0.03615077154199912, 0.048724952947911855, 0.945381511077374, 0.07044852004255438, 0.8453822405106525, 0.0969310782373838, 0.02643574861019558, 0.519903056000513, 0.083713203932286, 0.2731694023053543, 0.956003655573423, 0.6996865708226053, 0.14730243596265377, 0.14730243596265377, 0.13142006295933908, 0.8542304092357041, 0.7926353207594591, 0.18117378760216207, 0.14798147538284392, 0.8138981146056415, 0.09962932307195684, 0.6309857127890599, 0.23246842050123262, 0.06641954871463789, 0.07829652322725483, 0.8612617554998031, 0.5306040777176513, 0.30320233012437214, 0.17686802590588377, 0.9541336647583507, 0.2360327173104686, 0.07867757243682287, 0.629420579494583, 0.9803474154730116, 0.9212004442265394, 0.04525128794664304, 0.13575386383992913, 0.7692718950929317, 0.7205583777332089, 0.21616751331996267, 0.09504032652076196, 0.09504032652076196, 0.7603226121660956, 0.0889367509186971, 0.010672410110243653, 0.12451145128617594, 0.6225572564308797, 0.03557470036747884, 0.003557470036747884, 0.09960916102894075, 0.014229880146991536, 0.9401284621094022, 0.030326724584174264, 0.8186053767587969, 0.0818605376758797, 0.09347265150979708, 0.051929250838776156, 0.0986655765936747, 0.4985208080522511, 0.07789387625816423, 0.17655945285183894, 0.3375120165613334, 0.008881895172666668, 0.07993705655400002, 0.06217326620866668, 0.5062680248420001, 0.9277458723811353, 0.03702691726208328, 0.43691762369258264, 0.01481076690483331, 0.0814592179765832, 0.04443230071449993, 0.23697227047733296, 0.14070228559591644, 0.003509515410875052, 0.014038061643500207, 0.05966176198487588, 0.11230449314800166, 0.2105709246525031, 0.031585638697875465, 0.34744202567663013, 0.14038061643500208, 0.08071885445012619, 0.7088607652204915, 0.2531645590073184, 0.9368768272637975, 0.05405058618829601, 0.0716162917946993, 0.12532851064072378, 0.716162917946993, 0.0716162917946993, 0.7107312505808943, 0.2369104168602981, 0.9699657171629885, 0.7313196835629135, 0.22853740111341048, 0.9912214818730005, 0.935065149705075, 0.026716147134430716, 0.15346166472620965, 0.8184622118731181, 0.9470263459432264, 0.7111958145618804, 0.1777989536404701, 0.03698255345573288, 0.09862014254862103, 0.7396510691146577, 0.12327517818577628, 0.2744476654869479, 0.6861191637173698, 0.9341242810042554, 0.02880413885943247, 0.9505365823612715, 0.1811955254273836, 0.7247821017095344, 0.8063649491800878, 0.0895961054644542, 0.0934858945956964, 0.6544012621698747, 0.0934858945956964, 0.17938639297778097, 0.17938639297778097, 0.6054290763000109, 0.02242329912222262, 0.1554055056820252, 0.08158789048306324, 0.0388513764205063, 0.3652029383527593, 0.17094605625022774, 0.09324330340921513, 0.06216220227281009, 0.027195963494354415, 0.03740624246092913, 0.8728123240883463, 0.08728123240883463, 0.8669449172952994, 0.07224540977460829, 0.04596553479739926, 0.7354485567583882, 0.18386213918959704, 0.8733635881819501, 0.06718181447553463, 0.9062419459632485, 0.07617079940131914, 0.06855371946118723, 0.030468319760527655, 0.8150275535941148, 0.9544177294912388, 0.41586054099113795, 0.14756341777104895, 0.06707428080502224, 0.1341485616100445, 0.21463769857607118, 0.028244460184055714, 0.2589075516871774, 0.07531856049081524, 0.23537050153379763, 0.37188539242340024, 0.028244460184055714, 0.05519444467229958, 0.02759722233614979, 0.012265432149399907, 0.16251697597954876, 0.21464506261449837, 0.4384891993410467, 0.0061327160746999535, 0.07972530897109939, 0.03531551690154318, 0.08828879225385795, 0.6356793042277772, 0.07945991302847215, 0.1589198260569443, 0.976068576197761, 0.11125443104524506, 0.024723206898943344, 0.791142620766187, 0.06180801724735836, 0.8242605653342541, 0.09891126784011049, 0.032970422613370166, 0.03459984964247252, 0.4930478574052334, 0.14272437977519914, 0.0735246804902541, 0.2075990978548351, 0.04324981205309065, 0.9526032336505356, 0.09746278851340026, 0.08832565209026899, 0.1309656220648816, 0.15533131919323168, 0.14010275848801287, 0.2345198348603694, 0.09746278851340026, 0.05786853067983141, 0.08794274160128056, 0.051299932600746986, 0.07328561800106713, 0.6449134384093907, 0.13924267420202754, 0.9498011683197579, 0.11076613749379062, 0.7753629624565342, 0.05538306874689531, 0.08307460312034295, 0.736305977432629, 0.18407649435815726, 0.046019123589539315, 0.06891707083071787, 0.18952194478447412, 0.482419495815025, 0.2412097479075125, 0.15893294416646314, 0.8343979568739315, 0.4932580792527993, 0.09134408875051839, 0.40191399050228094, 0.9156176092828808, 0.9473666632559158, 0.36831488552443004, 0.5839138429045841, 0.035933159563359025, 0.9871915031112868, 0.4914970596381084, 0.5019544438857277, 0.2152433585453751, 0.7533517549088129, 0.9837587423251084, 0.06401908626911565, 0.05201550759365646, 0.14004175121369047, 0.05601670048547619, 0.23206918772554422, 0.13203936543005101, 0.016004771567278912, 0.2160644161582653, 0.08402505072821428, 0.022677832771867707, 0.06349793176122957, 0.13153143007683268, 0.2857406929255331, 0.08617576453309728, 0.4082009898936187, 0.015577343078942128, 0.07269426770172993, 0.031154686157884255, 0.20769790771922836, 0.5036674262191287, 0.031154686157884255, 0.13500364001749843, 0.1524406996972053, 0.787610281768894, 0.0508135665657351, 0.9539081074678194, 0.9150742252746196, 0.08936194538303917, 0.8042575084473524, 0.9591140527456612, 0.28008963112924795, 0.672215114710195, 0.2897074230488328, 0.7081737007860357, 0.09537997368428043, 0.1165755233918983, 0.21195549707617872, 0.39211766959093064, 0.16956439766094297, 0.02119554970761787, 0.1132926779202013, 0.3050187482466958, 0.5751782109794835, 0.08810025878526083, 0.10697888566781671, 0.22025064696315205, 0.3335224082584874, 0.16361476631548438, 0.08810025878526083, 0.040322623631592, 0.0075604919309235005, 0.131048526802674, 0.0327621317006685, 0.17893164236518952, 0.15625016657241902, 0.30241967723694, 0.075604919309235, 0.070564591355286, 0.004991376663356588, 0.04741807830188759, 0.15473267656405423, 0.06488789662363564, 0.06488789662363564, 0.4442325230387363, 0.0823577149453837, 0.13476716991062787, 0.04002487701711854, 0.0880547294376608, 0.2241393112958638, 0.6243880814670493, 0.016009950806847417, 0.11446498248235999, 0.09953476737596521, 0.3881855927662643, 0.1891160580143339, 0.18413931964553565, 0.014930215106394782, 0.04849687978736539, 0.9214407159599424, 0.4858773719537062, 0.47115381522783634, 0.02944711345173977, 0.8941160808014502, 0.03568199889835745, 0.11989151629848102, 0.09562775704759796, 0.057091198237371917, 0.23550119272915915, 0.0727912777526492, 0.28402871123092527, 0.0713639977967149, 0.02711831916275166, 0.047405416671219926, 0.07110812500682988, 0.047405416671219926, 0.8177434375785436, 0.06358393127195416, 0.023121429553437878, 0.046242859106875756, 0.052023216495235225, 0.40462501718516286, 0.03468214433015682, 0.34682144330156817, 0.03468214433015682, 0.01300931709147404, 0.042280280547290626, 0.6081855740264113, 0.035775622001553604, 0.00650465854573702, 0.18538276855350505, 0.10732686600466082, 0.084195294194971, 0.84195294194971, 0.9117833234365003, 0.8483235236579324, 0.09425816929532582, 0.09939418988093887, 0.019878837976187773, 0.5764863013094454, 0.2584248936904411, 0.039757675952375546, 0.03347871205354359, 0.03347871205354359, 0.903925225445677, 0.9869303907118422, 0.07152965384169575, 0.09103774125306732, 0.409669835638803, 0.26661052795541146, 0.09103774125306732, 0.07152965384169575, 0.8815804667153984, 0.8814524401172573, 0.6156692158050757, 0.05353645354826746, 0.26768226774133724, 0.05353645354826746, 0.23907914759533314, 0.6906730930531846, 0.053128699465629585, 0.5021688914349084, 0.03862837626422372, 0.4249121389064609, 0.03862837626422372, 0.9557844519287044, 0.14318701631974393, 0.11454961305579514, 0.28637403263948785, 0.07159350815987196, 0.30785208508744943, 0.07159350815987196, 0.9356213380494428, 0.014567607900762385, 0.05098662765266835, 0.036419019751905965, 0.8959078858968866, 0.03454556053046051, 0.0740262011367011, 0.1825979628038627, 0.15792256242496233, 0.1825979628038627, 0.17272780265230256, 0.19246812295542284, 0.12760237452659767, 0.03645782129331362, 0.045572276616642024, 0.26431920437652373, 0.09114455323328405, 0.09114455323328405, 0.3554637576098078, 0.08582596474565846, 0.8582596474565846, 0.9610159998370637, 0.11542186350695682, 0.8656639763021762, 0.07704096746423537, 0.09854077233797548, 0.10929067477484553, 0.012541552843015061, 0.619911040526173, 0.06449941462122032, 0.017916504061450088, 0.017917489620588384, 0.017917489620588384, 0.0836149515627458, 0.5793321643990245, 0.12542242734411868, 0.1313949238843148, 0.0418074757813729, 0.3357329226957811, 0.06104234958105111, 0.6104234958105111, 0.07145613261678112, 0.06788332598594206, 0.14291226523356224, 0.06788332598594206, 0.12504823207936694, 0.12504823207936694, 0.3144069835138369, 0.01786403315419528, 0.064310519355103, 0.05681270740098267, 0.9090033184157227, 0.05514396182560417, 0.05514396182560417, 0.8271594273840625, 0.07394072844688948, 0.3031569866322468, 0.03697036422344474, 0.08872887413626737, 0.22921625818535737, 0.2661866224088021, 0.24207180828641642, 0.07262154248592492, 0.6535938823733243, 0.9627957492472253, 0.07268380745054381, 0.06460782884492783, 0.08883576466177577, 0.13325364699266365, 0.23824136886567138, 0.07268380745054381, 0.032303914422463914, 0.044417882330887885, 0.24631734747128736, 0.034214842629344384, 0.04399051195201421, 0.7038481912322273, 0.08798102390402841, 0.1270837011947077, 0.06820375757009658, 0.06820375757009658, 0.7275067474143636, 0.045469171713397726, 0.09093834342679545, 0.675776866856315, 0.29668252691252855, 0.016482362606251588, 0.002542747345131205, 0.12459461991142906, 0.35598462831836875, 0.12459461991142906, 0.00508549469026241, 0.05848318893801772, 0.08391066238932977, 0.15256484070787232, 0.06356868362828012, 0.027970220796443256, 0.0391268163544239, 0.0782536327088478, 0.4773471595239716, 0.01565072654176956, 0.01565072654176956, 0.17215799195946518, 0.17998335523034995, 0.01565072654176956, 0.8055658213010911, 0.089507313477899, 0.00978705901243739, 0.07829647209949912, 0.04893529506218695, 0.3914823604974956, 0.35233412444774603, 0.10765764913681128, 0.9522044032995708, 0.0013642455434758065, 0.1064111523911129, 0.1241463444562984, 0.13233181771715322, 0.169166447391, 0.20736532260832258, 0.015006700978233872, 0.05866255836945968, 0.038198875217322584, 0.1446100276084355, 0.9806044304118805, 0.4447860705112645, 0.13343582115337935, 0.17791442820450581, 0.22239303525563225, 0.09118560474699926, 0.2127664110763316, 0.11651493939894349, 0.05065866930388847, 0.2583592134498312, 0.055724536234277325, 0.2127664110763316, 0.05994767622063089, 0.03155140853717415, 0.07572338048921796, 0.3975477475683943, 0.1703776061007404, 0.0031551408537174148, 0.14198133841728366, 0.11989535244126177, 0.11293714321479077, 0.19764000062588385, 0.08470285741109307, 0.5646857160739538, 0.11467367292690915, 0.802715710488364, 0.06678147200677197, 0.9015498720914215, 0.08543670793091446, 0.8543670793091446, 0.9498718721947678], \"Term\": [\"ability\", \"ability\", \"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"absolute\", \"absolute\", \"absolute\", \"abuse\", \"abuse\", \"abuse\", \"abuse\", \"abuse\", \"abuse\", \"abused\", \"abused\", \"abuser\", \"abuser\", \"accepted\", \"accepted\", \"acts\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"admit\", \"admit\", \"adults\", \"adults\", \"advice\", \"advice\", \"advice\", \"affect\", \"afford\", \"afford\", \"afford\", \"afraid\", \"afraid\", \"afraid\", \"afraid\", \"afterwards\", \"ago\", \"ago\", \"ago\", \"ago\", \"ago\", \"ago\", \"agreed\", \"agreed\", \"agreed\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"always\", \"always\", \"always\", \"always\", \"always\", \"always\", \"always\", \"annoying\", \"anonymous\", \"answers\", \"anxiety\", \"anxiety\", \"anxious\", \"anxious\", \"anxious\", \"anxious\", \"anyone\", \"anyone\", \"anyone\", \"anyone\", \"anyone\", \"anyone\", \"anyone\", \"anyone\", \"anything\", \"anything\", \"anything\", \"anything\", \"anything\", \"anything\", \"anything\", \"anything\", \"anything\", \"apartment\", \"apartment\", \"apartment\", \"app\", \"app\", \"april\", \"area\", \"area\", \"arm\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"art\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"asked\", \"asked\", \"asked\", \"asked\", \"asked\", \"asked\", \"asks\", \"asks\", \"asleep\", \"asleep\", \"assault\", \"asshole\", \"asshole\", \"assistance\", \"assistance\", \"assumed\", \"attacks\", \"aunt\", \"awareness\", \"awareness\", \"away\", \"away\", \"away\", \"away\", \"away\", \"away\", \"away\", \"awesome\", \"awful\", \"awful\", \"awful\", \"awful\", \"baby\", \"baby\", \"baby\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bar\", \"bar\", \"believed\", \"believed\", \"benefit\", \"benefit\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best_friend\", \"best_friend\", \"best_friend\", \"better\", \"better\", \"better\", \"better\", \"beyond\", \"beyond\", \"bf\", \"bf\", \"bills\", \"bills\", \"birth\", \"birth\", \"body\", \"body\", \"body\", \"body\", \"book\", \"book\", \"boss\", \"boss\", \"bottle\", \"bottom\", \"bottom\", \"boyfriend\", \"boyfriend\", \"boyfriend\", \"boyfriend\", \"boyfriend\", \"breath\", \"breathe\", \"breathe\", \"breathing\", \"breathing\", \"brothers\", \"buy\", \"call\", \"call\", \"call\", \"call\", \"call\", \"call\", \"came\", \"came\", \"came\", \"came\", \"came\", \"cancer\", \"cancer\", \"car\", \"car\", \"card\", \"carry\", \"carry\", \"certain\", \"certain\", \"charge\", \"cheated\", \"cheated\", \"child\", \"child\", \"child\", \"child\", \"choose\", \"choose\", \"chronic\", \"chronic\", \"city\", \"city\", \"clean\", \"clean\", \"clean\", \"clear\", \"clear\", \"clear\", \"clear\", \"click\", \"closer\", \"closer\", \"community\", \"community\", \"community\", \"complete\", \"complete\", \"complete\", \"complete\", \"computer\", \"computer\", \"computer\", \"connect\", \"consider\", \"consider\", \"control\", \"control\", \"control\", \"control\", \"control\", \"convinced\", \"convinced\", \"cost\", \"couch\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"couple_months\", \"couple_months\", \"credit\", \"cry\", \"cry\", \"cute\", \"cycle\", \"cycle\", \"dad\", \"dad\", \"dad\", \"dad\", \"dad\", \"dated\", \"dated\", \"dates\", \"dating\", \"dating\", \"dating\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"deal\", \"deal\", \"deal\", \"debt\", \"debt\", \"decisions\", \"denied\", \"denied\", \"department\", \"depression\", \"depression\", \"designed\", \"designed\", \"despite\", \"despite\", \"despite\", \"determined\", \"diagnosed\", \"diagnosed\", \"diagnosis\", \"diagnosis\", \"directly\", \"directly\", \"dm\", \"dm\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"dog\", \"dog\", \"dog\", \"dogs\", \"dogs\", \"dollar\", \"domestic_violence\", \"domestic_violence\", \"domestic_violence\", \"donate\", \"done\", \"done\", \"done\", \"done\", \"done\", \"done\", \"done\", \"dont\", \"doubt\", \"doubt\", \"dreams\", \"drinks\", \"drinks\", \"dropped\", \"dropped\", \"drug\", \"drug\", \"drugs\", \"drugs\", \"drugs\", \"drunk\", \"due\", \"due\", \"due\", \"due\", \"email\", \"email\", \"emdr\", \"emdr\", \"empty\", \"english\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"event\", \"event\", \"every\", \"every\", \"every\", \"every\", \"every\", \"everything\", \"everything\", \"everything\", \"everything\", \"everything\", \"everything\", \"evidence\", \"evidence\", \"ex\", \"ex\", \"ex\", \"ex\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"experienced\", \"experienced\", \"experienced\", \"experienced\", \"experiencing\", \"extra\", \"extra\", \"extra\", \"eye\", \"eye\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"fault\", \"fault\", \"fear\", \"fear\", \"fear\", \"fear\", \"fear\", \"fear\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel_free\", \"feel_guilty\", \"feel_guilty\", \"feel_like\", \"feel_like\", \"feel_like\", \"feeling\", \"feeling\", \"feeling\", \"feeling\", \"feeling\", \"feels_like\", \"feels_like\", \"figured\", \"figured\", \"fill\", \"finances\", \"finding\", \"finding\", \"finding\", \"finished\", \"fire\", \"fire\", \"fire\", \"fired\", \"fired\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"five\", \"five\", \"five\", \"flashbacks\", \"flashbacks\", \"following\", \"following\", \"following\", \"food\", \"food\", \"food\", \"food\", \"forever\", \"form\", \"form\", \"form\", \"forward\", \"forward\", \"forward\", \"forward\", \"four\", \"four\", \"four\", \"freaking\", \"friend\", \"friend\", \"friend\", \"friend\", \"friends\", \"friends\", \"friends\", \"friends\", \"friends\", \"friends\", \"friends\", \"friendship\", \"friendship\", \"fucking\", \"fucking\", \"fucking\", \"fucking\", \"full\", \"full\", \"full\", \"full\", \"full\", \"funds\", \"gas\", \"gas\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"gf\", \"gf\", \"gift\", \"girl\", \"girl\", \"girlfriend\", \"girlfriend\", \"girlfriend\", \"girlfriend\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"goal\", \"going\", \"going\", \"going\", \"going\", \"going\", \"going\", \"going\", \"going\", \"going\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"government\", \"government\", \"grabbed\", \"grabbed\", \"grabbed\", \"grandma\", \"greatly\", \"groceries\", \"guy\", \"guy\", \"guy\", \"guy\", \"gym\", \"hair\", \"hang\", \"hang\", \"hang\", \"hang\", \"hang\", \"hate\", \"hate\", \"hate\", \"hate\", \"hates\", \"hates\", \"headaches\", \"healing\", \"healing\", \"heard\", \"heard\", \"heard\", \"heard\", \"hello\", \"hello\", \"hello\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"hitting\", \"hitting\", \"home\", \"home\", \"home\", \"home\", \"home\", \"homeless\", \"homeless\", \"horrible\", \"horrible\", \"horrible\", \"hot\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"housing\", \"human\", \"human\", \"human\", \"hurts\", \"hurts\", \"idk\", \"idk\", \"ignoring\", \"illness\", \"im\", \"im\", \"im\", \"im\", \"improve\", \"improve\", \"income\", \"income\", \"intense\", \"interested\", \"interested\", \"interested\", \"interested\", \"interview\", \"interview\", \"issues\", \"issues\", \"issues\", \"issues\", \"issues\", \"job\", \"job\", \"job\", \"jobs\", \"jobs\", \"july\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keeps\", \"keeps\", \"knew\", \"knew\", \"knew\", \"knife\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"language\", \"laptop\", \"later\", \"later\", \"later\", \"later\", \"later\", \"laundry\", \"laundry\", \"lease\", \"lease\", \"lease\", \"least\", \"least\", \"least\", \"least\", \"least\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"legs\", \"legs\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let_know\", \"let_know\", \"let_know\", \"level\", \"level\", \"lexapro\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"liked\", \"liked\", \"limited\", \"link\", \"link\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"live\", \"live\", \"live\", \"live\", \"live\", \"live\", \"lived\", \"lived\", \"lived\", \"lived\", \"living\", \"living\", \"living\", \"living\", \"living\", \"loan\", \"loans\", \"longer\", \"longer\", \"longer\", \"longer\", \"longer\", \"longer\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"loves\", \"loves\", \"mad\", \"mad\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"major\", \"major\", \"major\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"makes\", \"makes\", \"makes\", \"makes\", \"makes\", \"management\", \"manager\", \"manager\", \"manipulative\", \"manipulative\", \"many_times\", \"many_times\", \"married\", \"married\", \"married\", \"match\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"meaning\", \"meaning\", \"medication\", \"medication\", \"medication\", \"medication\", \"medications\", \"meds\", \"member\", \"member\", \"members\", \"memories\", \"memories\", \"men\", \"men\", \"men\", \"mental\", \"mental\", \"mental\", \"mental\", \"mental_health\", \"mental_health\", \"met\", \"met\", \"met\", \"mg\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"military\", \"military\", \"minimum\", \"minutes\", \"minutes\", \"minutes\", \"minutes\", \"mistake\", \"mom\", \"mom\", \"mom\", \"mom\", \"mom\", \"mom\", \"money\", \"money\", \"money\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"months\", \"months\", \"months\", \"months\", \"months\", \"months\", \"months_ago\", \"months_ago\", \"months_ago\", \"mother\", \"mother\", \"mother\", \"mother\", \"mother\", \"move\", \"move\", \"move\", \"move\", \"move\", \"moved\", \"moved\", \"moved\", \"moved\", \"movie\", \"movie\", \"moving\", \"moving\", \"moving\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"mum\", \"mum\", \"music\", \"music\", \"mutual\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"nbsp\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"negative\", \"negative\", \"negative\", \"nervous\", \"nervous\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"new_job\", \"new_job\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"night\", \"night\", \"night\", \"night\", \"nightmares\", \"nightmares\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"number\", \"number\", \"number\", \"obvious\", \"obvious\", \"occasionally\", \"occasions\", \"occasions\", \"ocd\", \"odd\", \"older\", \"older\", \"older\", \"older\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"online\", \"online\", \"opened\", \"opened\", \"original\", \"others\", \"others\", \"others\", \"others\", \"others\", \"others\", \"paid\", \"paid\", \"paid\", \"pain\", \"pain\", \"pain\", \"pain\", \"painful\", \"painful\", \"panic\", \"panic\", \"panic_attacks\", \"panic_attacks\", \"parents\", \"parents\", \"parents\", \"parents\", \"parents\", \"participate\", \"participate\", \"participation\", \"particular\", \"past\", \"past\", \"past\", \"past\", \"past\", \"past\", \"patient\", \"patient\", \"pay\", \"pay\", \"paying\", \"payments\", \"payments\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"per\", \"per\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"perspective\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"physically\", \"physically\", \"physically\", \"physically\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place\", \"please\", \"please\", \"please\", \"please\", \"please\", \"pm\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"post\", \"post\", \"post\", \"post\", \"post\", \"post\", \"pregnant\", \"pregnant\", \"prior\", \"project\", \"project\", \"proud\", \"provide\", \"provide\", \"provide\", \"provide\", \"psychological\", \"psychology\", \"ptsd\", \"ptsd\", \"ptsd\", \"ptsd\", \"ptsd\", \"ptsd\", \"purpose\", \"put\", \"put\", \"put\", \"put\", \"put\", \"put\", \"put\", \"put\", \"questions\", \"questions\", \"questions\", \"questions\", \"quick\", \"quick\", \"random\", \"random\", \"read\", \"read\", \"read\", \"read\", \"reading\", \"reading\", \"reading\", \"reading\", \"reading\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"receive\", \"red\", \"red\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relevant\", \"remember\", \"remember\", \"remember\", \"rent\", \"rent\", \"research\", \"research\", \"responded\", \"responded\", \"response\", \"response\", \"response\", \"response\", \"responses\", \"responses\", \"rest\", \"rest\", \"rest\", \"restraining\", \"result\", \"result\", \"result\", \"return\", \"rock\", \"romantic\", \"romantic\", \"romantic\", \"ruin\", \"ruin\", \"safety\", \"safety\", \"safety\", \"said\", \"said\", \"said\", \"said\", \"said\", \"said\", \"said\", \"said\", \"save\", \"save\", \"saving\", \"saving\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"scared\", \"scared\", \"scared\", \"scared\", \"scared\", \"scary\", \"school\", \"school\", \"school\", \"school\", \"school\", \"school\", \"school\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"seeking\", \"seeking\", \"seemed\", \"seemed\", \"seen\", \"seen\", \"seen\", \"seen\", \"series\", \"series\", \"session\", \"sessions\", \"sessions\", \"sex\", \"sexual\", \"sexual\", \"shake\", \"shake\", \"shaking\", \"shame\", \"shame\", \"share\", \"share\", \"share\", \"share\", \"shared\", \"shared\", \"sharing\", \"shelter\", \"shelter\", \"shortly\", \"shortly\", \"shy\", \"shy\", \"silent\", \"silent\", \"silent\", \"similar\", \"similar\", \"similar\", \"similar\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"sister\", \"sister\", \"sister\", \"site\", \"site\", \"situations\", \"situations\", \"situations\", \"skills\", \"skills\", \"skin\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sobbing\", \"social\", \"social\", \"social\", \"social\", \"social\", \"someone\", \"someone\", \"someone\", \"someone\", \"someone\", \"someone\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"sometimes\", \"sometimes\", \"sometimes\", \"sometimes\", \"sometimes\", \"son\", \"sorry\", \"sorry\", \"sorry\", \"sorry\", \"speak\", \"speak\", \"speak\", \"started\", \"started\", \"started\", \"started\", \"started\", \"started\", \"stayed\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"storage\", \"store\", \"store\", \"store\", \"store\", \"stories\", \"stories\", \"stories\", \"story\", \"story\", \"story\", \"story\", \"struggle\", \"struggle\", \"study\", \"study\", \"study\", \"subject\", \"suffering\", \"support\", \"support\", \"support\", \"surgery\", \"survey\", \"survey\", \"survivors\", \"survivors\", \"symptoms\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tells\", \"tells\", \"tells\", \"tend\", \"terms\", \"tests\", \"tests\", \"texted\", \"texts\", \"texts\", \"thank\", \"thank\", \"therapist\", \"therapist\", \"therapist\", \"therapist\", \"therapist\", \"therapist\", \"therapy\", \"therapy\", \"therapy\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"things\", \"things\", \"things\", \"things\", \"things\", \"things\", \"things\", \"things\", \"things\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"thinking\", \"thinking\", \"thinking\", \"thinking\", \"thinking\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"threatened\", \"threatened\", \"three\", \"three\", \"three\", \"throat\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tired\", \"tired\", \"tired\", \"tired\", \"together\", \"together\", \"together\", \"together\", \"together\", \"together\", \"together\", \"together\", \"told\", \"told\", \"told\", \"told\", \"told\", \"told\", \"told\", \"topic\", \"topic\", \"training\", \"transfer\", \"transfer\", \"trauma\", \"trauma\", \"trauma\", \"trauma\", \"trauma\", \"traumatic\", \"traumatic\", \"traumatic\", \"treatment\", \"tried\", \"tried\", \"tried\", \"tried\", \"tried\", \"tried\", \"tries\", \"triggering\", \"true\", \"true\", \"true\", \"true\", \"truly\", \"truly\", \"truly\", \"trust\", \"trust\", \"trust\", \"trust\", \"turning\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"unemployed\", \"url\", \"url\", \"url\", \"url\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"vacation\", \"vacation\", \"vet\", \"wake\", \"wake\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"wanted\", \"wanted\", \"wanted\", \"wanted\", \"wanted\", \"wanted\", \"wanted\", \"water\", \"water\", \"water\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"wear\", \"wear\", \"weed\", \"weed\", \"weed\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"weekend\", \"weekend\", \"weekend\", \"welcome\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"went\", \"went\", \"went\", \"went\", \"went\", \"whenever\", \"whenever\", \"whenever\", \"whenever\", \"whenever\", \"wish\", \"wish\", \"wish\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"working\", \"working\", \"working\", \"working\", \"working\", \"working\", \"working\", \"working\", \"worries\", \"worries\", \"worse\", \"worse\", \"worse\", \"worse\", \"worse\", \"worse\", \"worthless\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"write\", \"writing\", \"writing\", \"writing\", \"writing\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years_old\", \"years_old\", \"years_old\", \"years_old\", \"yell\", \"yell\", \"younger\", \"younger\", \"yrs\", \"yrs\", \"zoloft\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el781401096980457767022966737\", ldavis_el781401096980457767022966737_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el781401096980457767022966737\", ldavis_el781401096980457767022966737_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el781401096980457767022966737\", ldavis_el781401096980457767022966737_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bigram(df):\n",
        "    \"\"\"\n",
        "    For the test data we only need the bigram data. This is a requirement due to \n",
        "    the shapes Gensim functions expect in the test-vector transformation below.\n",
        "    With both these in hand, we can make the test corpus.\n",
        "    \"\"\"\n",
        "    df['text'] = df.text.str.strip() \n",
        "    words = list(sent_to_words(df['text']))\n",
        "    words = remove_stopwords(words)\n",
        "    bigram = bigrams(words)\n",
        "    bigram = [bigram[r] for r in words]\n",
        "    return bigram\n",
        "\n",
        "bigram_test = get_bigram(test)\n",
        "\n",
        "test_corpus = [train_id2word.doc2bow(text) for text in bigram_test]"
      ],
      "metadata": {
        "id": "xeMFyqpek7D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec embedding"
      ],
      "metadata": {
        "id": "8HmbrwMi4SU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## word2vec embedding\n",
        "\n",
        "w2v_X_train = train['text_clean2']\n",
        "w2v_X_test = test['text_clean2'] \n",
        "w2v_y_train = train['label']\n",
        "w2v_y_test = test['label']\n",
        "\n",
        "# Train the word2vec model\n",
        "w2v_model = gensim.models.Word2Vec(w2v_X_train,\n",
        "                                   size=300,\n",
        "                                   window=5,\n",
        "                                   min_count=2)\n",
        "\n",
        "w2v_words = set(w2v_model.wv.index2word) #index2word\n",
        "w2v_X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in w2v_words])\n",
        "                         for ls in w2v_X_train])\n",
        "w2v_X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in w2v_words])\n",
        "                         for ls in w2v_X_test])\n",
        "\n",
        "w2v_X_train_vect_avg = []\n",
        "for v in w2v_X_train_vect:\n",
        "    if v.size:\n",
        "        w2v_X_train_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        w2v_X_train_vect_avg.append(np.zeros(300, dtype=float))\n",
        "        \n",
        "w2v_X_test_vect_avg = []\n",
        "for v in w2v_X_test_vect:\n",
        "    if v.size:\n",
        "        w2v_X_test_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        w2v_X_test_vect_avg.append(np.zeros(300, dtype=float))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q1l50FBFX8J",
        "outputId": "b781d2ff-3139-424f-b368-85e9cebbf266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gloVe embedding"
      ],
      "metadata": {
        "id": "DSq5tGCK6QZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_x_train = train['text_clean2']\n",
        "glove_y_train = train['label']\n",
        "glove_x_test = test['text_clean2']\n",
        "glove_y_test = test['label']\n",
        "\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(glove_x_train)\n",
        "seq = token.texts_to_sequences(glove_x_train)\n",
        "\n",
        "pad_seq = pad_sequences(seq,maxlen=300)\n",
        "\n",
        "vocab_size = len(token.word_index)+1"
      ],
      "metadata": {
        "id": "CMHSyXedHZRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(glove_x_train)\n",
        "\n",
        "words_to_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "Zi3OmQvGH7Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loading_embeddings():\n",
        "    embeddings_index = {}\n",
        "    f = open('glove.6B.100d.txt', encoding='utf8') # loading the file\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "    return embeddings_index\n",
        "\n",
        "def prepare_embedding_matrix(word_index):\n",
        "    embeddings_index = loading_embeddings()\n",
        "    num_words = min(MAX_NB_WORDS, len(word_index))\n",
        "    embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= MAX_NB_WORDS:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix, num_words\n",
        "\n",
        "\n",
        "def vectorizing_data(df):\n",
        "    label_s = df['label'].tolist()\n",
        "    l = list(set(label_s))\n",
        "    l.sort()\n",
        "    labels_index = dict([(j,i) for i, j in enumerate(l)])\n",
        "    labels = [labels_index[i] for i in label_s]\n",
        "    print('Found %s texts.' % len(df['text_clean2']))\n",
        "    print('labels_index — ', labels_index)\n",
        "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "    tokenizer.fit_on_texts(df['text_clean2'])\n",
        "    sequences = tokenizer.texts_to_sequences(df['text_clean2'])\n",
        "    word_index = tokenizer.word_index\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "    df = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    labels = np.asarray(labels)\n",
        "\n",
        "    return df, labels, word_index"
      ],
      "metadata": {
        "id": "Okgvn6ujVw2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_NB_WORDS = 100000\n",
        "MAX_SEQUENCE_LENGTH = 300\n",
        "\n",
        "df_train_for_glove = train[['text_clean2', 'label']]\n",
        "df_test_for_glove = test[['text_clean2', 'label']]\n",
        "\n",
        "glove_vecs_train, labels_train, word_index_train = vectorizing_data(df_train_for_glove)\n",
        "glove_vecs_test, labels_test, word_index_test = vectorizing_data(df_test_for_glove)"
      ],
      "metadata": {
        "id": "BIFIGwAlY1nD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43887042-2803-496f-8dff-765dec255712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2838 texts.\n",
            "labels_index —  {0: 0, 1: 1}\n",
            "Found 11397 unique tokens.\n",
            "Found 715 texts.\n",
            "labels_index —  {0: 0, 1: 1}\n",
            "Found 5599 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FastText"
      ],
      "metadata": {
        "id": "giU3QbQd7s5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        "ft_model = fasttext.load_model('cc.en.300.bin')\n",
        "window_length = 500 # The amount of words we look at per example.\n",
        "n_features = ft_model.get_dimension()"
      ],
      "metadata": {
        "id": "rsKIdwx34wGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238b114f-ce32-4a7d-9be5-1acb70a49a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_vector(text):\n",
        "    \"\"\"\n",
        "    Given a string, normalizes it, then splits it into words and finally converts\n",
        "    it to a sequence of word vectors.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    window = words[-window_length:]\n",
        "    \n",
        "    x = np.zeros((window_length, n_features))\n",
        "\n",
        "    for i, word in enumerate(window):\n",
        "        x[i, :] = ft_model.get_word_vector(word).astype('float32')\n",
        "\n",
        "    return x\n",
        "\n",
        "def df_to_data(df):\n",
        "    \"\"\"\n",
        "    Convert a given dataframe to a dataset of inputs for the NN.\n",
        "    \"\"\"\n",
        "    x = np.zeros((len(df), window_length, n_features), dtype='float32')\n",
        "\n",
        "    for i, comment in enumerate(df['text'].values):\n",
        "        x[i, :] = text_to_vector(comment)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "1Y7OSAJENvKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "FoDIrZYq7ojD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "VOeT63eHEwBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X,y):\n",
        "  kf = KFold(5, shuffle=True, random_state=42)\n",
        "  cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
        "\n",
        "  for train_ind, val_ind in kf.split(X, y):\n",
        "      # Assign CV IDX\n",
        "      X_train, y_train = X[train_ind], y[train_ind]\n",
        "      X_val, y_val = X[val_ind], y[val_ind]\n",
        "      \n",
        "      # Scale Data\n",
        "      scaler = StandardScaler()\n",
        "      X_train_scale = scaler.fit_transform(X_train)\n",
        "      X_val_scale = scaler.transform(X_val)\n",
        "\n",
        "      # Logisitic Regression\n",
        "      lr = LogisticRegression(\n",
        "          solver= 'liblinear',\n",
        "          max_iter=1000,\n",
        "          verbose=1,\n",
        "          penalty='l2',\n",
        "          C=5\n",
        "          # fit_intercept=True\n",
        "      ).fit(X_train_scale, y_train)\n",
        "\n",
        "\n",
        "      y_pred = lr.predict(X_val_scale)\n",
        "      cv_lr_f1.append(f1_score(y_val, y_pred))\n",
        "      \n",
        "\n",
        "  print(f'Logistic Regression Val f1: {np.mean(cv_lr_f1):.4f} +- {np.std(cv_lr_f1):.3f}')\n",
        "\n",
        "  return lr"
      ],
      "metadata": {
        "id": "uaDcAfaGw3Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(train.label)"
      ],
      "metadata": {
        "id": "P4DfeIMmshGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model - features only\n",
        "train_vecs0 = []\n",
        "for i in range(len(train)):\n",
        "    topic_vec = []                                    \n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_Tone']])                                   # Features\n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_Clout']])                                  # Features\n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_i']])                                      # Features\n",
        "    train_vecs0.append(topic_vec)\n",
        "\n",
        "X_train0 = np.array(train_vecs0)\n",
        "lr0 = train_model(X_train0,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpHmXCzhlBxn",
        "outputId": "91641034-b5c4-4052-8d69-b321fe1f3ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Logistic Regression Val f1: 0.7535 +- 0.021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model - features + new\n",
        "train_vecs1 = []\n",
        "for i in range(len(train)):\n",
        "    topic_vec = []                                    \n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_Tone']])                                   # Features\n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_Clout']])                                  # Features\n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_i']])                                      # Features\n",
        "    topic_vec.extend([train.iloc[i]['punctuations']])                                    # Features - new\n",
        "    train_vecs1.append(topic_vec)\n",
        "\n",
        "X_train1 = np.array(train_vecs1)\n",
        "lr1 = train_model(X_train1, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id3nsoFLlBvc",
        "outputId": "1f44f470-9ff8-49b8-a176-58262ba86f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Logistic Regression Val f1: 0.7552 +- 0.018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model - features + new + LDA\n",
        "train_vecs2 = []\n",
        "for i in range(len(train)):\n",
        "    top_topics = lda_train.get_document_topics(train_corpus[i], minimum_probability=0.0) # LDA\n",
        "    topic_vec = [top_topics[i][1] for i in range(10)]                                    # LDA                                    \n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_Tone']])                                   # Features\n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_Clout']])                                  # Features\n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_i']])                                      # Features\n",
        "    topic_vec.extend([train.iloc[i]['punctuations']])                                    # Features - new\n",
        "    train_vecs2.append(topic_vec)\n",
        "\n",
        "X_train2 = np.array(train_vecs2)\n",
        "lr2 = train_model(X_train2, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3p2_PvolBtU",
        "outputId": "b9a3af4e-8193-40c1-c639-883f60a513e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Logistic Regression Val f1: 0.6910 +- 0.008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model - features + new + w2v\n",
        "train_vecs3 = []\n",
        "for i in range(len(train)):\n",
        "    topic_vec = []                                    \n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_Tone']])                                   # Features\n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_Clout']])                                  # Features\n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_i']])                                      # Features\n",
        "    topic_vec.extend([train.iloc[i]['punctuations']])                                    # Features - new\n",
        "    topic_vec.extend([w2v_X_train_vect_avg[i][j] for j in range(300)])                   # word2vec embedding\n",
        "    train_vecs3.append(topic_vec)\n",
        "\n",
        "X_train3 = np.array(train_vecs3)\n",
        "lr3 = train_model(X_train3, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwXHeWdQlBmY",
        "outputId": "05a55c6c-a345-4516-ae87-dfec578ddb85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Logistic Regression Val f1: 0.7591 +- 0.020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model - features + new + gloVe\n",
        "train_vecs4 = []\n",
        "for i in range(len(train)):\n",
        "    topic_vec = []                                    \n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_Tone']])                                   # Features\n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_Clout']])                                  # Features\n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_i']])                                      # Features\n",
        "    topic_vec.extend([train.iloc[i]['punctuations']])                                    # Features - new\n",
        "    topic_vec.extend([glove_vecs_train[i][j] for j in range(300)])                       # gloVe embedding\n",
        "    train_vecs4.append(topic_vec)\n",
        "\n",
        "X_train4 = np.array(train_vecs4)\n",
        "lr4 = train_model(X_train4, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGmE9_ImlBjz",
        "outputId": "38360725-6159-405c-c346-036931304ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Logistic Regression Val f1: 0.7393 +- 0.018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model - features + new + fastText\n",
        "train_vecs5 = []\n",
        "for i in range(len(train)):\n",
        "    topic_vec = []                                    \n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_Tone']])                                   # Features\n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_Clout']])                                  # Features\n",
        "    topic_vec.extend([train.iloc[i]['lex_liwc_i']])                                      # Features\n",
        "    topic_vec.extend([train.iloc[i]['punctuations']])                                    # Features - new\n",
        "    topic_vec.extend([fasttext_X_train_vect_avg[i][j] for j in range(300)])              # fastText embedding\n",
        "    train_vecs5.append(topic_vec)\n",
        "\n",
        "X_train5 = np.array(train_vecs5)\n",
        "lr5 = train_model(X_train5, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpUu9sVqlBg0",
        "outputId": "0653466f-9ddc-475c-e49d-42cfa3c52bb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Logistic Regression Val f1: 0.7632 +- 0.010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "28RFMelDEzUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(X,y, lr):\n",
        "  ss = StandardScaler()\n",
        "  X = ss.fit_transform(X)\n",
        "\n",
        "  y_pred_lr = lr.predict(X)\n",
        "\n",
        "  f1_scored = f1_score(y, y_pred_lr,average='binary')\n",
        "  precision_scored = precision_score(y, y_pred_lr,average='binary')\n",
        "  recall_scored = recall_score(y, y_pred_lr,average='binary')\n",
        "  \n",
        "  print(f'F1 score: {f1_scored}')\n",
        "  print(f'Precision: {precision_scored}')\n",
        "  print(f'recall: {recall_scored}')\n",
        "\n",
        "  return round(f1_scored, 4), round(precision_scored, 4), round(recall_scored, 4)"
      ],
      "metadata": {
        "id": "8uEC70vcmKay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.array(test.label)"
      ],
      "metadata": {
        "id": "w47u3SIts5x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model - features only\n",
        "name0 = 'features only'\n",
        "test_vecs0 = []\n",
        "for i in range(len(test)):\n",
        "    topic_vec = []                                    \n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_Tone']])                                   # Features\n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_Clout']])                                  # Features\n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_i']])                                      # Features\n",
        "    test_vecs0.append(topic_vec)\n",
        "\n",
        "X_test0 = np.array(test_vecs0)\n",
        "f1_score0, precision_score0, recall_score0 = eval_model(X_test0,y_test, lr0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mGVdwTxs5vV",
        "outputId": "cd7e198a-6d54-4afe-bd67-942e6aacd4ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.7605263157894737\n",
            "Precision: 0.7391304347826086\n",
            "recall: 0.7831978319783198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model - features + new\n",
        "name1 = 'features + new'\n",
        "test_vecs1 = []\n",
        "for i in range(len(test)):\n",
        "    topic_vec = []                                    \n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_Tone']])                                   # Features\n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_Clout']])                                  # Features\n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_i']])                                      # Features\n",
        "    topic_vec.extend([test.iloc[i]['punctuations']])                                    # Features - new\n",
        "    test_vecs1.append(topic_vec)\n",
        "\n",
        "X_test1 = np.array(test_vecs1)\n",
        "f1_score1, precision_score1, recall_score1 = eval_model(X_test1,y_test, lr1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ3ShhzKs5tW",
        "outputId": "39c213f9-fcfe-4f88-a547-a51f6b09203b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.764629388816645\n",
            "Precision: 0.735\n",
            "recall: 0.7967479674796748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model - features + new + LDA\n",
        "name2 = 'features + new + LDA'\n",
        "test_vecs2 = []\n",
        "for i in range(len(test)):\n",
        "    top_topics = lda_train.get_document_topics(test_corpus[i], minimum_probability=0.0) # LDA\n",
        "    topic_vec = [top_topics[i][1] for i in range(10)]                                   # LDA                                   \n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_Tone']])                                   # Features\n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_Clout']])                                  # Features\n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_i']])                                      # Features\n",
        "    topic_vec.extend([test.iloc[i]['punctuations']])                                    # Features - new\n",
        "    test_vecs2.append(topic_vec)\n",
        "\n",
        "X_test2 = np.array(test_vecs2)\n",
        "f1_score2, precision_score2, recall_score2 = eval_model(X_test2,y_test, lr2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMDSnaLRs5rN",
        "outputId": "c85ff3c2-9d0b-4ec4-938d-084a026d7223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.6984126984126984\n",
            "Precision: 0.6821705426356589\n",
            "recall: 0.7154471544715447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model - features + new + w2v\n",
        "name3 = 'features + new + w2v'\n",
        "test_vecs3 = []\n",
        "for i in range(len(test)):\n",
        "    topic_vec = []                                    \n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_Tone']])                                   # Features\n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_Clout']])                                  # Features\n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_i']])                                      # Features\n",
        "    topic_vec.extend([test.iloc[i]['punctuations']])                                    # Features - new\n",
        "    topic_vec.extend([w2v_X_test_vect_avg[i][j] for j in range(300)])                   # word2vec embedding\n",
        "    test_vecs3.append(topic_vec)\n",
        "\n",
        "X_test3 = np.array(test_vecs3)\n",
        "f1_score3, precision_score3, recall_score3 = eval_model(X_test3,y_test, lr3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78H5Gqsas5oa",
        "outputId": "f5c25d9a-aeff-4ec1-900c-b5bda4e9966d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.7735099337748346\n",
            "Precision: 0.7564766839378239\n",
            "recall: 0.7913279132791328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model - features + new + gloVe\n",
        "name4 = 'features + new + gloVe'\n",
        "test_vecs4 = []\n",
        "for i in range(len(test)):\n",
        "    topic_vec = []                                    \n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_Tone']])                                   # Features\n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_Clout']])                                  # Features\n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_i']])                                      # Features\n",
        "    topic_vec.extend([test.iloc[i]['punctuations']])                                    # Features - new\n",
        "    topic_vec.extend([glove_vecs_test[i][j] for j in range(300)])                       # gloVe embedding\n",
        "    test_vecs4.append(topic_vec)\n",
        "\n",
        "X_test4 = np.array(test_vecs4)\n",
        "f1_score4, precision_score4, recall_score4 = eval_model(X_test4,y_test, lr4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7Qj1wdis5l0",
        "outputId": "ef4dcd4d-b530-4761-a83b-1390f51163c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.8015873015873015\n",
            "Precision: 0.7829457364341085\n",
            "recall: 0.8211382113821138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model - features + new + fastText\n",
        "name5 = 'features + new + fastText'\n",
        "test_vecs5 = []\n",
        "for i in range(len(test)):\n",
        "    topic_vec = []                                    \n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_Tone']])                                   # Features\n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_Clout']])                                  # Features\n",
        "    topic_vec.extend([test.iloc[i]['lex_liwc_i']])                                      # Features\n",
        "    topic_vec.extend([test.iloc[i]['punctuations']])                                    # Features - new\n",
        "    topic_vec.extend([fattext_X_test_vect_avg[i][j] for j in range(300)])               # fastText embedding\n",
        "    test_vecs5.append(topic_vec)\n",
        "\n",
        "X_test5 = np.array(test_vecs5)\n",
        "f1_score5, precision_score5, recall_score5 = eval_model(X_test5,y_test, lr5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YO4lEuI4Xmu",
        "outputId": "a1409d9d-e558-47af-bdd3-be5c83ed2d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.8348993288590604\n",
            "Precision: 0.8271276595744681\n",
            "recall: 0.8428184281842819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "oNSAxD8qrr96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "78RcgV1PruI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTpLPcu_sBzW",
        "outputId": "ba519498-ff80-4a63-cb6c-2f890e0cee6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 90.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 56.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = train['text']\n",
        "train_labels = train['label']\n",
        "\n",
        "test_text = test['text']\n",
        "test_labels = test['label']\n",
        "\n",
        "\n",
        "train_text, val_text, train_labels, val_labels = train_test_split(train['text'], train['label'], \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.15, \n",
        "                                                                stratify=train['label'])"
      ],
      "metadata": {
        "id": "mDfGn8BzsBxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing2(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # text = text.lower()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "Ou_quNW6D44y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# Create a function to tokenize a set of texts\n",
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing2(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "metadata": {
        "id": "QfjEKye-hJpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN = 300\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(train_text)\n",
        "val_inputs, val_masks = preprocessing_for_bert(val_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQK2ZwxfhRBG",
        "outputId": "d24b11fa-952f-428b-ad68-5435355596a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labelss = torch.tensor(train_labels.to_numpy())\n",
        "val_labelss = torch.tensor(val_labels.to_numpy())\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labelss)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labelss)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "r8O72pFZhj-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V04rsjcBhksl",
        "outputId": "b8f3bb96-90d1-4775-d82d-453a4f351c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 44 µs, sys: 0 ns, total: 44 µs\n",
            "Wall time: 47 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(epochs=4):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "metadata": {
        "id": "lyO9-GMoh3Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "DcjHAcd0i3vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729,
          "referenced_widgets": [
            "8ed4a89349eb4402a00cc2e76137b884",
            "d1a93959e8a1495fbef04da51c736584",
            "d0bfd7a5ffd34941952cb0ccaf4c8ea5",
            "b802e34e49a246e5bb866d1134f9203c",
            "9b02c83c77004c319064e9c0d6dc6a40",
            "4a7f8de16a044b2b945eaa95b39698b9",
            "c5f019bcabf6453f925fa46a2319ef42",
            "5a1cc6b422ed49329a490ff91963509a",
            "7712506241b74f9c8254218588b1ef51",
            "02ef16e49d2f4a47900ee1a6ac228245",
            "9e384b8367c44c3c97da9f9a4aa6de52"
          ]
        },
        "id": "yWcXHRsAi-RU",
        "outputId": "906db8ea-9e70-44c1-d956-4a2740826691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ed4a89349eb4402a00cc2e76137b884"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.697030   |     -      |     -     |   13.26  \n",
            "   1    |   40    |   0.691691   |     -      |     -     |   9.76   \n",
            "   1    |   60    |   0.677739   |     -      |     -     |   9.75   \n",
            "   1    |   80    |   0.603047   |     -      |     -     |   9.74   \n",
            "   1    |   100   |   0.582703   |     -      |     -     |   9.74   \n",
            "   1    |   120   |   0.590601   |     -      |     -     |   9.74   \n",
            "   1    |   140   |   0.533755   |     -      |     -     |   9.74   \n",
            "   1    |   150   |   0.545170   |     -      |     -     |   4.77   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.620398   |  0.527350  |   74.03   |   80.72  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.454671   |     -      |     -     |   10.22  \n",
            "   2    |   40    |   0.457129   |     -      |     -     |   9.75   \n",
            "   2    |   60    |   0.399795   |     -      |     -     |   9.74   \n",
            "   2    |   80    |   0.423902   |     -      |     -     |   9.74   \n",
            "   2    |   100   |   0.445280   |     -      |     -     |   9.74   \n",
            "   2    |   120   |   0.435444   |     -      |     -     |   9.74   \n",
            "   2    |   140   |   0.393635   |     -      |     -     |   9.75   \n",
            "   2    |   150   |   0.402061   |     -      |     -     |   4.76   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.428294   |  0.475982  |   78.66   |   77.66  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ],
      "metadata": {
        "id": "EFhgIVdljCIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "# probs = bert_predict(bert_classifier, val_dataloader)"
      ],
      "metadata": {
        "id": "EogseGqcjlZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the train set and the validation set\n",
        "full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n",
        "full_train_sampler = RandomSampler(full_train_data)\n",
        "full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=16)\n",
        "\n",
        "# Train the Bert Classifier on the entire training data\n",
        "set_seed(42)\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
        "train(bert_classifier, full_train_dataloader, epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ5bNW1_joSW",
        "outputId": "40881758-f33a-4c5d-8596-a44f377f466b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.634572   |     -      |     -     |   10.27  \n",
            "   1    |   40    |   0.468481   |     -      |     -     |   9.76   \n",
            "   1    |   60    |   0.456016   |     -      |     -     |   9.74   \n",
            "   1    |   80    |   0.410451   |     -      |     -     |   9.75   \n",
            "   1    |   100   |   0.411934   |     -      |     -     |   9.75   \n",
            "   1    |   120   |   0.440629   |     -      |     -     |   9.74   \n",
            "   1    |   140   |   0.419315   |     -      |     -     |   9.76   \n",
            "   1    |   160   |   0.420392   |     -      |     -     |   9.73   \n",
            "   1    |   177   |   0.448730   |     -      |     -     |   7.98   \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.295763   |     -      |     -     |   10.23  \n",
            "   2    |   40    |   0.232310   |     -      |     -     |   9.73   \n",
            "   2    |   60    |   0.199796   |     -      |     -     |   9.73   \n",
            "   2    |   80    |   0.288306   |     -      |     -     |   9.73   \n",
            "   2    |   100   |   0.178601   |     -      |     -     |   9.74   \n",
            "   2    |   120   |   0.220097   |     -      |     -     |   9.74   \n",
            "   2    |   140   |   0.160841   |     -      |     -     |   9.73   \n",
            "   2    |   160   |   0.175672   |     -      |     -     |   9.74   \n",
            "   2    |   177   |   0.200866   |     -      |     -     |   7.99   \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run `preprocessing_for_bert` on the test set\n",
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(test_text)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7aNsUvLj-AW",
        "outputId": "f27ece57-0aa1-46a6-9387-95da7f4adc5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "preds = np.argmax(probs, axis=1)\n",
        "# Get predictions from the probabilities\n",
        "# threshold = 0.9\n",
        "# preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "# print(\"Number of tweets predicted non-negative: \", preds.sum())"
      ],
      "metadata": {
        "id": "2PjvI3PwkQ6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "y_true = np.array(test_labels)\n",
        "y_pred = np.array(preds)\n",
        "\n",
        "precision_recall_fscore_support(y_true, y_pred, average='macro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79FmgsFxlBzL",
        "outputId": "055eaa47-1ac7-4318-81f2-0df1cbf4dd55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8263062720091368, 0.8222425865877155, 0.822802586858006, None)"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    }
  ]
}