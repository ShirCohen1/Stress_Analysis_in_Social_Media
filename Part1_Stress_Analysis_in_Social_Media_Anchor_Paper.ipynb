{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stress Analysis in Social Media.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stress Analysis in Social Media - Anchor Paper \n",
        "\n",
        "Shir Cohen 205790124\n",
        "\n",
        "Guy Assa 204118616\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lyAYHtuEE_Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd 'gdrive/My Drive/AML/Final Project'\n",
        "!ls"
      ],
      "metadata": {
        "id": "4fQ83eYffZFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pm4U29cjnU7",
        "outputId": "c3bc224b-d6b9-40fb-a16a-cd44a65aead2"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "T_LVgZgQa92V",
        "outputId": "1fa0063d-6696-442a-fb1d-d154353914d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          subreddit post_id sentence_range  \\\n",
              "1              ptsd  8601tu       (15, 20)   \n",
              "2        assistance  8lbrx9         (0, 5)   \n",
              "3              ptsd  9ch1zh       (15, 20)   \n",
              "4     relationships  7rorpp        [5, 10]   \n",
              "5  survivorsofabuse  9p2gbc         [0, 5]   \n",
              "\n",
              "                                                text     id  label  \\\n",
              "1  He said he had not felt that way before, sugge...  33181      1   \n",
              "2  Hey there r/assistance, Not sure if this is th...   2606      0   \n",
              "3  My mom then hit me with the newspaper and it s...  38816      1   \n",
              "4  until i met my new boyfriend, he is amazing, h...    239      1   \n",
              "5  October is Domestic Violence Awareness Month a...   1421      1   \n",
              "\n",
              "   confidence  social_timestamp  social_karma  syntax_ari  ...  \\\n",
              "1         0.8        1521614353             5    1.806818  ...   \n",
              "2         1.0        1527009817             4    9.429737  ...   \n",
              "3         0.8        1535935605             2    7.769821  ...   \n",
              "4         0.6        1516429555             0    2.667798  ...   \n",
              "5         0.8        1539809005            24    7.554238  ...   \n",
              "\n",
              "   lex_dal_min_pleasantness  lex_dal_min_activation  lex_dal_min_imagery  \\\n",
              "1                     1.000                  1.1250                  1.0   \n",
              "2                     1.125                  1.0000                  1.0   \n",
              "3                     1.000                  1.1429                  1.0   \n",
              "4                     1.000                  1.1250                  1.0   \n",
              "5                     1.000                  1.1250                  1.0   \n",
              "\n",
              "   lex_dal_avg_activation  lex_dal_avg_imagery  lex_dal_avg_pleasantness  \\\n",
              "1                 1.77000              1.52211                   1.89556   \n",
              "2                 1.69586              1.62045                   1.88919   \n",
              "3                 1.83088              1.58108                   1.85828   \n",
              "4                 1.75356              1.52114                   1.98848   \n",
              "5                 1.77644              1.64872                   1.81456   \n",
              "\n",
              "   social_upvote_ratio  social_num_comments  syntax_fk_grade  sentiment  \n",
              "1                 0.86                    1         3.253573  -0.002742  \n",
              "2                 0.65                    2         8.828316   0.292857  \n",
              "3                 0.67                    0         7.841667   0.011894  \n",
              "4                 0.50                    5         4.104027   0.141671  \n",
              "5                 1.00                    1         7.910952  -0.204167  \n",
              "\n",
              "[5 rows x 116 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dc6004df-fe51-4bb8-b86f-71f74bf61ffc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddit</th>\n",
              "      <th>post_id</th>\n",
              "      <th>sentence_range</th>\n",
              "      <th>text</th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>confidence</th>\n",
              "      <th>social_timestamp</th>\n",
              "      <th>social_karma</th>\n",
              "      <th>syntax_ari</th>\n",
              "      <th>...</th>\n",
              "      <th>lex_dal_min_pleasantness</th>\n",
              "      <th>lex_dal_min_activation</th>\n",
              "      <th>lex_dal_min_imagery</th>\n",
              "      <th>lex_dal_avg_activation</th>\n",
              "      <th>lex_dal_avg_imagery</th>\n",
              "      <th>lex_dal_avg_pleasantness</th>\n",
              "      <th>social_upvote_ratio</th>\n",
              "      <th>social_num_comments</th>\n",
              "      <th>syntax_fk_grade</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ptsd</td>\n",
              "      <td>8601tu</td>\n",
              "      <td>(15, 20)</td>\n",
              "      <td>He said he had not felt that way before, sugge...</td>\n",
              "      <td>33181</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1521614353</td>\n",
              "      <td>5</td>\n",
              "      <td>1.806818</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.1250</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.77000</td>\n",
              "      <td>1.52211</td>\n",
              "      <td>1.89556</td>\n",
              "      <td>0.86</td>\n",
              "      <td>1</td>\n",
              "      <td>3.253573</td>\n",
              "      <td>-0.002742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>assistance</td>\n",
              "      <td>8lbrx9</td>\n",
              "      <td>(0, 5)</td>\n",
              "      <td>Hey there r/assistance, Not sure if this is th...</td>\n",
              "      <td>2606</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1527009817</td>\n",
              "      <td>4</td>\n",
              "      <td>9.429737</td>\n",
              "      <td>...</td>\n",
              "      <td>1.125</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.69586</td>\n",
              "      <td>1.62045</td>\n",
              "      <td>1.88919</td>\n",
              "      <td>0.65</td>\n",
              "      <td>2</td>\n",
              "      <td>8.828316</td>\n",
              "      <td>0.292857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ptsd</td>\n",
              "      <td>9ch1zh</td>\n",
              "      <td>(15, 20)</td>\n",
              "      <td>My mom then hit me with the newspaper and it s...</td>\n",
              "      <td>38816</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1535935605</td>\n",
              "      <td>2</td>\n",
              "      <td>7.769821</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.1429</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.83088</td>\n",
              "      <td>1.58108</td>\n",
              "      <td>1.85828</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0</td>\n",
              "      <td>7.841667</td>\n",
              "      <td>0.011894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>relationships</td>\n",
              "      <td>7rorpp</td>\n",
              "      <td>[5, 10]</td>\n",
              "      <td>until i met my new boyfriend, he is amazing, h...</td>\n",
              "      <td>239</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1516429555</td>\n",
              "      <td>0</td>\n",
              "      <td>2.667798</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.1250</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.75356</td>\n",
              "      <td>1.52114</td>\n",
              "      <td>1.98848</td>\n",
              "      <td>0.50</td>\n",
              "      <td>5</td>\n",
              "      <td>4.104027</td>\n",
              "      <td>0.141671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>survivorsofabuse</td>\n",
              "      <td>9p2gbc</td>\n",
              "      <td>[0, 5]</td>\n",
              "      <td>October is Domestic Violence Awareness Month a...</td>\n",
              "      <td>1421</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1539809005</td>\n",
              "      <td>24</td>\n",
              "      <td>7.554238</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.1250</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.77644</td>\n",
              "      <td>1.64872</td>\n",
              "      <td>1.81456</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1</td>\n",
              "      <td>7.910952</td>\n",
              "      <td>-0.204167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 116 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc6004df-fe51-4bb8-b86f-71f74bf61ffc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dc6004df-fe51-4bb8-b86f-71f74bf61ffc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dc6004df-fe51-4bb8-b86f-71f74bf61ffc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_train = pd.read_csv(\"dreaddit-train.csv\", index_col = 0)\n",
        "df_test = pd.read_csv(\"dreaddit-test.csv\", index_col = 0)\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train.shape, df_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pzBxE6If1tQ",
        "outputId": "2b65d117-4d3a-4678-9101-3f8a8a301bf5"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2838, 116) (715, 116)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()"
      ],
      "metadata": {
        "id": "8mHJoQoFx_Qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b311d72-1460-43c6-e757-566c3c69581e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2838 entries, 1 to 2838\n",
            "Columns: 116 entries, subreddit to sentiment\n",
            "dtypes: float64(106), int64(6), object(4)\n",
            "memory usage: 2.5+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.info()"
      ],
      "metadata": {
        "id": "NxJUWaYRyA8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2448b9e-0268-451b-a98b-37d7a643b953"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 715 entries, 1 to 715\n",
            "Columns: 116 entries, id to sentiment\n",
            "dtypes: float64(106), int64(6), object(4)\n",
            "memory usage: 653.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.describe()"
      ],
      "metadata": {
        "id": "71D1mU96NGDF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "21ef6d79-0000-4857-b22a-b979fb6941ed"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 id        label   confidence  social_timestamp  social_karma  \\\n",
              "count   2838.000000  2838.000000  2838.000000      2.838000e+03   2838.000000   \n",
              "mean   13751.999295     0.524313     0.808972      1.518107e+09     18.262156   \n",
              "std    17340.161897     0.499497     0.177038      1.552209e+07     79.419166   \n",
              "min        4.000000     0.000000     0.428571      1.483274e+09      0.000000   \n",
              "25%      926.250000     0.000000     0.600000      1.509698e+09      2.000000   \n",
              "50%     1891.500000     1.000000     0.800000      1.517066e+09      5.000000   \n",
              "75%    25473.750000     1.000000     1.000000      1.530898e+09     10.000000   \n",
              "max    55757.000000     1.000000     1.000000      1.542592e+09   1435.000000   \n",
              "\n",
              "        syntax_ari  lex_liwc_WC  lex_liwc_Analytic  lex_liwc_Clout  \\\n",
              "count  2838.000000  2838.000000        2838.000000     2838.000000   \n",
              "mean      4.684272    85.996124          35.240941       40.948231   \n",
              "std       3.316435    32.334887          26.486189       31.587117   \n",
              "min      -6.620000     5.000000           1.000000        1.000000   \n",
              "25%       2.464243    65.000000          12.410000       12.135000   \n",
              "50%       4.321886    81.000000          29.420000       33.520000   \n",
              "75%       6.505657   101.000000          55.057500       69.320000   \n",
              "max      24.074231   310.000000          99.000000       99.000000   \n",
              "\n",
              "       lex_liwc_Authentic  ...  lex_dal_min_pleasantness  \\\n",
              "count         2838.000000  ...               2838.000000   \n",
              "mean            67.044249  ...                  1.088001   \n",
              "std             32.880644  ...                  0.117159   \n",
              "min              1.000000  ...                  1.000000   \n",
              "25%             41.070000  ...                  1.000000   \n",
              "50%             80.710000  ...                  1.000000   \n",
              "75%             96.180000  ...                  1.142900   \n",
              "max             99.000000  ...                  1.900000   \n",
              "\n",
              "       lex_dal_min_activation  lex_dal_min_imagery  lex_dal_avg_activation  \\\n",
              "count             2838.000000          2838.000000             2838.000000   \n",
              "mean                 1.120099             1.000211                1.722759   \n",
              "std                  0.085227             0.006500                0.047835   \n",
              "min                  1.000000             1.000000                1.485400   \n",
              "25%                  1.000000             1.000000                1.691430   \n",
              "50%                  1.142900             1.000000                1.721430   \n",
              "75%                  1.142900             1.000000                1.751760   \n",
              "max                  1.500000             1.200000                2.007400   \n",
              "\n",
              "       lex_dal_avg_imagery  lex_dal_avg_pleasantness  social_upvote_ratio  \\\n",
              "count          2838.000000               2838.000000          2838.000000   \n",
              "mean              1.536400                  1.879385             0.843517   \n",
              "std               0.102971                  0.058932             0.174794   \n",
              "min               1.200000                  1.561150             0.140000   \n",
              "25%               1.469745                  1.841782             0.750000   \n",
              "50%               1.530295                  1.878250             0.890000   \n",
              "75%               1.596030                  1.916243             1.000000   \n",
              "max               2.066670                  2.158490             1.000000   \n",
              "\n",
              "       social_num_comments  syntax_fk_grade    sentiment  \n",
              "count          2838.000000      2838.000000  2838.000000  \n",
              "mean              9.948555         5.448836     0.040740  \n",
              "std              21.798032         2.535829     0.195490  \n",
              "min               0.000000        -1.918000    -1.000000  \n",
              "25%               2.000000         3.729973    -0.072222  \n",
              "50%               5.000000         5.210000     0.044821  \n",
              "75%              10.000000         6.855217     0.166667  \n",
              "max             416.000000        21.198919     1.000000  \n",
              "\n",
              "[8 rows x 112 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-52f0e8e7-53b6-4289-8b87-27a2cfdf9033\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>confidence</th>\n",
              "      <th>social_timestamp</th>\n",
              "      <th>social_karma</th>\n",
              "      <th>syntax_ari</th>\n",
              "      <th>lex_liwc_WC</th>\n",
              "      <th>lex_liwc_Analytic</th>\n",
              "      <th>lex_liwc_Clout</th>\n",
              "      <th>lex_liwc_Authentic</th>\n",
              "      <th>...</th>\n",
              "      <th>lex_dal_min_pleasantness</th>\n",
              "      <th>lex_dal_min_activation</th>\n",
              "      <th>lex_dal_min_imagery</th>\n",
              "      <th>lex_dal_avg_activation</th>\n",
              "      <th>lex_dal_avg_imagery</th>\n",
              "      <th>lex_dal_avg_pleasantness</th>\n",
              "      <th>social_upvote_ratio</th>\n",
              "      <th>social_num_comments</th>\n",
              "      <th>syntax_fk_grade</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2.838000e+03</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2838.000000</td>\n",
              "      <td>2838.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>13751.999295</td>\n",
              "      <td>0.524313</td>\n",
              "      <td>0.808972</td>\n",
              "      <td>1.518107e+09</td>\n",
              "      <td>18.262156</td>\n",
              "      <td>4.684272</td>\n",
              "      <td>85.996124</td>\n",
              "      <td>35.240941</td>\n",
              "      <td>40.948231</td>\n",
              "      <td>67.044249</td>\n",
              "      <td>...</td>\n",
              "      <td>1.088001</td>\n",
              "      <td>1.120099</td>\n",
              "      <td>1.000211</td>\n",
              "      <td>1.722759</td>\n",
              "      <td>1.536400</td>\n",
              "      <td>1.879385</td>\n",
              "      <td>0.843517</td>\n",
              "      <td>9.948555</td>\n",
              "      <td>5.448836</td>\n",
              "      <td>0.040740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>17340.161897</td>\n",
              "      <td>0.499497</td>\n",
              "      <td>0.177038</td>\n",
              "      <td>1.552209e+07</td>\n",
              "      <td>79.419166</td>\n",
              "      <td>3.316435</td>\n",
              "      <td>32.334887</td>\n",
              "      <td>26.486189</td>\n",
              "      <td>31.587117</td>\n",
              "      <td>32.880644</td>\n",
              "      <td>...</td>\n",
              "      <td>0.117159</td>\n",
              "      <td>0.085227</td>\n",
              "      <td>0.006500</td>\n",
              "      <td>0.047835</td>\n",
              "      <td>0.102971</td>\n",
              "      <td>0.058932</td>\n",
              "      <td>0.174794</td>\n",
              "      <td>21.798032</td>\n",
              "      <td>2.535829</td>\n",
              "      <td>0.195490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>1.483274e+09</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-6.620000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.485400</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>1.561150</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.918000</td>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>926.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1.509698e+09</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.464243</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>12.410000</td>\n",
              "      <td>12.135000</td>\n",
              "      <td>41.070000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.691430</td>\n",
              "      <td>1.469745</td>\n",
              "      <td>1.841782</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.729973</td>\n",
              "      <td>-0.072222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1891.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1.517066e+09</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.321886</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>29.420000</td>\n",
              "      <td>33.520000</td>\n",
              "      <td>80.710000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.142900</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.721430</td>\n",
              "      <td>1.530295</td>\n",
              "      <td>1.878250</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.210000</td>\n",
              "      <td>0.044821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>25473.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.530898e+09</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>6.505657</td>\n",
              "      <td>101.000000</td>\n",
              "      <td>55.057500</td>\n",
              "      <td>69.320000</td>\n",
              "      <td>96.180000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.142900</td>\n",
              "      <td>1.142900</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.751760</td>\n",
              "      <td>1.596030</td>\n",
              "      <td>1.916243</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>6.855217</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>55757.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.542592e+09</td>\n",
              "      <td>1435.000000</td>\n",
              "      <td>24.074231</td>\n",
              "      <td>310.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.900000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>2.007400</td>\n",
              "      <td>2.066670</td>\n",
              "      <td>2.158490</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>416.000000</td>\n",
              "      <td>21.198919</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 112 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52f0e8e7-53b6-4289-8b87-27a2cfdf9033')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-52f0e8e7-53b6-4289-8b87-27a2cfdf9033 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-52f0e8e7-53b6-4289-8b87-27a2cfdf9033');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical features\n",
        "df_train.describe(include = 'object')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "jtoz8BplyECg",
        "outputId": "890e260e-0d07-4736-808e-adf0e4cfd2ab"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       subreddit post_id sentence_range  \\\n",
              "count       2838    2838           2838   \n",
              "unique        10    2343            173   \n",
              "top         ptsd  9md6qz         (0, 5)   \n",
              "freq         584       6            337   \n",
              "\n",
              "                                                     text  \n",
              "count                                                2838  \n",
              "unique                                               2820  \n",
              "top     Hello, You are invited to complete a survey fo...  \n",
              "freq                                                    4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3fdc2045-4351-499e-b7a5-0b1f35ff0e24\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddit</th>\n",
              "      <th>post_id</th>\n",
              "      <th>sentence_range</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2838</td>\n",
              "      <td>2838</td>\n",
              "      <td>2838</td>\n",
              "      <td>2838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>10</td>\n",
              "      <td>2343</td>\n",
              "      <td>173</td>\n",
              "      <td>2820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>ptsd</td>\n",
              "      <td>9md6qz</td>\n",
              "      <td>(0, 5)</td>\n",
              "      <td>Hello, You are invited to complete a survey fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>584</td>\n",
              "      <td>6</td>\n",
              "      <td>337</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3fdc2045-4351-499e-b7a5-0b1f35ff0e24')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3fdc2045-4351-499e-b7a5-0b1f35ff0e24 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3fdc2045-4351-499e-b7a5-0b1f35ff0e24');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Types of subreddit\n",
        "df_train.subreddit.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKSkdxGGykoj",
        "outputId": "0c95d158-95da-4046-e782-90a2020f7e72"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ptsd                584\n",
              "relationships       552\n",
              "anxiety             503\n",
              "domesticviolence    316\n",
              "assistance          289\n",
              "survivorsofabuse    245\n",
              "homeless            168\n",
              "almosthomeless       80\n",
              "stress               64\n",
              "food_pantry          37\n",
              "Name: subreddit, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = df_train.drop(['post_id', 'sentence_range', 'id'], axis = 1)\n",
        "test = df_test.drop(['post_id', 'sentence_range', 'id'], axis = 1)"
      ],
      "metadata": {
        "id": "Y2_fsdL0yoqh"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing "
      ],
      "metadata": {
        "id": "nqGLaF30LLYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lable to a numeric feature\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "train['subreddit'] = le.fit_transform(train['subreddit'])\n",
        "train.subreddit.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99eUfttjywBA",
        "outputId": "c00d10bd-fd23-4383-f7b5-5b78920b2540"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6    584\n",
              "7    552\n",
              "1    503\n",
              "3    316\n",
              "2    289\n",
              "9    245\n",
              "5    168\n",
              "0     80\n",
              "8     64\n",
              "4     37\n",
              "Name: subreddit, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select high-correlation features according to the paper"
      ],
      "metadata": {
        "id": "XWRhKrFGLndw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.corr().abs()['label'].sort_values(ascending = False)[:10]"
      ],
      "metadata": {
        "id": "o48BYre8zeN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ba1d85d-6262-402b-f64c-057e04bcb5e8"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label                       1.000000\n",
              "lex_liwc_Tone               0.436005\n",
              "lex_liwc_Clout              0.403804\n",
              "lex_liwc_i                  0.400440\n",
              "lex_liwc_negemo             0.387979\n",
              "sentiment                   0.305157\n",
              "lex_dal_min_pleasantness    0.297439\n",
              "lex_liwc_Authentic          0.275282\n",
              "lex_liwc_posemo             0.270672\n",
              "lex_liwc_anx                0.260146\n",
              "Name: label, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corr_features_04 = train.corr().abs()['label'].sort_values(ascending = False).where(lambda x : x>=0.4).dropna().keys().tolist()\n",
        "corr_features_04"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz8LlYRHLGkK",
        "outputId": "37ae2ba3-d60f-4afc-b063-6423a0aad55d"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['label', 'lex_liwc_Tone', 'lex_liwc_Clout', 'lex_liwc_i']"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing"
      ],
      "metadata": {
        "id": "BJ7AtAb_NK2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stop = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwmqLwHPNTv7",
        "outputId": "73377776-681a-42da-a6e9-42d4c99220d3"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stop])\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('','', punctuation))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = lower(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = remove_punctuation(text)\n",
        "    return text\n",
        "\n",
        "train['clean_text']=train['text'].apply(clean_text)\n",
        "test['clean_text']=test['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "HHW98vg5NNc8"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing common words\n",
        "from collections import Counter\n",
        "cnt_train=Counter()\n",
        "cnt_test=Counter()\n",
        "\n",
        "for text in train['clean_text'].values:\n",
        "    for word in text.split():\n",
        "        cnt_train[word]+=1\n",
        "        \n",
        "print(cnt_train.most_common(10))\n",
        "\n",
        "for text in test['clean_text'].values:\n",
        "    for word in text.split():\n",
        "        cnt_test[word]+=1\n",
        "        \n",
        "print(cnt_test.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXkxFHJFNZU-",
        "outputId": "65f43b2f-2e22-42f8-f6b2-36829a64c366"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('im', 1352), ('like', 1180), ('know', 854), ('get', 850), ('feel', 762), ('time', 756), ('would', 753), ('me', 678), ('really', 650), ('even', 597)]\n",
            "[('im', 364), ('like', 323), ('get', 233), ('feel', 204), ('know', 198), ('time', 191), ('would', 190), ('me', 177), ('really', 163), ('ive', 160)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_FREQWORDS = set([w for (w, wc) in cnt_train.most_common(10)])\n",
        "test_FREQWORDS = set([w for (w, wc) in cnt_test.most_common(10)])\n",
        "\n",
        "def remove_freqwords(text, FREQWORDS):\n",
        "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
        "\n",
        "train[\"clean_text\"] = train[\"clean_text\"].apply(lambda text: remove_freqwords(text, train_FREQWORDS))\n",
        "test[\"clean_text\"] = test[\"clean_text\"].apply(lambda text: remove_freqwords(text, test_FREQWORDS))"
      ],
      "metadata": {
        "id": "gooBCtEINf5F"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUgXt3rIf2sJ",
        "outputId": "91f54118-fca2-417d-ce4c-0331f540b126"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lematizer=WordNetLemmatizer()\n",
        "\n",
        "def lemmatizer_words(text):\n",
        "    return \" \".join([lematizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "train['clean_text']=train['clean_text'].apply(lambda text: lemmatizer_words(text))\n",
        "test['clean_text']=test['clean_text'].apply(lambda text: lemmatizer_words(text))"
      ],
      "metadata": {
        "id": "UW06_2lBTqbH"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Majority Baseline Implementation"
      ],
      "metadata": {
        "id": "Fi-y_95yvMH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from sklearn.metrics import *\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score"
      ],
      "metadata": {
        "id": "jjZAB2Y0vQeU"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MajorityBaselineClassifier():\n",
        "    def __init__(self, num_classes):\n",
        "        self.num_classes = num_classes\n",
        "        self.class_counter_dict = dict()\n",
        "        for c in range(self.num_classes):\n",
        "            self.class_counter_dict.update({c:0})\n",
        "    \n",
        "    def fit(self, train_x):\n",
        "        for x in train_x:\n",
        "            prev_value = self.class_counter_dict[x]\n",
        "            self.class_counter_dict.update({x:prev_value + 1})\n",
        "        best_c = -1 \n",
        "        max_amount = -1\n",
        "        for c in range(self.num_classes):\n",
        "            if (self.class_counter_dict[c] > max_amount or best_c == -1):\n",
        "                best_c = c\n",
        "                max_amount = self.class_counter_dict[c]\n",
        "        self.best_c = best_c\n",
        "        \n",
        "    def evaluate(self, test_x):\n",
        "        y_pred = [self.best_c for i in range(len(test_x))]\n",
        "        y_true = test_x\n",
        "        accuracy = accuracy_score(y_pred, y_true)*100\n",
        "        f1 = f1_score(y_pred, y_true, zero_division=0)*100\n",
        "        precision = precision_score(test_labels, y_pred, zero_division=1)*100\n",
        "        recall = recall_score(y_pred, y_true, zero_division=0)*100\n",
        "        return accuracy, f1, precision, recall"
      ],
      "metadata": {
        "id": "e1e1LdYSvRzs"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = train[[\"label\"]].to_numpy().transpose()[0]\n",
        "test_labels = test[[\"label\"]].to_numpy().transpose()[0]\n",
        "\n",
        "model = MajorityBaselineClassifier(2)\n",
        "model.fit(train_labels)"
      ],
      "metadata": {
        "id": "-2dm4c2wvUzb"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\t - Test Accuracy: {:.4f}'.format(accuracy))\n",
        "print('\\t - Test Precision: {:.4f}'.format(precision))\n",
        "print('\\t - Test Recall: {:.4f}'.format(recall))\n",
        "print('\\t - Test F1: {:.4f}\\n'.format(f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es_HqRPjviec",
        "outputId": "8ea0f9bd-7a77-431e-e849-3b1d910b3afe"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t - Test Accuracy: 51.6084\n",
            "\t - Test Precision: 51.6084\n",
            "\t - Test Recall: 51.6084\n",
            "\t - Test F1: 68.0812\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised Models with BERT Embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "EX7PDJj3Ppiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers "
      ],
      "metadata": {
        "id": "lYZ_CDOxzjWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba46d13-509b-4bcd-cc59-3e0c41f80d9f"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import transformers\n",
        "import tqdm\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "def func_tokenizer(tokenizer_name, docs):\n",
        "    features = []\n",
        "    for doc in tqdm.tqdm(docs, desc = 'converting text to numerical features'):\n",
        "        tokens = tokenizer_name.tokenize(doc)\n",
        "        ids = tokenizer_name.convert_tokens_to_ids(tokens)\n",
        "        features.append(ids)\n",
        "    return features"
      ],
      "metadata": {
        "id": "zVDV37ADzErf"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize bert tokenizer\n",
        "bert_tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased') #bert-large-uncased\n",
        "features = ['text'] + corr_features_04\n",
        "print(features)\n",
        "\n",
        "# Prepare the dataset with bert features\n",
        "X,y = train[features], df_train['label']\n",
        "bert_features = func_tokenizer(bert_tokenizer, X['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjZQPF1Mzq0Y",
        "outputId": "bcc47640-14da-48b5-9a46-a5a33c264fac"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['text', 'label', 'lex_liwc_Tone', 'lex_liwc_Clout', 'lex_liwc_i']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "converting text to numerical features: 100%|██████████| 2838/2838 [00:12<00:00, 219.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_trg = sequence.pad_sequences(bert_features, maxlen = 500)\n",
        "X = pd.DataFrame(bert_trg)\n",
        "\n",
        "# Add high-correlated numerical features\n",
        "X = X.assign(lex_liwc_Tone= train['lex_liwc_Tone'].values)\n",
        "X = X.assign(lex_liwc_Clout = train['lex_liwc_Clout'].values)\n",
        "X = X.assign(lex_liwc_i = train['lex_liwc_i'].values)\n",
        "X"
      ],
      "metadata": {
        "id": "2KwwF-03z2Xn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "d32aa7b6-a878-4f5b-b9f5-1a0a6eded0ca"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      0  1  2  3  4  5  6  7  8  9  ...   493   494   495    496   497   498  \\\n",
              "0     0  0  0  0  0  0  0  0  0  0  ...  2031  1037  3558   1999  4466  2847   \n",
              "1     0  0  0  0  0  0  0  0  0  0  ...  2093  1002  2753   9733  1043  6169   \n",
              "2     0  0  0  0  0  0  0  0  0  0  ...  2003  1999  2010   3438  1005  1055   \n",
              "3     0  0  0  0  0  0  0  0  0  0  ...  2126  1010  1045   7977  2870  2041   \n",
              "4     0  0  0  0  0  0  0  0  0  0  ...  2438  3350  2000   2031  1037  2553   \n",
              "...  .. .. .. .. .. .. .. .. .. ..  ...   ...   ...   ...    ...   ...   ...   \n",
              "2833  0  0  0  0  0  0  0  0  0  0  ...  2673  1998  2017   2467  2097  2022   \n",
              "2834  0  0  0  0  0  0  0  0  0  0  ...  2000  2031  2026   2166  2067  2153   \n",
              "2835  0  0  0  0  0  0  0  0  0  0  ...  1010  2030  2128  15222  8950  1007   \n",
              "2836  0  0  0  0  0  0  0  0  0  0  ...  1012  2023  2003   4326  2000  2033   \n",
              "2837  0  0  0  0  0  0  0  0  0  0  ...  2589  1012  2085   1045  2572  6015   \n",
              "\n",
              "       499  lex_liwc_Tone  lex_liwc_Clout  lex_liwc_i  \n",
              "0     1012           1.00           15.04        9.48  \n",
              "1     1012          98.18           76.85        1.83  \n",
              "2     1012          25.77           76.38        8.98  \n",
              "3     1012          79.26           15.25       16.12  \n",
              "4     1012           1.00           28.71        7.87  \n",
              "...    ...            ...             ...         ...  \n",
              "2833  1012          99.00           99.00        5.62  \n",
              "2834  1012           1.00            4.45       11.11  \n",
              "2835  1012          80.01           97.34        0.00  \n",
              "2836  1012          25.77           61.58        4.41  \n",
              "2837  1012           1.00           29.92       14.04  \n",
              "\n",
              "[2838 rows x 503 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5a505dcd-4525-4936-ac1a-0f8b8f296ba3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>493</th>\n",
              "      <th>494</th>\n",
              "      <th>495</th>\n",
              "      <th>496</th>\n",
              "      <th>497</th>\n",
              "      <th>498</th>\n",
              "      <th>499</th>\n",
              "      <th>lex_liwc_Tone</th>\n",
              "      <th>lex_liwc_Clout</th>\n",
              "      <th>lex_liwc_i</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2031</td>\n",
              "      <td>1037</td>\n",
              "      <td>3558</td>\n",
              "      <td>1999</td>\n",
              "      <td>4466</td>\n",
              "      <td>2847</td>\n",
              "      <td>1012</td>\n",
              "      <td>1.00</td>\n",
              "      <td>15.04</td>\n",
              "      <td>9.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2093</td>\n",
              "      <td>1002</td>\n",
              "      <td>2753</td>\n",
              "      <td>9733</td>\n",
              "      <td>1043</td>\n",
              "      <td>6169</td>\n",
              "      <td>1012</td>\n",
              "      <td>98.18</td>\n",
              "      <td>76.85</td>\n",
              "      <td>1.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2003</td>\n",
              "      <td>1999</td>\n",
              "      <td>2010</td>\n",
              "      <td>3438</td>\n",
              "      <td>1005</td>\n",
              "      <td>1055</td>\n",
              "      <td>1012</td>\n",
              "      <td>25.77</td>\n",
              "      <td>76.38</td>\n",
              "      <td>8.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2126</td>\n",
              "      <td>1010</td>\n",
              "      <td>1045</td>\n",
              "      <td>7977</td>\n",
              "      <td>2870</td>\n",
              "      <td>2041</td>\n",
              "      <td>1012</td>\n",
              "      <td>79.26</td>\n",
              "      <td>15.25</td>\n",
              "      <td>16.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2438</td>\n",
              "      <td>3350</td>\n",
              "      <td>2000</td>\n",
              "      <td>2031</td>\n",
              "      <td>1037</td>\n",
              "      <td>2553</td>\n",
              "      <td>1012</td>\n",
              "      <td>1.00</td>\n",
              "      <td>28.71</td>\n",
              "      <td>7.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2833</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2673</td>\n",
              "      <td>1998</td>\n",
              "      <td>2017</td>\n",
              "      <td>2467</td>\n",
              "      <td>2097</td>\n",
              "      <td>2022</td>\n",
              "      <td>1012</td>\n",
              "      <td>99.00</td>\n",
              "      <td>99.00</td>\n",
              "      <td>5.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2834</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2000</td>\n",
              "      <td>2031</td>\n",
              "      <td>2026</td>\n",
              "      <td>2166</td>\n",
              "      <td>2067</td>\n",
              "      <td>2153</td>\n",
              "      <td>1012</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4.45</td>\n",
              "      <td>11.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2835</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1010</td>\n",
              "      <td>2030</td>\n",
              "      <td>2128</td>\n",
              "      <td>15222</td>\n",
              "      <td>8950</td>\n",
              "      <td>1007</td>\n",
              "      <td>1012</td>\n",
              "      <td>80.01</td>\n",
              "      <td>97.34</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2836</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1012</td>\n",
              "      <td>2023</td>\n",
              "      <td>2003</td>\n",
              "      <td>4326</td>\n",
              "      <td>2000</td>\n",
              "      <td>2033</td>\n",
              "      <td>1012</td>\n",
              "      <td>25.77</td>\n",
              "      <td>61.58</td>\n",
              "      <td>4.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2837</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2589</td>\n",
              "      <td>1012</td>\n",
              "      <td>2085</td>\n",
              "      <td>1045</td>\n",
              "      <td>2572</td>\n",
              "      <td>6015</td>\n",
              "      <td>1012</td>\n",
              "      <td>1.00</td>\n",
              "      <td>29.92</td>\n",
              "      <td>14.04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2838 rows × 503 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a505dcd-4525-4936-ac1a-0f8b8f296ba3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5a505dcd-4525-4936-ac1a-0f8b8f296ba3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5a505dcd-4525-4936-ac1a-0f8b8f296ba3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=UserWarning)"
      ],
      "metadata": {
        "id": "CzF1cRLd0LFX"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_models():\n",
        "    models = dict()\n",
        "    models['SVM'] = make_pipeline(StandardScaler(),SVC())\n",
        "    models['Naive Bayes'] = make_pipeline(StandardScaler(), GaussianNB())\n",
        "    models['Logistic Regression'] = make_pipeline(StandardScaler(),LogisticRegression(solver = 'saga', C = 70.0))\n",
        "    \n",
        "    # decision trees\n",
        "    models['Descision Tree'] = DecisionTreeClassifier(max_depth = 1)\n",
        "    models['XGBoost'] = XGBClassifier(n_estimators = 11, max_depth = 1)\n",
        "    models['Random Forest'] = RandomForestClassifier(n_estimators = 10)\n",
        "    models['AdaBoost'] = AdaBoostClassifier(n_estimators= 12)\n",
        "    return models"
      ],
      "metadata": {
        "id": "b13BH8xN0WcJ"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X, y):\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=4, random_state=1)\n",
        "\tprecision_scores = cross_val_score(model, X, y, scoring='precision', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\trecall_scores = cross_val_score(model, X, y, scoring='recall', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\tf1_scores = cross_val_score(model, X, y, scoring='f1', cv=cv, n_jobs=-1, error_score='raise') \n",
        "\treturn precision_scores, recall_scores, f1_scores"
      ],
      "metadata": {
        "id": "l8b4fVAw0dcx"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(columns=['Model','Precision','Recall','F1-Score'])\n",
        "\n",
        "models = get_models()\n",
        "for name, model in models.items():\n",
        "\tprecision_scores, recall_scores, f1_scores = evaluate_model(model, X, y)\n",
        "\tto_append = [name, round(precision_scores.mean(),4), round(recall_scores.mean(),4), round(f1_scores.mean(),4)]\n",
        "\ta_series = pd.Series(to_append, index = df_results.columns)\n",
        "\tdf_results = df_results.append(a_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "0kLG2Oj00eS_"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = df_results.sort_values(by = 'F1-Score', ascending = True)\n",
        "df_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "TxL2gvAT-L2Y",
        "outputId": "782f0992-5f0c-48b2-86fc-62bc80e844f0"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Model  Precision  Recall  F1-Score\n",
              "1          Naive Bayes     0.7697  0.0398    0.0753\n",
              "6        Random Forest     0.6867  0.6043    0.6335\n",
              "3       Descision Tree     0.6750  0.8123    0.7370\n",
              "7             AdaBoost     0.7273  0.7554    0.7403\n",
              "4              XGBoost     0.7240  0.7592    0.7406\n",
              "0                  SVM     0.6995  0.7989    0.7453\n",
              "2  Logistic Regression     0.7244  0.7708    0.7463"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4e971c6a-1f7e-452a-a79b-95560422e16d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Naive Bayes</td>\n",
              "      <td>0.7697</td>\n",
              "      <td>0.0398</td>\n",
              "      <td>0.0753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.6867</td>\n",
              "      <td>0.6043</td>\n",
              "      <td>0.6335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Descision Tree</td>\n",
              "      <td>0.6750</td>\n",
              "      <td>0.8123</td>\n",
              "      <td>0.7370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>0.7273</td>\n",
              "      <td>0.7554</td>\n",
              "      <td>0.7403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.7240</td>\n",
              "      <td>0.7592</td>\n",
              "      <td>0.7406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SVM</td>\n",
              "      <td>0.6995</td>\n",
              "      <td>0.7989</td>\n",
              "      <td>0.7453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.7244</td>\n",
              "      <td>0.7708</td>\n",
              "      <td>0.7463</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e971c6a-1f7e-452a-a79b-95560422e16d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4e971c6a-1f7e-452a-a79b-95560422e16d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4e971c6a-1f7e-452a-a79b-95560422e16d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results.to_csv('df_results_supervised_with_BERT_embedding.csv')"
      ],
      "metadata": {
        "id": "FtAvyP7rEvf7"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised Model with Word2Vec Embedding (Logistic Regression) "
      ],
      "metadata": {
        "id": "NKtduSd7P9D0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "FLK115K-b494",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6359b4-70a9-4c2b-d7f5-118d8856dc72"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "hZNDDx1kjMCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5bec65d-0f8b-4d92-9de2-35b796b0d8db"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show gensim | grep Version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfiAN_SDd-i4",
        "outputId": "eea86292-f1e0-4da9-ffc0-7b6e10e1533e"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version: 4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim"
      ],
      "metadata": {
        "id": "vcW6Cwz7jZCd"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean data using the built in cleaner in gensim\n",
        "train['text_clean'] = train['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
        "test['text_clean'] = test['text'].apply(lambda x: gensim.utils.simple_preprocess(x))"
      ],
      "metadata": {
        "id": "r9MFLbp2fv3F"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train['text_clean']\n",
        "X_test = test['text_clean'] \n",
        "y_train = train['label']\n",
        "y_test = test['label']"
      ],
      "metadata": {
        "id": "Xukvi4CDgRoL"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the word2vec model\n",
        "w2v_model = gensim.models.Word2Vec(X_train,\n",
        "                                   #size=100,\n",
        "                                   window=5,\n",
        "                                   min_count=2)"
      ],
      "metadata": {
        "id": "JcJuXc-CcDAC"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = set(w2v_model.wv.index_to_key) #index2word\n",
        "X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
        "                         for ls in X_train])\n",
        "X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
        "                         for ls in X_test])"
      ],
      "metadata": {
        "id": "TKPxmgtZcC44"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute sentence vectors by averaging the word vectors for the words contained in the sentence\n",
        "X_train_vect_avg = []\n",
        "for v in X_train_vect:\n",
        "    if v.size:\n",
        "        X_train_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
        "        \n",
        "X_test_vect_avg = []\n",
        "for v in X_test_vect:\n",
        "    if v.size:\n",
        "        X_test_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_test_vect_avg.append(np.zeros(100, dtype=float))"
      ],
      "metadata": {
        "id": "eNl7bxwkcCwo"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.DataFrame(X_train_vect_avg)\n",
        "\n",
        "# Add high-correlated numerical features\n",
        "X = X.assign(lex_liwc_Tone= train['lex_liwc_Tone'].values)\n",
        "X = X.assign(lex_liwc_Clout = train['lex_liwc_Clout'].values)\n",
        "X = X.assign(lex_liwc_i = train['lex_liwc_i'].values)"
      ],
      "metadata": {
        "id": "Na9EC0B-ktzA"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = get_models()\n",
        "for name, model in models.items():\n",
        "\tprecision_scores, recall_scores, f1_scores = evaluate_model(model, X, y_train)\n",
        "\tto_append = [name, round(precision_scores.mean(),4), round(recall_scores.mean(),4), round(f1_scores.mean(),4)]\n",
        "\ta_series = pd.Series(to_append, index = df_results.columns)\n",
        "\tdf_results = df_results.append(a_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "MKC4S1arbmFf"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = df_results.sort_values(by = 'F1-Score', ascending = True)\n",
        "df_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "Z6BiR4bTcxUW",
        "outputId": "8d403633-a8be-4279-a010-6bb7139a352e"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Model  Precision  Recall  F1-Score\n",
              "7        Random Forest     0.7193  0.6615    0.6943\n",
              "2          Naive Bayes     0.6560  0.7621    0.7046\n",
              "4       Descision Tree     0.6750  0.8123    0.7370\n",
              "5              XGBoost     0.7208  0.7648    0.7413\n",
              "8             AdaBoost     0.7253  0.7692    0.7458\n",
              "1                  SVM     0.7323  0.7891    0.7592\n",
              "3  Logistic Regression     0.7496  0.7901    0.7688"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd5faeda-98cd-436f-b6f8-5da48acc2a74\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.7193</td>\n",
              "      <td>0.6615</td>\n",
              "      <td>0.6943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Naive Bayes</td>\n",
              "      <td>0.6560</td>\n",
              "      <td>0.7621</td>\n",
              "      <td>0.7046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Descision Tree</td>\n",
              "      <td>0.6750</td>\n",
              "      <td>0.8123</td>\n",
              "      <td>0.7370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.7208</td>\n",
              "      <td>0.7648</td>\n",
              "      <td>0.7413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>0.7253</td>\n",
              "      <td>0.7692</td>\n",
              "      <td>0.7458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SVM</td>\n",
              "      <td>0.7323</td>\n",
              "      <td>0.7891</td>\n",
              "      <td>0.7592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.7496</td>\n",
              "      <td>0.7901</td>\n",
              "      <td>0.7688</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd5faeda-98cd-436f-b6f8-5da48acc2a74')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dd5faeda-98cd-436f-b6f8-5da48acc2a74 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dd5faeda-98cd-436f-b6f8-5da48acc2a74');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results.to_csv('df_results_supervised_with_Word2vec.csv')"
      ],
      "metadata": {
        "id": "vvgXnDrlvBWp"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning of pretrained BERT-base on Reddit Dataset\n"
      ],
      "metadata": {
        "id": "cybBcrLFQFI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZYEGbexeivk",
        "outputId": "b3243f72-732f-46d3-9da9-323c62efcfdc"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tabulate import tabulate\n",
        "from tqdm import trange\n",
        "import random"
      ],
      "metadata": {
        "id": "Pi6LYONHcv6b"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "lTG5zokep8qP"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = train.clean_text.values\n",
        "labels = train.label.values"
      ],
      "metadata": {
        "id": "5V_6Zj-KVR6M"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    do_lower_case = True\n",
        "    )"
      ],
      "metadata": {
        "id": "I5OVaGbRcpap"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = []\n",
        "attention_masks = []\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  return tokenizer.encode_plus(\n",
        "                        input_text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 32,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt'\n",
        "                   )\n",
        "\n",
        "\n",
        "for sample in text:\n",
        "  encoding_dict = preprocessing(sample, tokenizer)\n",
        "  token_id.append(encoding_dict['input_ids']) \n",
        "  attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "\n",
        "token_id = torch.cat(token_id, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels = torch.tensor(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sHWDXKCcxGZ",
        "outputId": "750a5d51-2b47-4538-ec11-c0babf418a87"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_ratio = 0.2\n",
        "batch_size = 128\n",
        "\n",
        "# Indices of the train and validation splits stratified by labels\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(labels)),\n",
        "    test_size = val_ratio,\n",
        "    shuffle = True,\n",
        "    stratify = labels)\n",
        "\n",
        "# Train and validation sets\n",
        "train_set = TensorDataset(token_id[train_idx], \n",
        "                          attention_masks[train_idx], \n",
        "                          labels[train_idx])\n",
        "\n",
        "val_set = TensorDataset(token_id[val_idx], \n",
        "                        attention_masks[val_idx], \n",
        "                        labels[val_idx])\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "            train_set,\n",
        "            sampler = RandomSampler(train_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_set,\n",
        "            sampler = SequentialSampler(val_set),\n",
        "            batch_size = batch_size\n",
        "        )"
      ],
      "metadata": {
        "id": "35NEzlVrsDt3"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def b_tp(preds, labels):\n",
        "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
        "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_fp(preds, labels):\n",
        "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
        "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_tn(preds, labels):\n",
        "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
        "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_fn(preds, labels):\n",
        "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
        "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_metrics(preds, labels):\n",
        "  '''\n",
        "  Returns the following metrics:\n",
        "    - accuracy    = (TP + TN) / N\n",
        "    - precision   = TP / (TP + FP)\n",
        "    - recall      = TP / (TP + FN)\n",
        "    - specificity = TN / (TN + FP)\n",
        "  '''\n",
        "  preds = np.argmax(preds, axis = 1).flatten()\n",
        "  labels = labels.flatten()\n",
        "  tp = b_tp(preds, labels)\n",
        "  tn = b_tn(preds, labels)\n",
        "  fp = b_fp(preds, labels)\n",
        "  fn = b_fn(preds, labels)\n",
        "  b_accuracy = (tp + tn) / len(labels)\n",
        "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
        "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
        "  b_f1 = tp / (tp + 0.5*(fp+fn)) if (tp + 0.5*(fp+fn)) > 0 else 'nan'\n",
        "  return b_accuracy, b_precision, b_recall, b_f1"
      ],
      "metadata": {
        "id": "SXpQ7KUudz-B"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BertForSequenceClassification model\n",
        "model = BertForSequenceClassification.from_pretrained( \n",
        "    'bert-base-uncased',\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                              lr = 5e-5,\n",
        "                              eps = 1e-08\n",
        "                              )\n",
        "\n",
        "# Run on GPU\n",
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVYDOVL-eNPl",
        "outputId": "18608daf-c8ff-41a0-b425-6aa741a59665"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "epochs = 3 # according to the paper\n",
        "\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "    \n",
        "    # ========== Training ==========\n",
        "    \n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    \n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids, \n",
        "                             token_type_ids = None, \n",
        "                             attention_mask = b_input_mask, \n",
        "                             labels = b_labels)\n",
        "        # Backward pass\n",
        "        train_output.loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += train_output.loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_f1 = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = model(b_input_ids, \n",
        "                              token_type_ids = None, \n",
        "                              attention_mask = b_input_mask)\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        # Calculate validation metrics\n",
        "        b_accuracy, b_precision, b_recall, b_f1 = b_metrics(logits, label_ids)\n",
        "        val_accuracy.append(b_accuracy)\n",
        "        # Update precision only when (tp + fp) !=0; ignore nan\n",
        "        if b_precision != 'nan': val_precision.append(b_precision)\n",
        "        # Update recall only when (tp + fn) !=0; ignore nan\n",
        "        if b_recall != 'nan': val_recall.append(b_recall)\n",
        "        # Update f1 only when (tn + fp) !=0; ignore nan\n",
        "        if b_f1 != 'nan': val_f1.append(b_f1)\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "    print('\\t - Validation F1: {:.4f}\\n'.format(sum(val_f1)/len(val_f1)) if len(val_f1)>0 else '\\t - Validation Specificity: NaN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsPVmzn2ePxo",
        "outputId": "75016eab-9cf4-4e61-d8e3-90d457a83204"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  33%|███▎      | 1/3 [00:12<00:24, 12.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.6738\n",
            "\t - Validation Accuracy: 0.6667\n",
            "\t - Validation Precision: 0.6266\n",
            "\t - Validation Recall: 0.8492\n",
            "\t - Validation F1: 0.7195\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [00:24<00:12, 12.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.5565\n",
            "\t - Validation Accuracy: 0.7031\n",
            "\t - Validation Precision: 0.6492\n",
            "\t - Validation Recall: 0.9125\n",
            "\t - Validation F1: 0.7559\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 3/3 [00:37<00:00, 12.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.4082\n",
            "\t - Validation Accuracy: 0.7406\n",
            "\t - Validation Precision: 0.6892\n",
            "\t - Validation Recall: 0.8952\n",
            "\t - Validation F1: 0.7773\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = test.clean_text.values\n",
        "labels = test.label.values"
      ],
      "metadata": {
        "id": "Rb-SAThllhdj"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = []\n",
        "attention_masks = []\n",
        "\n",
        "for sample in text:\n",
        "  encoding_dict = preprocessing(sample, tokenizer)\n",
        "  token_id.append(encoding_dict['input_ids']) \n",
        "  attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "token_id = torch.cat(token_id, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels = torch.tensor(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMtPQHDGl7xo",
        "outputId": "cef3bfcf-10f9-41a7-f562-9b0d1b36ef8a"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "val_set = TensorDataset(token_id, \n",
        "                        attention_masks, \n",
        "                        labels)\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_set,\n",
        "            sampler = SequentialSampler(val_set),\n",
        "            batch_size = batch_size\n",
        "        )"
      ],
      "metadata": {
        "id": "iXquzupxlViN"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracking variables \n",
        "val_accuracy = []\n",
        "val_precision = []\n",
        "val_recall = []\n",
        "val_f1 = []\n",
        "\n",
        "for batch in validation_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    with torch.no_grad():\n",
        "      # Forward pass\n",
        "      eval_output = model(b_input_ids, \n",
        "                          token_type_ids = None, \n",
        "                          attention_mask = b_input_mask)\n",
        "    logits = eval_output.logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    # Calculate validation metrics\n",
        "    b_accuracy, b_precision, b_recall, b_f1 = b_metrics(logits, label_ids)\n",
        "    val_accuracy.append(b_accuracy)\n",
        "    # Update precision only when (tp + fp) !=0; ignore nan\n",
        "    if b_precision != 'nan': val_precision.append(b_precision)\n",
        "    # Update recall only when (tp + fn) !=0; ignore nan\n",
        "    if b_recall != 'nan': val_recall.append(b_recall)\n",
        "    # Update f1 only when (tn + fp) !=0; ignore nan\n",
        "    if b_f1 != 'nan': val_f1.append(b_f1)\n",
        "\n",
        "print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "print('\\t - Validation F1: {:.4f}\\n'.format(sum(val_f1)/len(val_f1)) if len(val_f1)>0 else '\\t - Validation Specificity: NaN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuu-9qRao17U",
        "outputId": "8b96c4e2-8bf9-42b4-acba-b2e8c710e133"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.3683\n",
            "\t - Validation Accuracy: 0.7370\n",
            "\t - Validation Precision: 0.7054\n",
            "\t - Validation Recall: 0.8450\n",
            "\t - Validation F1: 0.7684\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References for external code repositories:\n",
        "*   https://paperswithcode.com/dataset/dreaddit\n",
        "*   https://www.kaggle.com/code/sohommajumder21/bert-tokenizer-with-9-models-nlp-stress-analysis\n",
        "*   https://github.com/fsentin/dreaddit\n",
        "*   https://medium.com/@dilip.voleti/classification-using-word2vec-b1d79d375381\n",
        "*   https://towardsdatascience.com/fine-tuning-bert-for-text-classification-54e7df642894\n",
        "*   https://mccormickml.com/2019/07/22/BERT-fine-tuning/#1-setup\n"
      ],
      "metadata": {
        "id": "zN1jKJgAFJ69"
      }
    }
  ]
}
